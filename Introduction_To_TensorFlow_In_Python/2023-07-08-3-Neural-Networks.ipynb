{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "  \n",
    "The previous chapters taught you how to build models in TensorFlow 2. In this chapter, you will apply those same tools to build, train, and make predictions with neural networks. You will learn how to define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both low-level linear algebra and high-level Keras API operations to define and train models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[TensorFlow Documentation](https://www.tensorflow.org)  \n",
    "[TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.20148&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)  \n",
    "[Tensor LaTex Formulas](https://www.math-linux.com/latex-26/faq/latex-faq/article/latex-tensor-product)  \n",
    "[3Blue1Brown - Youtube playlist on linear algebra - RECOMMEND](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)  \n",
    "[LaTeX commands](https://en.wikipedia.org/wiki/List_of_mathematical_symbols_by_subject)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>tf.constant()</td>\n",
    "    <td>Creates a constant tensor with a specified value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>tf.Variable()</td>\n",
    "    <td>Creates a mutable tensor variable that can be modified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>tf.zeros()</td>\n",
    "    <td>Creates a tensor filled with zeros.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>tf.ones()</td>\n",
    "    <td>Creates a tensor filled with ones.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>tf.zeros_like()</td>\n",
    "    <td>Creates a tensor of zeros with the same shape as another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>tf.ones_like()</td>\n",
    "    <td>Creates a tensor of ones with the same shape as another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>tf.fill()</td>\n",
    "    <td>Creates a tensor filled with a specified scalar value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>tf.add()</td>\n",
    "    <td>Performs element-wise addition of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>tf.multiply()</td>\n",
    "    <td>Performs element-wise multiplication of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>tf.matmul()</td>\n",
    "    <td>Performs matrix multiplication of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>tf.reduce_sum()</td>\n",
    "    <td>Computes the sum of elements across specified dimensions of a tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>tf.gradient()</td>\n",
    "    <td>Computes the gradients of a tensor with respect to another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>tf.GradientTape</td>\n",
    "    <td>Records operations for automatic differentiation to compute gradients.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>tf.reshape()</td>\n",
    "    <td>Reshapes a tensor into a specified shape.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>tf.random()</td>\n",
    "    <td>Generates random values from a specified distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>tf.random().uniform()</td>\n",
    "    <td>Generates random values from a uniform distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>tf.GradientTape.watch()</td>\n",
    "    <td>Used to start tracing Tensor by the Tape</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>tf.cast()</td>\n",
    "    <td>Casts a tensor to a new datatype</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>tensorflow.keras.losses</td>\n",
    "    <td>Retrieves a Keras loss as a function</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>tensorflow.keras.losses.mse()</td>\n",
    "    <td>Computes the mean squared error between labels and predictions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>21</td>\n",
    "    <td>tensorflow.keras.losses.mae()</td>\n",
    "    <td>Computes the mean absolute error between labels and predictions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>tensorflow.keras.losses.Huber()</td>\n",
    "    <td>Computes Huber loss value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>23</td>\n",
    "    <td>tf.math.log()</td>\n",
    "    <td>Computes natural logarithm of x element-wise.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>24</td>\n",
    "    <td>tf.keras.optimizers.Adam</td>\n",
    "    <td>Optimizer that implements the Adam algorithm. Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25</td>\n",
    "    <td>tf.keras.optimizers.Adam.minimize()</td>\n",
    "    <td>Calling .minimize() takes care of both computing the gradients and applying them to the variables.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26</td>\n",
    "    <td>tf.convert_to_tensor()</td>\n",
    "    <td>Converts a numpy array or Python list to a Tensorflow tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27</td>\n",
    "    <td>tf.gather()</td>\n",
    "    <td>Gathers slices from a tensor along a specified axis.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>28</td>\n",
    "    <td>tf.keras.optimizers.SGD</td>\n",
    "    <td>Optimizer that implements the Stochastic Gradient Descent algorithm.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>29</td>\n",
    "    <td>tf.keras.optimizers.RMSprop</td>\n",
    "    <td>Optimizer that implements the RMSprop algorithm.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>30</td>\n",
    "    <td>tf.keras.activations.sigmoid</td>\n",
    "    <td>Computes the sigmoid activation function.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31</td>\n",
    "    <td>tf.keras.activations.relu</td>\n",
    "    <td>Computes the ReLU activation function.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>32</td>\n",
    "    <td>tf.random.normal()</td>\n",
    "    <td>Generates random values from a normal distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>33</td>\n",
    "    <td>tf.keras.layers.Dense()</td>\n",
    "    <td>A fully connected layer in a neural network.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>34</td>\n",
    "    <td>tf.keras.layers.Dropout()</td>\n",
    "    <td>Applies dropout regularization to the input.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>35</td>\n",
    "    <td>tf.keras.losses.binary_crossentropy()</td>\n",
    "    <td>Computes the binary cross-entropy loss.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>36</td>\n",
    "    <td>tf.math.confusion_matrix()</td>\n",
    "    <td>Computes the confusion matrix.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>37</td>\n",
    "    <td>pd.crosstab()</td>\n",
    "    <td>Computes a cross-tabulation of two or more factors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>38</td>\n",
    "    <td>np.hstack()</td>\n",
    "    <td>Stacks arrays in sequence horizontally (column-wise).</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: tensorflow  \n",
    "Version: 2.13.0  \n",
    "Summary: TensorFlow is an open source machine learning framework for everyone.  \n",
    "  \n",
    "Name: scikit-learn  \n",
    "Version: 1.3.0  \n",
    "Summary: A set of python modules for machine learning and data mining  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "$\\begin{bmatrix}2\\\\3\\end{bmatrix} = \\begin{bmatrix}x\\\\y\\end{bmatrix} \\rightarrow (2, 3) = (x, y)$ \n",
    "  \n",
    "- Think of a collection of vectors as points on a plane. \n",
    "- Also, think of vectors as a line, an ordered list, and as a coordinate. when you have a pair of numbers that is meant to describe a vector, think of each coordinate($x$,$y$) as a scaler for $i$ and $j$. \n",
    "- A matrix transforms a vector\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "$example.$\n",
    "  \n",
    "$where.$\n",
    "  \n",
    "$\\vec{V} = \\begin{bmatrix}-1\\\\2\\end{bmatrix}=-1_{i} + 2_{j}$\n",
    "\n",
    "$i = \\begin{bmatrix}1\\\\-2\\end{bmatrix}$, $j = \\begin{bmatrix}3\\\\0\\end{bmatrix}$\n",
    "  \n",
    "$\\text{Transformed }\\vec{V}= -1(\\text{Transformed }{i}) + 2(\\text{Transformed }{j})$\n",
    "  \n",
    "\n",
    "  \n",
    "$applied.$\n",
    "  \n",
    "$\\begin{bmatrix}x\\\\y\\end{bmatrix}\\rightarrow x\\begin{bmatrix}i_0\\\\i_1\\end{bmatrix}+y\\begin{bmatrix}j_0\\\\j_1\\end{bmatrix}= \\begin{bmatrix}x(i_0) + y(j_0) \\\\ x(i_1) + y(j_1)\\end{bmatrix} = \\begin{bmatrix}\\hat i_x & \\hat j_x \\\\ \\hat i_y & \\hat j_y\\end{bmatrix} = \\hat{\\vec{V}}$\n",
    "  \n",
    "$\\begin{bmatrix}-1\\\\2\\end{bmatrix}\\rightarrow -1\\begin{bmatrix}1\\\\-2\\end{bmatrix}+2\\begin{bmatrix}3\\\\0\\end{bmatrix} = \\begin{bmatrix}-1(1) + 2(3) \\\\ -1(-2) + 2(0)\\end{bmatrix} = \\begin{bmatrix}1x+3y\\\\-2x+0y\\end{bmatrix} = \\begin{bmatrix}3 & 2\\\\-2 & 0\\end{bmatrix} = \\begin{bmatrix}5\\\\2\\end{bmatrix}$\n",
    "  \n",
    "$\\hat{\\vec{V}} = \\begin{bmatrix}5\\\\2\\end{bmatrix}$\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` = get self defined function schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 21:03:06.946357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import tensorflow as tf             # TensorFlow:               Deep-Learning Neural Networks\n",
    "import math                         # Math:                     Standard Library for mathmatical operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense layers\n",
    "  \n",
    "In this chapter, we will focus on training neural networks in TensorFlow. We will start with an overview of a frequently used component of neural networks: the dense layer.\n",
    "  \n",
    "**The linear regression model**\n",
    "  \n",
    "Throughout this chapter, we'll make use of a dataset on credit card default. It contains features, such as marital status and payment amount, which we'll use to predict a target, default. Here, we have the familiar linear regression model. We take marital status, which is 1, and bill amount, which is 3. We then multiply the inputs by their respective weights, 0.10 and -0.25, and sum.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow.png' alt='img' width='740'>\n",
    "  \n",
    "**What is a neural network?**\n",
    "  \n",
    "So how do we get from a linear regression to a neural network? By adding a hidden layer, which, in this case, consists of two nodes. Each hidden layer node takes our two inputs, multiplies them by their respective weights, and sums them together. We also typically pass the hidden layer output to an activation function, but we will come back to that later. Finally, we sum together the outputs of the two hidden layers to compute our prediction for default. This entire process of generating a prediction is referred to as *forward propagation*.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow1.png' alt='img' width='740'>\n",
    "  \n",
    "**What is a neural network?**\n",
    "  \n",
    "In this chapter, we will construct neural networks with three types of layers: an input layer, some number of hidden layers, and an output layer. The input layer consists of our features. The output layer contains our prediction. Each hidden layer takes inputs from the previous layer, applies numerical weights to them, sums them together, and then applies an activation function. In the neural network graph, we have applied a particular type of hidden layer called a dense layer. \n",
    "  \n",
    "A dense layer applies weights to all nodes from the previous layer. We will use dense layers throughout this chapter to construct networks.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow2.png' alt='img' width='740'>\n",
    "  \n",
    "**A simple dense layer**\n",
    "  \n",
    "Let's look at a simple example of a dense layer. We'll first define a constant tensor that contains the marital status and age data as the input layer. We then initialize weights as a variable, since we will train those weights to predict the output from the inputs. We also define a bias, which will play a similar role to the intercept in the linear regression model.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow3.png' alt='img' width='740'>\n",
    "  \n",
    "**A simple dense layer**\n",
    "  \n",
    "Finally, we define a dense layer. Note that we first perform a matrix multiplication of the inputs by the weights and assign that to the tensor named product. We then add product to the bias and apply a non-linear transformation, in this case the sigmoid function. This is called the activation function and we will explore this in more depth in the next video, but do not worry about it for now. Furthermore, note that the bias is not associated with a feature and is analogous to the intercept in a linear regression. We will typically not draw it in neural network diagrams for simplicity.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow4.png' alt='img' width='740'>\n",
    "  \n",
    "**Defining a complete model**\n",
    "  \n",
    "Note that TensorFlow also comes with higher level operations, such as `tf.keras.layers.Dense`, which allows us to skip the linear algebra. In this example, we take input data and convert it to a 32-bit float tensor. We then define a first hidden dense layer using `keras.layers.Dense`. The first argument specifies the number of outgoing nodes. And the second argument is the activation function. By default, a bias will be included. Note that we've also passed inputs as an argument to the first dense layer.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow5.png' alt='img' width='740'>\n",
    "  \n",
    "**Defining a complete model**\n",
    "  \n",
    "We can easily define another dense layer, which takes the first dense layer as an argument and then reduces the number of nodes. The output layer reduces this again to one node.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow6.png' alt='img' width='740'>\n",
    "  \n",
    "**High-level versus low-level approach**\n",
    "  \n",
    "Finally, let's compare the high-level and low-level approaches. The high-level approach relies on complex operations in high-level APIs, such as Keras and Estimators, reducing the amount of code needed. The weights and the mathematical operations will typically be hidden by the layer constructor. The low-level approach uses linear algebra, which allows for the construction of any model. TensorFlow allows us to use either approach or even combine them. You now know how to construct dense layers using both the high and low-level approaches.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow7.png' alt='img' width='740'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear algebra of dense layers\n",
    "  \n",
    "There are two ways to define a dense layer in `tensorflow`. The first involves the use of low-level, linear algebraic operations. The second makes use of high-level `keras` operations. In this exercise, we will use the first method to construct the network shown in the image below.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow8.png' alt='img' width='450'>\n",
    "  \n",
    "The input layer contains 3 features -- `education`, `marital status`, and `age` -- which are available as `borrower_features`. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "  \n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function.\n",
    "  \n",
    "1. Initialize `weights1` as a variable using a 3x2 tensor of ones.\n",
    "2. Compute the product of `borrower_features` by `weights1` using matrix multiplication.\n",
    "3. Use a sigmoid activation function to transform `product1` + `bias1`.\n",
    "4. Initialize `weights2` as a variable using a 2x1 tensor of ones.\n",
    "5. Compute the product of `dense1` by `weights2` using matrix multiplication.\n",
    "6. Use a sigmoid activation function to transform `product2` + `bias2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  2. 43.]]\n"
     ]
    }
   ],
   "source": [
    "# Features as shape 1x3 for education, marital status, and age\n",
    "borrower_features = np.array([[2., 2., 43.]], np.float32)\n",
    "print(borrower_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias1: \n",
      " 1.0\n",
      "weights1: \n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "product1: \n",
      " [[47. 47.]]\n",
      "dense1: \n",
      " [[1. 1.]]\n",
      "dense1's output shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Initialize bias1\n",
    "bias1 = tf.Variable(1.0, tf.float32)\n",
    "print('bias1: \\n', bias1.numpy())\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = tf.Variable(tf.ones((3, 2)))\n",
    "print('weights1: \\n', weights1.numpy())\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = tf.matmul(borrower_features, weights1)\n",
    "print('product1: \\n', product1.numpy())\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(product1 + bias1)\n",
    "print('dense1: \\n', dense1.numpy())\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"dense1's output shape: {}\".format(dense1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias2: \n",
      " 1.0\n",
      "weights2: \n",
      " [[1.]\n",
      " [1.]]\n",
      "bias1: \n",
      " [[2.]]\n",
      "prediction values: \n",
      " tf.Tensor([[0.95257413]], shape=(1, 1), dtype=float32)\n",
      "\n",
      "y_prediction: 0.9525741338729858\n",
      "y_actual: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize bias2 and weights2\n",
    "bias2 = tf.Variable(1.0)\n",
    "weights2 = tf.Variable(tf.ones((2, 1)))\n",
    "print('bias2: \\n', bias2.numpy())\n",
    "print('weights2: \\n', weights2.numpy())\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = tf.matmul(dense1, weights2)\n",
    "print('bias1: \\n', product2.numpy())\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = tf.keras.activations.sigmoid(product2 + bias2)\n",
    "print('prediction values: \\n', prediction)\n",
    "print('\\ny_prediction: {}'.format(prediction.numpy()[0, 0]))\n",
    "print('y_actual: 1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model produces predicted values in the interval between 0 and 1. For the example we considered, the actual value was 1 and the predicted value was a probability between 0 and 1. This, of course, is not meaningful, since we have not yet trained our model's parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The low-level approach with multiple examples\n",
    "  \n",
    "In this exercise, we'll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We'll assume the model is trained and the first layer weights, `weights1`, and bias, `bias1`, are available. We'll then perform matrix multiplication of the `borrower_features` tensor by the weights1 variable. Recall that the `borrower_features` tensor includes `education`, `marital status`, and `age`. Finally, we'll apply the sigmoid function to the elements of `products1` + `bias1`, yielding `dense1`.\n",
    "  \n",
    "$products1 = \\begin{bmatrix} 3 & 3 & 23 \\\\ 2 & 1 & 24 \\\\ 1 & 1 &   49 \\\\ 1 & 1 & 49 \\\\ 2 & 1 & 29 \\end{bmatrix} \\begin{bmatrix} -0.6 & 0.6 \\\\ 0.8 & -0.3 \\\\ -0.09 & -0.08 \\end{bmatrix}$\n",
    "  \n",
    "1. Compute `products1` by matrix multiplying the features tensor by the weights.\n",
    "2. Use a sigmoid activation function to transform `products1` + `bias1`.\n",
    "3. Print the shapes of `borrower_features`, `weights1`, `bias1`, and `dense1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of borrower_features:  (5, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of bias1:  (1,)\n",
      "\n",
      " shape of dense1:  (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the three features and 5 observations\n",
    "borrower_features = tf.cast(np.array(\n",
    "    [[ 3.,  3., 23.],\n",
    "     [ 2.,  1., 24.],\n",
    "     [ 1.,  1., 49.],\n",
    "     [ 1.,  1., 49.],\n",
    "     [ 2.,  1., 29.]]), tf.float32)\n",
    "\n",
    "# Creating the weights\n",
    "weights1 = tf.cast(np.array(\n",
    "    [[-0.6 ,  0.6 ],\n",
    "     [ 0.8 , -0.3 ],\n",
    "     [-0.09, -0.08]]), tf.float32)\n",
    "\n",
    "# Creating the bias\n",
    "bias1 = tf.Variable([0.1], tf.float32)\n",
    "\n",
    "# Compute the product of borrower_features and weights1\n",
    "products1 = tf.matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(products1 + bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#7393B3'>NOTE:</span> Here's the $\\prod{A\\cdot B}$ for the above matrix multiplication equation:\n",
    "\n",
    "$\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{31} & A_{32} & A_{33} \\\\ A_{41} & A_{42} & A_{43} \\\\ A_{51} & A_{52} & A_{53} \\end{bmatrix}\\cdot \\mathbf{B} = \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}$\n",
    "  \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "C_{11} & C_{12} \\\\\n",
    "C_{21} & C_{22} \\\\\n",
    "C_{31} & C_{32} \\\\\n",
    "C_{41} & C_{42} \\\\\n",
    "C_{51} & C_{52}\\end{bmatrix} = \\begin{bmatrix}\n",
    "A_{11}B_{11} + A_{12}B_{21} + A_{13}B_{31} & A_{11}B_{12} + A_{12}B_{22} + A_{13}B_{32} \\\\\n",
    "A_{21}B_{11} + A_{22}B_{21} + A_{23}B_{31} & A_{21}B_{12} + A_{22}B_{22} + A_{23}B_{32} \\\\\n",
    "A_{31}B_{11} + A_{32}B_{21} + A_{33}B_{31} & A_{31}B_{12} + A_{32}B_{22} + A_{33}B_{32} \\\\\n",
    "A_{41}B_{11} + A_{42}B_{21} + A_{43}B_{31} & A_{41}B_{12} + A_{42}B_{22} + A_{43}B_{32} \\\\\n",
    "A_{51}B_{11} + A_{52}B_{21} + A_{53}B_{31} & A_{51}B_{12} + A_{52}B_{22} + A_{53}B_{32}\n",
    "\\end{bmatrix}$\n",
    "  \n",
    "The resulting matrix is:\n",
    "  \n",
    "$\\begin{bmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\\\ C_{31} & C_{32} \\\\ C_{41} & C_{42} \\\\ C_{51} & C_{52} \\end{bmatrix}$\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data, `borrower_features`, is 5x3 because it consists of 5 examples for 3 features. The shape of weights1 is 3x2, as it was in the previous exercise, since it does not depend on the number of examples. Additionally, `bias1` is a scalar. Finally, `dense1` is 5x2, which means that we can multiply it by the following set of weights, `weights2`, which we defined to be 2x1 in the previous exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the dense layer operation\n",
    "  \n",
    "We've now seen how to define dense layers in `tensorflow` using linear algebra. In this exercise, we'll skip the linear algebra and let `keras` work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "  \n",
    "<img src='../_images/basics-of-neural-networks-in-tensorflow9.png' alt='img' width='450'>\n",
    "  \n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor: `borrower_features`. Additionally, the `keras.layers` module is available.\n",
    "  \n",
    "1. Set `dense1` to be a dense layer with 7 output nodes and a sigmoid activation function.\n",
    "2. Define `dense2` to be dense layer with 3 output nodes and a sigmoid activation function.\n",
    "3. Define `predictions` to be a dense layer with 1 output node and a sigmoid activation function.\n",
    "4. Print the shapes of `dense1`, `dense2`, and `predictions` in that order using the `.shape` method. \n",
    "5. Why do each of these tensors have 100 rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../_datasets/uci_credit_card.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5']\n",
      "(30000, 10) <class 'numpy.ndarray'>\n",
      "(30000, 10) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(100,) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(100, 10) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "# Selecting the desired features from the dataset\n",
    "features = df.columns[1:11].tolist()\n",
    "print(features)\n",
    "\n",
    "# Extract the values from the list of features we want to use\n",
    "borrower_features = df[features].values\n",
    "print(borrower_features.shape, type(borrower_features))\n",
    "\n",
    "# Converting to borrower observations and features to flow of tensors object, recommended over tf.cast()\n",
    "borrower_features = tf.convert_to_tensor(borrower_features, tf.float32)\n",
    "print(borrower_features.shape, type(borrower_features))\n",
    "\n",
    "# Creating the tensor index for slicing\n",
    "idx = tf.constant(list(range(0,100)))\n",
    "print(idx.shape, type(idx))\n",
    "\n",
    "# Slicing the tensor to select only 100 oservations\n",
    "borrower_features = tf.gather(borrower_features, idx)\n",
    "print(borrower_features.shape, type(borrower_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input:  (100, 10)\n",
      "Shape of dense1:  (100, 7)\n",
      "Shape of dense2:  (100, 3)\n",
      "Shape of predictions:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = tf.keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = tf.keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node, output = prediction\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('Shape of input: ', borrower_features.shape)\n",
    "print('Shape of dense1: ', dense1.shape)\n",
    "print('Shape of dense2: ', dense2.shape)\n",
    "print('Shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 8 lines of code, you were able to define 2 dense hidden layers and an output layer. This is the advantage of using high-level operations in `tensorflow`. Note that each layer has 100 rows because the input data contains 100 examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "  \n",
    "In the previous video, we discussed dense layers. We also briefly introduced the concept of an activation function through the sigmoid function. We will now return to activation functions.\n",
    "  \n",
    "**What is an activation function?**\n",
    "  \n",
    "A typical hidden layer consists of two operations. The first performs matrix multiplication, which is a linear operation, and the second applies an activation function, which is nonlinear operation.\n",
    "  \n",
    "**Components of a hidden layer**\n",
    "  \n",
    "1. Matrix multiplication, a linear operation\n",
    "1. Activation function, a non-linear operation\n",
    "  \n",
    "**Why nonlinearities are important**\n",
    "  \n",
    "Why do we need this nonlinear component? Consider a simple model using the credit card data. The features are borrower age and credit card bill amount. The target variable is default.\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks.png' alt='img' width='740'>\n",
    "  \n",
    "**Why nonlinearities are important**\n",
    "  \n",
    "Let's say we create a scatterplot of age and bill amount. We can see that bill amount usually increases early in life and decreases later in life. This suggests that a high bill for young and older borrowers may mean something different for default. If we want our model to capture this, it can't be linear. It must allow the impact of the bill amount to depend on the borrower's age. This is what an activation function does.\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks1.png' alt='img' width='740'>\n",
    "  \n",
    "**A simple example**\n",
    "  \n",
    "Let's look at a simple example, where we assume that the weight on age is 1 and the weight on the bill amount is 2. Note that ages are divided by 100 and the bill's amount is divided by 10000. We then perform the matrix multiplication step for all combinations of features: young with a high bill, young with a low bill, old with a high bill, and old with a low bill.\n",
    "  \n",
    "- matrix multiplication step for all combinations of features\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks2.png' alt='img' width='740'>\n",
    "  \n",
    "**A simple example**\n",
    "  \n",
    "If we don't apply an activation function and we assume the bias is zero, we find that the impact of bill size on default does not depend on age. In both cases, we predict a value of 0.8. Note that our target is a binary variable that is equal to 1 when the borrower defaults; however, predictions will be real numbers between 0 and 1, where values over 0.5 will be treated as predicting default.\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks3.png' alt='img' width='740'>\n",
    "  \n",
    "**A simple example**\n",
    "  \n",
    "But what if we apply a sigmoid activation function? The impact of bill amount on default now depends on the borrower's age. In particular, we can see that the change in the predicted value for default is larger for young borrowers than it is for old borrowers.\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks4.png' alt='img' width='740'>\n",
    "  \n",
    "**The sigmoid activation function**\n",
    "  \n",
    "In this course, we'll use the three most common activation functions: sigmoid, relu, and softmax. The sigmoid activation function is used primarily in the output layer of binary classification problems. When we use the low-level approach, we'll pass the sum of the product of weights and inputs into `tf.keras.activations.sigmoid()`. When we use the high-level approach, we'll simply pass `'sigmoid'` as a parameter to a `keras.Dense.layer()`.\n",
    "  \n",
    "- High-level: `tf.keras.activations.sigmoid()`\n",
    "- Low-level: `keras.Dense.layer(activation='sigmoid')`\n",
    "- Primarily used in the output layer of a binary classification problem\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks5.png' alt='img' width='740'>\n",
    "  \n",
    "**The relu activation function**\n",
    "  \n",
    "We'll typically use the rectified linear unit or relu activation in *all* layers other than the output layer. This activation simply takes the maximum of the value passed to it and 0.\n",
    "  \n",
    "- High-level: `tf.keras.activations.relu()`\n",
    "- Low-level: `keras.Dense.layer(activation='relu')`\n",
    "- Typically used in all layers\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks6.png' alt='img' width='740'>\n",
    "  \n",
    "**The softmax activation function**\n",
    "  \n",
    "Finally, the softmax activation function is used in the output layer in classification problems with more than two classes. The outputs from a softmax activation function can be interpreted as predicted class probabilities in multiclass classification problems.\n",
    "  \n",
    "- High-level: `tf.keras.activations.softmax()`\n",
    "- Low-level: `keras.Dense.layer(activation='softmax')`\n",
    "- Output layer (>2 classes)\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks7.png' alt='img' width='385'>\n",
    "  \n",
    "**Activation functions in neural networks**\n",
    "  \n",
    "Let's wrap up by applying some activation functions in a neural network. We'll do this using the high-level approach, starting with an input layer. We'll pass this to our first dense layer, which has 16 output nodes and a relu activation. Dense layer 2 then reduces the number of nodes from 16 to 8 and applies a sigmoid activation. Finally, we apply a softmax activation function in the output layer, since there are more than 2 outputs. We've now seen what an activation function is and how to use the most common activation functions.\n",
    "  \n",
    "<img src='../_images/activation-functions-in-neural-networks8.png' alt='img' width='740'>\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification problems\n",
    "  \n",
    "In this exercise, you will again make use of credit card data. The target variable, `default`, indicates whether a credit card holder defaults on his or her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, `outputs`, and compare those the target variable, `default`.\n",
    "  \n",
    "The tensor of features has been loaded and is available as `bill_amounts`. Additionally, the `tf.constant()`, `tf.float32`, and `tf.keras.layers.Dense()` operations are available.\n",
    "  \n",
    "1. Define `inputs` as a 32-bit floating point constant tensor using bill_amounts.\n",
    "2. Set `dense1` to be a dense layer with 3 output nodes and a `relu` activation function.\n",
    "3. Set `dense2` to be a dense layer with 2 output nodes and a `relu` activation function.\n",
    "4. Set the output layer to be a dense layer with a single output node and a `sigmoid` activation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network node schema**\n",
    "  \n",
    "Input layer:  \n",
    "3 nodes (['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3'])  \n",
    "\n",
    "Hidden layer1:  \n",
    "3 nodes  \n",
    "  \n",
    "Hidden layer2:  \n",
    "2 nodes  \n",
    "  \n",
    "Output layer:  \n",
    "1 node (y_pred for default)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
       "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default.payment.next.month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_amounts = df[['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3']].to_numpy()\n",
    "default = df[['default.payment.next.month']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from features\n",
    "inputs = tf.constant(bill_amounts, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples; y_true - y_pred\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code several times, you'll notice that the errors change each time. This is because you're using an untrained model with randomly initialized parameters. Furthermore, the errors fall on the interval between -1 and 1 because `default` is a binary variable that takes on values of 0 and 1 and `outputs` is a probability between 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification problems\n",
    "  \n",
    "In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns.\n",
    "  \n",
    "As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model's predictions, which are probabilities assigned to the classes. The tensor of features has been loaded and is available as `borrower_features`. Additionally, the `tf.constant()`, `tf.float32`, and `tf.keras.layers.Dense()` operations are available.\n",
    "  \n",
    "1. Define the input layer as a 32-bit constant tensor using `borrower_features`.\n",
    "2. Set the first dense layer to have 10 output nodes and a sigmoid activation function.\n",
    "3. Set the second dense layer to have 8 output nodes and a rectified linear unit activation function.\n",
    "4. Set the output layer to have 6 output nodes and the appropriate activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5']\n"
     ]
    }
   ],
   "source": [
    "features = df.columns[1:11].tolist()\n",
    "print(features)\n",
    "borrower_features = df[features].values\n",
    "\n",
    "# Converting to borrower observations and features to flow of tensors object, recommended over tf.cast()\n",
    "borrower_features = tf.convert_to_tensor(borrower_features, tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network node schema**\n",
    "  \n",
    "Input layer:  \n",
    "10 nodes (['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5'])  \n",
    "\n",
    "Hidden layer1:  \n",
    "10 nodes  \n",
    "  \n",
    "Hidden layer2:  \n",
    "8 nodes  \n",
    "  \n",
    "Output layer:  \n",
    "6 node (y_pred for education level)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13387074 0.11894083 0.18608409 0.18104851 0.25339946 0.12665643]\n",
      " [0.13387074 0.11894083 0.18608409 0.18104851 0.25339946 0.12665643]\n",
      " [0.13387074 0.11894083 0.18608409 0.18104851 0.25339946 0.12665643]\n",
      " [0.13387074 0.11894083 0.18608409 0.18104851 0.25339946 0.12665643]\n",
      " [0.13387074 0.11894083 0.18608409 0.18104851 0.25339946 0.12665643]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from borrower features\n",
    "inputs = tf.constant(borrower_features, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row of `outputs` sums to one. This is because a row contains the predicted class probabilities for one example. As with the previous exercise, our predictions are not yet informative, since we are using an untrained model with randomly initialized parameters. This is why the model tends to assign similar probabilities to each class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "  \n",
    "In chapter 2, you minimized a loss function with an optimizer. We'll revisit that here in the context of training neural networks. This entails finding the set of weights that corresponds to the minimum value of the loss.\n",
    "  \n",
    "**How to find a minimum**\n",
    "  \n",
    "So what is a minimization problem? And what can go wrong when we try to solve one? Let's start with a simple thought experiment: you want to find the lowest point in the Grand Canyon, but all you can do is pick a point, measure the elevation, and then repeat the same to nearby points. This is what you do when you train a neural network: you pick a starting point, measure the loss, and then try to move to a lower loss. We will see how a common optimization algorithm, gradient descent, solves this problem.\n",
    "  \n",
    "Let's start by picking a point and measuring the elevation. From that point, we'll move along the slope until we arrive on a flat surface. To understand what's going on, imagine you dropped a ball into the canyon from the point you selected. If you drop the ball on a slope above a plateau, the ball will stop when it reaches the plateau. If this happens, the gradient descent algorithm will fail. It will stop on a local minimum and will progress no further.\n",
    "  \n",
    "Let's say you pick a different spot. This time, the ball lands on a slope with an unobstructed path to the lowest point in the canyon. Here, the gradient descent algorithm works and ball reaches the global minimum. Notice that gravity performs the role of the gradient descent optimizer.\n",
    "  \n",
    "**Stochastic Gradient Descent**\n",
    "  \n",
    "Stochastic gradient descent or `SGD` is an improved version of gradient descent that is less likely to get stuck in local minima. For simple problems, the `SGD` algorithm performs well. Here, the `SGD` loss function value quickly falls below the losses for the more recently developed `RMSprop` and the `Adam` optimizers on a simple minimization task. `Adam` and `RMS` require 10 times as many iterations to achieve a similar loss.\n",
    "  \n",
    "<img src='../_images/optimization-algorithms-for-neural-networks.png' alt='img' width='740'>\n",
    "  \n",
    "**The Gradient Descent optimizer**\n",
    "  \n",
    "Let's move on to the TensorFlow implementation for these optimizers, starting with `SGD`, which you can instantiate using the `tensorflow.keras.optimizers.SGD` module. You can then supply a `learning_rate=`, typically between 0.5 and 0.001, which will determine how quickly the model parameters adjust during training. Think of a higher `learning_rate=` as exerting more force on the ball than gravity alone. The ball will move faster and skip over some plateaus, but it may miss the global minimum, too. The main advantage of `SGD` is that it is simpler and easier to interpret than more modern optimization algorithms.\n",
    "  \n",
    "**Stochastic Gradient Descent (`SGD`) Optimizer**  \n",
    "  \n",
    "- `tensorflow.keras.optimizers.SGD()`\n",
    "- `learning_rate=`, typically between 0.5 and 0.001\n",
    "- Simpler and easier to interpret than more modern optimization algorithms\n",
    "  \n",
    "**The RMS prop optimizer**\n",
    "  \n",
    "Next, we'll consider the Root Mean Squared (`RMS`) propagation optimizer, which has two advantages over `SGD`. First, it applies different learning rates to each feature, which can be useful for high dimensional problems. And second, it allows you to both build `momentum` and also allow it to `decay`. Setting a low value for the `decay` parameter will prevent `momentum` from accumulating over long periods during the training process.\n",
    "  \n",
    "**Root Mean Squared (`RMS`) Propagation Optimizer**  \n",
    "  \n",
    "- `tensorflow.keras.optimizers.RMSprop()`\n",
    "- `learning_rate=`\n",
    "- `momentum=`\n",
    "- `decay=`\n",
    "- Allows for momentum to both build and decay\n",
    "- Applies different learning rates to each feature\n",
    "  \n",
    "**The Adam optimizer**\n",
    "  \n",
    "Finally, the Adaptive Moment or \"Adam\" optimizer provides further improvements and is generally a good first choice. Similar to `RMSprop`, you can set the `momentum` to `decay` faster by lowering the `beta1=` parameter. Relative to `RMSprop`, the `Adam` optimizer will tend to perform better with the default parameter values, which we will typically use.\n",
    "  \n",
    "**Adaptive Moment (`Adam`) Optimizer**  \n",
    "  \n",
    "- `tensorflow.keras.optimizers.Adam()`\n",
    "- `learning_rate=`\n",
    "- `beta1=`, set momentum to decay faster by lowering beta1\n",
    "- Performs well with default parameters\n",
    "  \n",
    "**A complete example**\n",
    "  \n",
    "Let's return to our credit card default prediction problem and assume that features have been imported and weights have been initialized. We'll then define a model that computes the predictions and a loss function that computes the `binary_crossentropy` loss, which is the standard for binary classification problems. Finally, we define an `RMSprop` optimizer with parameters `learning_rate=0.1` and a `momentum=0.9`, and then perform minimization.\n",
    "  \n",
    "<img src='../_images/optimization-algorithms-for-neural-networks1.png' alt='img' width='740'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dangers of local minima\n",
    "  \n",
    "Consider the plot of the following loss function, `loss_function()`, which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "  \n",
    "<img src='../_images/optimization-algorithms-for-neural-networks2.png' alt='img' width='430'>\n",
    "  \n",
    "In this exercise, you will try to find the global minimum of `loss_function()` using `tensorflow.keras.optimizers.SGD()`. You will do this twice, each time with a different initial value of the input to `loss_function()`. First, you will use `x_1`, which is a variable with an initial value of 6.0. Second, you will use `x_2`, which is a variable with an initial value of 0.3. Note that `loss_function()` has been defined and is available.\n",
    "  \n",
    "1. Set `opt` to use the stochastic gradient descent optimizer (`SGD`) with a learning rate of 0.01.\n",
    "2. Perform minimization using the loss function, `loss_function()`, and the variable with an initial value of 6.0, `x_1`.\n",
    "3. Perform minimization using the loss function, `loss_function()`, and the variable with an initial value of 0.3, `x_2`.\n",
    "4. Print `x_1` and `x_2` as `numpy` arrays and check whether the values differ. These are the minima that the algorithm identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Loss function\n",
    "def loss_function(x):\n",
    "    return 4.0 * math.cos(x - 1) + math.cos(2.0 * math.pi * x) / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global | local\n",
      "6.027515 0.25\n"
     ]
    }
   ],
   "source": [
    "# Initializing x_1 and x_2\n",
    "x_1 = tf.Variable(6.0, tf.float32)\n",
    "x_2 = tf.Variable(0.3, tf.float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Build the optimizer with the variables\n",
    "opt.build([x_1, x_2])\n",
    "\n",
    "for j in range(100):\n",
    "    # Perfrom minimization using the loss function and x_1\n",
    "    opt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    \n",
    "    # Perform minimization using the loss function and x_2\n",
    "    opt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print('global | local')\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "<script.py> output:\n",
    "    4.3801394 0.42052683\n",
    "```\n",
    "Notice that we used the same optimizer and loss function, but two different initial values. When we started at 6.0 with `x_1`, we found the global minimum at 4.38?, marked by the dot on the right. When we started at 0.3, we stopped around 0.42? with `x_2`, the local minimum marked by a dot on the far left."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#7393B3'>NOTE:</span> The loss function was not defined or explained so what I did was use the code snippet below in order to extract the source lines of the function, this function however produced results that were different than the exercise, it is possible that exercise is just outdated.\n",
    "  \n",
    "```python\n",
    "In [11]:\n",
    "import inspect\n",
    "In [12]:\n",
    "print(inspect.getsourcelines(loss_function))\n",
    "(['def loss_function(x):\\n', '\\treturn 4.0*math.cos(x-1)+divide(math.cos(2.0*pi*x),x)\\n'], 5)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style='color:#E74C3C'>WARNING:</span> KeyError: 'The optimizer cannot recognize variable Variable:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.SGD.'  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get this KeyError, all you have to do is notify the optimizer of the variables via a codeline of:\n",
    "  \n",
    "```python\n",
    "optimizer.build(<variables>)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding local minima\n",
    "  \n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as `loss_function()`.\n",
    "  \n",
    "<img src='../_images/optimization-algorithms-for-neural-networks2.png' alt='img' width='430'>\n",
    "  \n",
    "Several optimizers in `tensorflow` have a `momentum=` parameter, including `SGD` and `RMSprop`. You will make use of `RMSprop` in this exercise. Note that `x_1` and `x_2` have been initialized to the same value this time. Furthermore, `tensorflow.keras.optimizers.RMSprop()` has also been imported for you from `tensorflow`.\n",
    "  \n",
    "1. Set the `opt_1` operation to use a `learning_rate=` of 0.01 and a `momentum=` of 0.99.\n",
    "2. Set `opt_2` to use the Root Mean Square Propagation (`RMS`) optimizer with a learning rate of 0.01 and a momentum of 0.00.\n",
    "3. Define the minimization operation for `opt_2`.\n",
    "Print `x_1` and `x_2` as `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.744512 0.24999999\n"
     ]
    }
   ],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = tf.Variable(0.05,tf.float32)\n",
    "x_2 = tf.Variable(0.05,tf.float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = tf.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = tf.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "# Notify the optimizers to recognize the variables\n",
    "opt_1.build([x_1])\n",
    "opt_2.build([x_2])\n",
    "\n",
    "for j in range(100):\n",
    "    # Define the minimization operation for opt_1\n",
    "    opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "    opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the global minimum is approximately 4.38?. Notice that `opt_1` built momentum, bringing `x_1` closer to the global minimum. To the contrary, `opt_2`, which had a momentum parameter of 0.0, got stuck in the local minimum on the left of the graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#7393B3'>NOTE:</span> Despite the exercise having incorrect values in its proof, one thing does hold true, which makes the exercise still valid even now. \n",
    "  \n",
    "Observe the two runs we have:\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Run</th>\n",
    "    <th>Global</th>\n",
    "    <th>Local</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>6.027515</td>\n",
    "    <td>0.25</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>2.744512</td>\n",
    "    <td>0.24999999</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "Here we can see that the local minima is equivalent, showcasing the existance of it. We can also see that we did in-fact 'roll' out of the local minima by applying some momentum to the optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a network in TensorFlow\n",
    "  \n",
    "In the final video in this chapter, we'll wrap-up by discussing important topics related to training neural networks in TensorFlow.\n",
    "  \n",
    "**Initializing variables**\n",
    "  \n",
    "We saw that finding the global minimum can be difficult, even when we're minimizing a simple loss function. We also saw that we could improve our chances by selecting better initial values for variables. But what can we do for more challenging problems with many variables? Take the eggholder function, for example, which has many local minima. It is difficult to see a global minimum on the plot, but it has one. How can we select initial values for $x$ and $y$, the two inputs to the eggholder function? Even worse, what if we have a loss function that depends on hundreds of variables?\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow.png' alt='img' width='740'>\n",
    "  \n",
    "**Random initializers**\n",
    "  \n",
    "We often need to initialize hundreds or thousands of variables. Simply using `tf.ones()` will not work. And selecting initial values individually is tedious and infeasible in many cases. A natural alternative to this is to use random or algorithmic generation of initial values. We can, for instance, draw them from a probability distribution, such as the normal or uniform distributions. There are also specialized options, such as the Glorot initializers, which are designed for ML algorithms.\n",
    "  \n",
    "**Random initializers**\n",
    "  \n",
    "- Often we need to initialize thousands of variables\n",
    "- `tf.ones()` may preform poorly\n",
    "- Tedious and difficult to initialize variables individually\n",
    "  \n",
    "**Alternatively, we can draw initial values from distributions**\n",
    "  \n",
    "- Normal\n",
    "- Uniform\n",
    "- Glorot initializer, designed for ML algorithms\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow1.png' alt='img' width='740'>\n",
    "  \n",
    "**Initializing variables in TensorFlow**\n",
    "  \n",
    "Let's start by using the low-level approach to initialize a 500x500 variable. We can do this using draws from a random normal distribution by passing the shape 500, 500 to `tf.random.normal()` and passing the result to `tf.Variable()`. Alternatively, we could use the truncated random normal distribution, which discards very large and very small draws.\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow2.png' alt='img' width='740'>\n",
    "  \n",
    "**Initializing variables in TensorFlow**\n",
    "  \n",
    "We can also use the high-level approach by initializing a dense layer using the default `keras` option, *currently the glorot uniform initializer*, as we've done in all exercises thus far. If we instead wish to initialize values to zero, we can do this using the `kernel_initializer=` parameter.\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow3.png' alt='img' width='740'>\n",
    "  \n",
    "**Neural networks and overfitting**\n",
    "  \n",
    "Overfitting is another important issue you'll encounter when training neural networks. Let's say you have a linear relationship between two variables. You decide to represent this relationship with a linear model, shown in red, and a more complex model, shown in blue. The complex model perfectly predicts the values in the training set, but performs worse in the test set. The complex model performed poorly because it overfit. It simply memorized examples, rather than learning general patterns. Overfitting is especially problematic for neural networks, which contain many parameters and are quite good at memorization.\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow4.png' alt='img' width='740'>\n",
    "  \n",
    "**Applying dropout**\n",
    "  \n",
    "A simple solution to the overfitting problem is to use dropout, an operation that will randomly drop the weights connected to certain nodes in a layer during the training process, as shown on the right. This will force your network to develop more robust rules for classification, since it cannot rely on any particular nodes being passed to an activation function. This will tend to improve out-of-sample performance.\n",
    "  \n",
    "- Applying dropout forces the network to make more robust rules for classification\n",
    "- Improves out-of-sample performance\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow5.png' alt='img' width='740'>\n",
    "  \n",
    "**Implementing dropout in a network**\n",
    "  \n",
    "Let's look at how dropout works. We first define an input layer using the borrower features from our credit card dataset as an input. We then pass the input layer to a dense layer, which has 32 nodes and uses a relu activation function.\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow6.png' alt='img' width='740'>\n",
    "  \n",
    "**Implementing dropout in a network**\n",
    "  \n",
    "We'll next pass the first dense layer to a second layer, which reduces the number of output nodes to 16. Before passing those nodes to the output layer, we'll apply a dropout layer. The only argument specifies that we want to drop the weights connected to 25% of nodes randomly. We'll then pass this to the output layer, which reduces the 16 nodes to 1 and applies a sigmoid activation function.\n",
    "  \n",
    "<img src='../_images/training-a-network-in-tensorflow7.png' alt='img' width='740'>\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization in TensorFlow\n",
    "  \n",
    "A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level `keras` operations. We will also expand the set of input features from 3 to 23. Several operations have been imported from `tensorflow`: `tf.Variable()`, `tf.random()`, and `tf.ones()`.\n",
    "  \n",
    "1. Initialize the layer 1 weights, `w1`, as a `tf.Variable()` with shape [23, 7], drawn from a normal distribution.\n",
    "2. Initialize the layer 1 bias using `tf.ones()`.\n",
    "3. Use a draw from the normal distribution to initialize `w2` as a `tf.Variable()` with shape [7, 1].\n",
    "4. Define b2 as a `tf.Variable()` and set its initial value to 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights, mean is 0 and normal dist is Â± std, however range can be Â± inf\n",
    "w1 = tf.Variable(tf.random.normal([23, 7]), tf.float32)\n",
    "\n",
    "# Initialize the layer 1 bias, all 1's\n",
    "b1 = tf.Variable(tf.ones([7]), tf.float32)\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = tf.Variable(tf.random.normal([7, 1]), tf.float32)\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = tf.Variable(0.0, tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercise, you will start where we've ended and will finish constructing the neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model and loss function\n",
    "  \n",
    "In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as `borrower_features` and `default`. You defined the weights and biases in the previous exercise.\n",
    "  \n",
    "Note that the `predictions` layer is defined as $\\sigma(layer1*w2+b2)$ where $\\sigma$ is the sigmoid activation, `layer1` is a tensor of nodes for the first hidden dense layer, `w2` is a tensor of weights, and `b2` is the bias tensor.\n",
    "  \n",
    "The trainable variables are `w1`, `b1`, `w2`, and `b2`. Additionally, the following operations have been imported for you: `tensorflow.keras.activations.relu()` and `tensorflow.keras.layers.Dropout()`.\n",
    "  \n",
    "1. Apply a rectified linear unit activation function to the first layer.\n",
    "2. Apply 25% dropout to `layer1`.\n",
    "3. Pass the target, `targets`, and the predicted values, `predictions`, to the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#X/y split\n",
    "X = df.iloc[:3000 ,1:24].astype(np.float32).to_numpy()\n",
    "y = df.iloc[:3000, 24].astype(np.float32).to_numpy()\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "borrower_features, test_features, borrower_targets, test_targets = train_test_split(X, \n",
    "                                                                                    y, \n",
    "                                                                                    test_size=0.25, \n",
    "                                                                                    stratify=y)\n",
    "\n",
    "# Reshaping y_train\n",
    "borrower_targets = tf.reshape(borrower_targets, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features=borrower_features):\n",
    "    # Apply relu activation function to layer 1\n",
    "    layer1 = tf.keras.activations.relu(tf.matmul(features, w1) + b1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    dropout = tf.keras.layers.Dropout(0.25)(layer1)\n",
    "    return tf.keras.activations.sigmoid(tf.matmul(dropout, w2) + b2)\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features=borrower_features, targets = borrower_targets):\n",
    "    predictions = model(w1, b1, w2, b2)\n",
    "    \n",
    "    # Pass targets and predictions to the cross entropy loss\n",
    "    return tf.keras.losses.binary_crossentropy(targets, predictions)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of using `tensorflow` is that you have the option to customize models down to the linear algebraic-level, as we've shown in the last two exercises. If you print `w1`, you can see that the objects we're working with are simply tensors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#7393B3'>NOTE:</span> General information on binary cross-entropy\n",
    "  \n",
    "`tf.keras.metrics.binary_crossentropy()` is a loss function commonly used in binary classification tasks. It measures the dissimilarity or error between predicted probabilities and `True` binary labels.\n",
    "  \n",
    "To put it simply, `binary_crossentropy()` quantifies how well a model's predicted probabilities match the true labels in a binary classification problem. It calculates the average \"distance\" or discrepancy between the predicted probabilities and the actual labels.\n",
    "  \n",
    "In other words, `binary_crossentropy()` helps determine how accurate the predicted probabilities are in representing the `True` binary labels. The lower the binary cross-entropy value, the better the model's predictions align with the `True` labels. The loss value is minimized during the training process to improve the model's performance.\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span> When to use  \n",
    "  \n",
    "Binary cross-entropy is typically used when there are only two possible classes or labels, such as classifying an email as spam or not spam. If you have multiple classes, you would use `categorical_crossentropy()` instead.\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span> Operations preformed  \n",
    "  \n",
    "Binary cross-entropy is calculated by comparing the predicted probabilities generated by a model with the true binary labels. Here's a simplified explanation of how it works:\n",
    "  \n",
    "1. For each instance or sample in the dataset, the model produces a predicted probability between 0 and 1. This probability represents the model's confidence that the instance belongs to the positive class (e.g., class 1 or \"True\") in a binary classification problem.\n",
    "  \n",
    "2. The true binary label for each instance is either 0 or 1, indicating the actual class membership.\n",
    "  \n",
    "3. Binary cross-entropy measures the dissimilarity between the predicted probabilities and the true labels. It penalizes large errors and rewards accurate predictions.\n",
    "  \n",
    "4. To calculate the binary cross-entropy, the predicted probabilities and true labels are compared using the following formula:\n",
    "  \n",
    "   - For a positive class instance (true label = 1): \n",
    "     - If the predicted probability is close to 1, the loss is low.\n",
    "     - If the predicted probability is close to 0, the loss is high.\n",
    "\n",
    "   - For a negative class instance (true label = 0): \n",
    "     - If the predicted probability is close to 0, the loss is low.\n",
    "     - If the predicted probability is close to 1, the loss is high.\n",
    "  \n",
    "5. The binary cross-entropy loss is calculated for each instance, and then the average loss is taken across all instances in the dataset.\n",
    "  \n",
    "6. During the training process, the goal is to minimize the binary cross-entropy loss by adjusting the model's parameters (weights and biases) through techniques like gradient descent. This optimization process helps the model learn to make better predictions and improve its performance on the binary classification task.\n",
    "  \n",
    "By minimizing the binary cross-entropy loss, the model learns to generate predicted probabilities that are more aligned with the true labels, improving its ability to classify instances accurately as either the positive or negative class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural networks with TensorFlow\n",
    "  \n",
    "In the previous exercise, you defined a model, `model(w1, b1, w2, b2, features)`, and a loss function, `loss_function(w1, b1, w2, b2, features, targets)`, both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of `test_features` and `test_targets` and is available to you. The trainable variables are `w1`, `b1`, `w2`, and `b2`. Additionally, the following operations have been imported for you: `tensorflow.keras.activations.relu()` and `tensorflow.keras.layers.Dropout()`.\n",
    "  \n",
    "1. Set the optimizer to perform minimization.\n",
    "2. Add the four trainable variables to `var_list=` in the order in which they appear as arguments to `loss_function()`.\n",
    "3. Use the model and `test_features` to predict the values for `test_targets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the optimizer used in the exercise\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[583,   0],\n",
       "       [166,   1]], dtype=int32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "for j in range(1000):\n",
    "    # Complete the optimizer\n",
    "    opt.minimize(lambda: loss_function(w1, b1, w2, b2), var_list=[w1, b1, w2, b2])\n",
    "    \n",
    "\n",
    "# Make predictions with model\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "tf.math.confusion_matrix(test_targets, model_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram shown is called a ``confusion matrix.'' The diagonal elements show the number of correct predictions. The off-diagonal elements show the number of incorrect predictions. We can see that the model performs reasonably-well, but does so by overpredicting non-default. This suggests that we may need to train longer, tune the model's hyperparameters, or change the model's architecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_plot(default, model_predictions):\n",
    "    # Binder for y_true and y_pred, contains binary threshold\n",
    "    df = pd.DataFrame(np.hstack([default, model_predictions.numpy() > 0.5]),\n",
    "                      columns = ['Actual','Predicted'])\n",
    "    # Mapping\n",
    "    confusion_matrix = pd.crosstab(df['Actual'], \n",
    "                                   df['Predicted'], \n",
    "                                   rownames=['Actual'], colnames=['Predicted'])\n",
    "    # Plots confusion matrix via sns.heatmap\n",
    "    sns.heatmap(confusion_matrix, cmap=\"Greys\", fmt=\"d\", annot=True, cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_plot(test_targets.reshape(-1,1), model_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
