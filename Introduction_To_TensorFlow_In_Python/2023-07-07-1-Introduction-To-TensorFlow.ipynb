{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow\n",
    "  \n",
    "Not long ago, cutting-edge computer vision algorithms couldn’t differentiate between images of cats and dogs. Today, a skilled data scientist equipped with nothing more than a laptop can classify tens of thousands of objects with greater accuracy than the human eye. In this course, you will use TensorFlow 2.6 to develop, train, and make predictions with the models that have powered major advances in recommendation systems, image classification, and FinTech. You will learn both high-level APIs, which will enable you to design and train deep learning models in 15 lines of code, and low-level APIs, which will allow you to move beyond off-the-shelf routines. You will also learn to accurately predict housing prices, credit card borrower defaults, and images of sign language gestures.\n",
    "  \n",
    "Before you can build advanced models in TensorFlow 2, you will first need to understand the basics. In this chapter, you’ll learn how to define constants and variables, perform tensor addition and multiplication, and compute derivatives. Knowledge of linear algebra will be helpful, but not necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "[TensorFlow Documentation](https://www.tensorflow.org)  \n",
    "[Linear Algebra Course - Youtube](https://www.youtube.com/watch?v=JnTa9XtvmfI)  \n",
    "[What is a Tensor? - University of Cambridge](https://www.doitpoms.ac.uk/tlplib/tensors/what_is_tensor.php#:~:text=Tensors%20are%20simply%20mathematical%20objects,is%20a%20first%20rank%20tensor.)  \n",
    "[Tensor - Wikipedia](https://en.wikipedia.org/wiki/Tensor)  \n",
    "[TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.20148&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)  \n",
    "[Tensor LaTex Formulas](https://www.math-linux.com/latex-26/faq/latex-faq/article/latex-tensor-product)  \n",
    "[HTML Color Codes](https://htmlcolorcodes.com)  \n",
    "[Matplotlib Color Maps](https://matplotlib.org/stable/tutorials/colors/colormaps.html)  \n",
    "  \n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.constant()</td>\n",
    "    <td>Creates a constant tensor with a specified value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.Variable()</td>\n",
    "    <td>Creates a mutable tensor variable that can be modified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.zeros()</td>\n",
    "    <td>Creates a tensor filled with zeros.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.ones()</td>\n",
    "    <td>Creates a tensor filled with ones.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.zeros_like()</td>\n",
    "    <td>Creates a tensor of zeros with the same shape as another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.ones_like()</td>\n",
    "    <td>Creates a tensor of ones with the same shape as another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.fill()</td>\n",
    "    <td>Creates a tensor filled with a specified scalar value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.add()</td>\n",
    "    <td>Performs element-wise addition of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.multiply()</td>\n",
    "    <td>Performs element-wise multiplication of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.matmul()</td>\n",
    "    <td>Performs matrix multiplication of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.reduce_sum()</td>\n",
    "    <td>Computes the sum of elements across specified dimensions of a tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.gradient()</td>\n",
    "    <td>Computes the gradients of a tensor with respect to another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.GradientTape</td>\n",
    "    <td>Records operations for automatic differentiation to compute gradients.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.reshape()</td>\n",
    "    <td>Reshapes a tensor into a specified shape.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.random()</td>\n",
    "    <td>Generates random values from a specified distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.random().uniform()</td>\n",
    "    <td>Generates random values from a uniform distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.GradientTape.watch()</td>\n",
    "    <td>Used to start tracing Tensor by the Tape</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 23:45:29.315679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import tensorflow as tf             # TensorFlow:               Deep-Learning Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and variables\n",
    "  \n",
    "This is a course on the fundamentals of the TensorFlow API in Python. In our first video, we will briefly introduce TensorFlow and then discuss its two basic objects of computation: constants and variables.\n",
    "  \n",
    "**What is TensorFlow?**\n",
    "  \n",
    "TensorFlow is an open-source library for graph-based numerical computation. It was developed by the Google Brain Team. It has both low and high level APIs. You can use TensorFlow to perform addition, multiplication, and differentiation. You can also use it to design and train machine learning models. TensorFlow 2.0 brought with it substantial changes. Eager execution is now enabled by default, which allows users to write simpler and more intuitive code. Additionally, model building is now centered around the Keras and Estimators high-level APIs.\n",
    "  \n",
    "**Tensorflow**  \n",
    "  \n",
    "- Open-source library for graph-based numerical computation  \n",
    "- Low and high level API  \n",
    "- Addition, multiplication, differentiation  \n",
    "- Machine Learning models  \n",
    "  \n",
    "**In v2.0**  \n",
    "  \n",
    "- Eager execution by default  \n",
    "- Model building with Keras and Estimators  \n",
    "  \n",
    "**What is a tensor?**\n",
    "  \n",
    "The TensorFlow documentation describes a tensor as \"a generalization of vectors and matrices to potentially higher dimensions.\" Now, if you are not familiar with linear algebra, you can simply think of a tensor as a collection of numbers, which is arranged into a particular shape.\n",
    "  \n",
    "**Tensor**\n",
    "  \n",
    "- Generalization of vectors and matrices  \n",
    "- Collection of numbers  \n",
    "- Specific shape  \n",
    "- Scaler -> Vector -> Matrix -> Tensor \n",
    "  \n",
    "**What is a tensor?**\n",
    "  \n",
    "As an example, let's say you have a slice of bread and you cut it into 9 pieces. One of those 9 pieces is a 0-dimensional tensor. This corresponds to a single number. A collection of 3 pieces that form a row or column is a 1-dimensional tensor. All 9 pieces together are a 2-dimensional tensor. And the whole loaf, which contains many slices, is a 3-dimensional tensor.\n",
    "  \n",
    "**Defining tensors in TensorFlow**\n",
    "  \n",
    "Now that you know what a tensor is, let's define a few. We will start by importing `tensorflow as tf`. We will then define 0-, 1-, 2-, and 3-dimensional tensors. Note that each object will be a `tf.Tensor` object.\n",
    "  \n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#0D Tensor\n",
    "d0 = tf.ones((1,))\n",
    "\n",
    "#1D Tensor\n",
    "d1 = tf.ones((2,))\n",
    "\n",
    "#2D Tensor\n",
    "d2 = tf.ones((2, 2))\n",
    "\n",
    "#3D Tensor\n",
    "d3 = tf.ones((2, 2, 2))\n",
    "```\n",
    "  \n",
    "**Defining tensors in TensorFlow**\n",
    "  \n",
    "If we want to print the array contained in that object, we can apply the `.numpy()` method and pass the resulting object to the `print()` function.\n",
    "  \n",
    "```python\n",
    "In [2]: print(d0.numpy())\n",
    "[1.]\n",
    "\n",
    "In [3]: print(d1.numpy())\n",
    "[1. 1.]\n",
    "\n",
    "In [4]: print(d2.numpy())\n",
    "[[1. 1.]\n",
    " [1. 1.]]\n",
    "\n",
    "In [5]: print(d3.numpy())\n",
    "[[[1. 1.]\n",
    "  [1. 1.]]\n",
    "\n",
    " [[1. 1.]\n",
    "  [1. 1.]]]\n",
    "```\n",
    "  \n",
    "**Defining constants in TensorFlow**\n",
    "  \n",
    "We next move on to constants, which are the simplest category of tensor in TensorFlow. A `constant` does not change and cannot be trained. It can, however, have any dimension. In the code block, we've defined two constants. The constant `a` is a 2x3 tensor of 3s. The constant `b` is a 2x2 tensor, which is constructed from the 1-dimensional tensor: [1, 2, 3, 4].\n",
    "  \n",
    "**Constants in TensorFlow**  \n",
    "  \n",
    "- Simplest category of tensor in TensorFlow  \n",
    "- Constants do NOT change and can NOT be trained  \n",
    "- Can have any dimension  \n",
    "  \n",
    "```python\n",
    "In [8]: import tensorflow as tf\n",
    "\n",
    "In [9]: #Define a 2x3 constant\n",
    "   ...: a = tf.constant(3, shape=[2,3])\n",
    "\n",
    "In [10]: # Define a 2x2 constant\n",
    "    ...: b = tf.constant([1, 2, 3, 4], shape=[2, 2])\n",
    "\n",
    "In [11]: print(a)\n",
    "tf.Tensor(\n",
    "[[3 3 3]\n",
    " [3 3 3]], shape=(2, 3), dtype=int32)\n",
    "\n",
    "In [12]: print(b)\n",
    "tf.Tensor(\n",
    "[[1 2]\n",
    " [3 4]], shape=(2, 2), dtype=int32)\n",
    "```\n",
    "  \n",
    "**Using convenience functions to define constants**\n",
    "  \n",
    "In the previous slide, we worked exclusively with the `tf.constant()` operation. However, in some cases, there are more convenient options for defining certain types of special tensors. You can use the `tf.zeros()` or `tf.ones()` operations to generate a tensor of arbitrary dimension that is populated entirely with zeros or ones. You can use the `tf.zeros_like()` or `tf.ones_like()` operations to populate tensors with zeros and ones, copying the dimension of some input tensor. Finally, you can use the `tf.fill()` operation to populate a tensor of arbitrary dimension with the same scalar value in each element.\n",
    "  \n",
    "**Defining and initializing variables**\n",
    "  \n",
    "Unlike a constant, a variable's value can change during computation. The value of `a` variable is shared, persistent, and modifiable. However, its data type and shape are fixed. Let's take a look at how variables are constructed and used in TensorFlow. In the code, we first define a variable, `a0`, which is a 1-dimensional tensor with 6 elements. We can set its datatype to a 32-bit float or something else, such as a 16-bit int, as we have for `a1`. We then define a constant, `b`. And define c0 as the product of `a0` and `b`. Note that certain TensorFlow operations, such as `tf.multiply()` are *overloaded*, which allows us to use the simpler `a0*b` expression instead.\n",
    "  \n",
    "```python\n",
    "In [13]: import tensorflow as tf\n",
    "    ...: \n",
    "    ...: \n",
    "    ...: # Define a variable\n",
    "    ...: a0 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.float32)\n",
    "    ...: a1 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.int16)\n",
    "    ...: \n",
    "    ...: # Define a constant\n",
    "    ...: b = tf.constant(2, tf.float32)\n",
    "    ...: \n",
    "    ...: # Compute their product\n",
    "    ...: c0 = tf.multiply(a0, b)\n",
    "    ...: \n",
    "    ...: c1 = a0*b\n",
    "    ...: \n",
    "    ...: print(c0)\n",
    "    ...: print(c1)\n",
    "tf.Tensor([ 2.  4.  6.  8. 10. 12.], shape=(6,), dtype=float32)\n",
    "tf.Tensor([ 2.  4.  6.  8. 10. 12.], shape=(6,), dtype=float32)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining data as constants\n",
    "  \n",
    "Throughout this course, we will use tensorflow version 2.6.0 and will exclusively import the submodules needed to complete each exercise. This will usually be done for you, but you will do it in this exercise by importing constant from `tensorflow`.\n",
    "  \n",
    "After you have imported constant, you will use it to transform a `numpy` array, `credit_numpy`, into a tensorflow constant, `credit_constant`. This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters.\n",
    "  \n",
    "Note that tensorflow 2 allows you to use data as either a numpy array or a `tensorflow` `constant` object. Using a constant will ensure that any operations performed with that object are done in tensorflow.\n",
    "  \n",
    "1. Import the `constant` submodule from the `tensorflow` module.\n",
    "2. Convert the `credit_numpy` array into a `constant` object in `tensorflow`. Do not set the data type.\n",
    "  \n",
    "---\n",
    "  \n",
    "NOTE: Features are as follows:  \n",
    "```python\n",
    "{\n",
    "'0': 'education \n",
    "'1': 'marriage'\n",
    "'2': 'age'\n",
    "'3': 'bill_amt'\n",
    "}\n",
    "```  \n",
    "*The dictionary above is what I used to process the `credit` dataframe, originally it was given without feature metadata context. The exercise has the metadata context so I just transformed it so mine will match. Helpful if I ever work with the dataset in the future.*\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>education</th>\n",
       "      <th>marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>bill_amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>46990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>8617.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   education  marriage   age  bill_amt\n",
       "0        2.0       1.0  24.0    3913.0\n",
       "1        2.0       2.0  26.0    2682.0\n",
       "2        2.0       2.0  34.0   29239.0\n",
       "3        2.0       1.0  37.0   46990.0\n",
       "4        2.0       1.0  57.0    8617.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading df\n",
    "credit = pd.read_csv('../_datasets/credit.csv')\n",
    "\n",
    "# Dataframe to numpy\n",
    "credit_numpy = credit.to_numpy()\n",
    "\n",
    "# Printing the shape of the ndarray\n",
    "print(credit_numpy.shape)\n",
    "\n",
    "# Display\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatype is: <dtype: 'float64'>\n",
      "The shape is: (30000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Convert the credit_numpy array into a tensorflow constant object\n",
    "credit_constant = tf.constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('The shape is:', credit_constant.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now understand how constants are used in `tensorflow`. In the following exercise, you'll practice defining variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "  \n",
    "Unlike a constant, a variable's value can be modified. This will be useful when we want to train a model by updating its parameters.\n",
    "  \n",
    "Let's try defining and printing a variable. We'll then convert the variable to a `numpy` array, print again, and check for differences. Note that `tf.Variable()`, which is used to create a variable tensor, has been imported from `tensorflow` and is available to use in the exercise.\n",
    "  \n",
    "1. Define a variable, `A1`, as the 1-dimensional tensor: `[1, 2, 3, 4]`.\n",
    "2. Apply `.numpy()` to `A1` and assign it to `B1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A1:  <tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n",
      "\n",
      " B1:  [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Detine the 1-dimensional variable A1\n",
    "A1 = tf.Variable([1, 2, 3, 4])\n",
    "\n",
    "# Print the variable A1\n",
    "print('\\n A1: ', A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print('\\n B1: ', B1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice any differences between the print statements for `A1` and `B1`? In our next exercise, we'll review how to check the properties of a tensor after it is already defined."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations\n",
    "  \n",
    "In this video, we'll talk about basic operations in TensorFlow.\n",
    "  \n",
    "**What is a TensorFlow operation?**\n",
    "  \n",
    "TensorFlow has a model of computation that revolves around the use of graphs. A TensorFlow graph contains edges and nodes, where the edges are tensors and the nodes are operations.\n",
    "  \n",
    "- Model of computation that revolves around the use of graphs\n",
    "- Graph contains edges and nodes, where edges are tensors and the nodes are operations\n",
    "  \n",
    "<img src='../_images/basic-operations-in-tensorflow.png' alt='img' width='740'>\n",
    "  \n",
    "In the graph shown, which was drawn using TensorFlow, the const operations define 2 by 2 constant tensors. Two tensors are summed using the add operation. Another two tensors are then summed using the add operation. Finally, the resulting matrices are multiplied together with the matmul operation.\n",
    "  \n",
    "**Applying the addition operator**\n",
    "  \n",
    "Let's start with the addition operator. We will first import the constant and add operations. We may now use constant to define 0-dimensional, 1-dimensional, and 2-dimensional tensors.\n",
    "  \n",
    "<img src='../_images/basic-operations-in-tensorflow1.png' alt='img' width='740'>\n",
    "  \n",
    "**Applying the addition operator**\n",
    "  \n",
    "Finally, let's add them together using the operation for tensor addition. Note that we can perform scalar addition with `A0` and `B0`, vector addition with `A1` and `B1`, and matrix addition with `A2` and `B2`.\n",
    "  \n",
    "<img src='../_images/basic-operations-in-tensorflow2.png' alt='img' width='740'>\n",
    "  \n",
    "**Performing tensor addition**\n",
    "  \n",
    "The `tf.add()` operation performs element-wise addition with two tensors. Each pair of tensors added must have the same shape. Element-wise addition of the scalars 1 and 2 yields the scalar 3. Element-wise addition of the vectors [1, 2] and [3, 4] yields the vector [4, 6]. Element-wise addition of the matrices [1, 2, 3, 4] and [5, 6, 7, 8] yields the matrix [6, 8, 10, 12]. Furthermore, the add operator is overloaded, which means that we can also perform addition using the plus symbol.\n",
    "  \n",
    "- The `tf.add()` operation performs *element-wise addition* with two tensors  \n",
    "- The `tf.add()` operator is overloaded for $+$, this means you can use $+$ to preform *element-wise addition* as TensorFlow wrapped/overloaded the standard behavior of the operator $+$ in python. Overloading means wrapping, essencially.\n",
    "  \n",
    "**Element-wise addition**\n",
    "  \n",
    "Scalar addition: $1 + 2 = 3$  \n",
    "  \n",
    "Vector addition: $[1, 2] + [3, 4] = [4, 6]$  \n",
    "  \n",
    "Matrix addition: $\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = \\begin{bmatrix} 6 & 8 \\\\ 10 & 12 \\end{bmatrix}$\n",
    "  \n",
    "**How to perform multiplication in TensorFlow**\n",
    "  \n",
    "We will consider both element-wise and matrix multiplication. For element-wise multiplication, which is performed with the multiply operation, the tensors involved must have the same shape. For instance, you may want to multiply the vector [1, 2, 3] by [3, 4, 5] or [1, 2] by [3, 4]. For matrix multiplication, you use the `matmul` operator. Note that performing `matmul(A,B)` requires that the number of columns of $A$ equal the number of rows of $B$.\n",
    "  \n",
    "**Multiplication in TensorFlow**\n",
    "  \n",
    "- Element-wise multiplication is preformed using `tf.multiply()`\n",
    "- Tensors must have the same shape for element-wise multiplication \n",
    "  \n",
    "- Matrix multiplication is preformed using `tf.matmul()`\n",
    "- Tensor $A$ must have the same number of columns as $B$ has for rows in matrix multiplication\n",
    "  \n",
    "**Applying the multiplication operators**\n",
    "  \n",
    "Let's look at some examples of multiplication in TensorFlow. We'll import the `tf.ones()` operator, along with the two types of multiplication we will use. We will also define a scalar, `A0`, a 3 by 1 vector of ones, a 3 by 4 vector of ones, and a 4 by 3 vector of ones. What operations can be performed using these tensors of ones? We can perform element-wise multiplication of any element by itself, such as `A0` by `A0`, `A31` by `A31`, or `A34` by `A34`. We can also perform matrix multiplication of `A43` by `A34`, but not `A43` by `A43`.\n",
    "  \n",
    "<img src='../_images/basic-operations-in-tensorflow3.png' alt='img' width='740'>\n",
    "  \n",
    "**Summing over tensor dimensions**\n",
    "  \n",
    "Finally, we end this lesson by discussing summation over tensors, which is performed using the `tf.reduce_sum` operator. This can be used to sum over all dimensions of a tensor or just one. Let's see how this works in practice. We will import ones and `reduce_sum` from `tensorflow`. We will then define a 2 by 3 by 4 tensor that consists of ones.\n",
    "  \n",
    "<img src='../_images/basic-operations-in-tensorflow4.png' alt='img' width='740'>\n",
    "  \n",
    "**Summing over tensor dimensions**\n",
    "  \n",
    "If we sum over all elements of $A$, we get 24, since the tensor contains 24 elements, all of which are 1. If we sum over dimension 0, we get a 3 by 4 matrix of 2s. If we sum over 1, we get a 2 by 4 matrix of 3s. And if we sum over 2, we get a 2 by 3 matrix of 4s. In each case, we reduce the size of the tensor by summing over one of its dimensions.\n",
    "  \n",
    "<img src='../_images/basic-operations-in-tensorflow5.png' alt='img' width='740'>\n",
    "  \n",
    "```python\n",
    "In [25]: A = tf.ones([2, 3, 4])\n",
    "\n",
    "In [26]: print(A)\n",
    "tf.Tensor(\n",
    "[[[1. 1. 1. 1.]\n",
    "  [1. 1. 1. 1.]\n",
    "  [1. 1. 1. 1.]]\n",
    "\n",
    " [[1. 1. 1. 1.]\n",
    "  [1. 1. 1. 1.]\n",
    "  [1. 1. 1. 1.]]], shape=(2, 3, 4), dtype=float32)\n",
    "\n",
    "In [27]: print(tf.reduce_sum(A))\n",
    "tf.Tensor(24.0, shape=(), dtype=float32)\n",
    "\n",
    "In [28]: print(tf.reduce_sum(A,0))\n",
    "tf.Tensor(\n",
    "[[2. 2. 2. 2.]\n",
    " [2. 2. 2. 2.]\n",
    " [2. 2. 2. 2.]], shape=(3, 4), dtype=float32)\n",
    "\n",
    "In [29]: print(tf.reduce_sum(A,1))\n",
    "tf.Tensor(\n",
    "[[3. 3. 3. 3.]\n",
    " [3. 3. 3. 3.]], shape=(2, 4), dtype=float32)\n",
    "\n",
    "In [30]: print(tf.reduce_sum(A,2))\n",
    "tf.Tensor(\n",
    "[[4. 4. 4.]\n",
    " [4. 4. 4.]], shape=(2, 3), dtype=float32)\n",
    "```\n",
    "  \n",
    "NOTE: New dimensions are appended to the beginning of a tensor:\n",
    "  \n",
    "(batches, observations, features)  \n",
    "(observations, features)  \n",
    "(features)  \n",
    "  \n",
    "or  \n",
    "  \n",
    "(z, y, x)  \n",
    "(y, x)  \n",
    "(x)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing element-wise multiplication\n",
    "  \n",
    "Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the $\\odot$ symbol, is shown below:\n",
    "  \n",
    "$\\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix} \\odot \\begin{bmatrix} 3 & 1 \\\\ 2 & 5 \\end{bmatrix} = \\begin{bmatrix} 3 & 2 \\\\ 4 & 5 \\end{bmatrix}$\n",
    "  \n",
    "In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply. Note that `tf.multiply()`, `tf.constant()`, and `tf.ones_like()` have been imported for you.\n",
    "  \n",
    "1. Define the tensors `A1` and `A23` as constants.\n",
    "2. Set `B1` to be a tensor of ones with the same shape as `A1`.\n",
    "3. Set `B23` to be a tensor of ones with the same shape as `A23`.\n",
    "4. Set `C1` and `C23` equal to the element-wise products of `A1` and `B1`, and `A23` and `B23`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: [1 2 3 4]\n",
      "C23: [[1 2 3]\n",
      " [1 6 4]]\n"
     ]
    }
   ],
   "source": [
    "# Define tensors A1 and A23 as constants\n",
    "A1 = tf.constant([1, 2, 3, 4])\n",
    "A23 = tf.constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = tf.ones_like(A1)\n",
    "B23 = tf.ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = tf.multiply(A1, B1)\n",
    "C23 = tf.multiply(A23, B23)\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('C1: {}'.format(C1.numpy()))\n",
    "print('C23: {}'.format(C23.numpy()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how performing element-wise multiplication with tensors of ones leaves the original tensors unchanged."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with matrix multiplication\n",
    "  \n",
    "In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, `features`, and a target vector, `bill`, which are taken from a credit card dataset we will use later in the course.\n",
    "  \n",
    "$features = \\begin{bmatrix} 2 & 24 \\\\ 2 & 26 \\\\ 2 & 57 \\\\ 1 & 37 \\end{bmatrix}, bill = \\begin{bmatrix} 3913 \\\\ 2682 \\\\ 8617 \\\\ 64400 \\end{bmatrix}, params = \\begin{bmatrix} 1000 \\\\ 150 \\end{bmatrix}$\n",
    "  \n",
    "The matrix of input data, `features`, contains two columns: `education` level and `age`. The target vector, `bill`, is the size of the credit card borrower's `bill`.\n",
    "  \n",
    "Since we have not trained the model, you will enter a guess for the values of the parameter vector, `params`. You will then use `tf.matmul()` to perform matrix multiplication of features by `params` to generate predictions, `billpred`, which you will compare with `bill`. Note that we have imported `tf.matmul()` and `tf.constant()`.\n",
    "  \n",
    "1. Define `features`, `params`, and `bill` as constants.\n",
    "Compute the predicted value vector, `billpred`, by multiplying the input data, `features`, by the parameters, `params`. Use matrix multiplication, rather than the element-wise product.\n",
    "Define `error` as the targets, `bill`, minus the predicted values, `billpred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1687]\n",
      " [-3218]\n",
      " [-1933]\n",
      " [57850]]\n"
     ]
    }
   ],
   "source": [
    "# Define features, params, and bill as constants\n",
    "features = tf.constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "params = tf.constant([[1000], [150]])\n",
    "bill = tf.constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = tf.matmul(features, params)\n",
    "\n",
    "# Compute and print the error, remember that when scoring models it is y_true - y_pred\n",
    "error = bill - billpred\n",
    "print(error.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding matrix multiplication will make things simpler when we start making predictions with linear models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing over tensor dimensions\n",
    "  \n",
    "You've been given a matrix, `wealth`. This contains the value of bond and stock wealth for five individuals in thousands of dollars.\n",
    "  \n",
    "$wealth = \\begin{bmatrix} 11 & 50 \\\\ 7 & 2 \\\\ 4 & 60 \\\\ 3 & 0 \\\\ 25 & 10 \\end{bmatrix}$\n",
    "  \n",
    "The first column corresponds to `bonds` and the second corresponds to `stocks`. Each row gives the bond and stock wealth for a single individual. Use `wealth`, `reduce_sum()`, and `.numpy()` to determine which statements are correct about `wealth`.\n",
    "  \n",
    "Possible answers\n",
    "  \n",
    "- [ ] The individual in the first row has the highest total wealth (i.e. stocks + bonds).\n",
    "- [ ] Combined, the 5 individuals hold $50,000 in stocks.\n",
    "- [x] Combined, the 5 individuals hold $50,000 in bonds.\n",
    "- [ ] The individual in the second row has the lowest total wealth (i.e. stocks + bonds).\n",
    "  \n",
    "Correct. Understanding how to sum over tensor dimensions will be helpful when preparing datasets and training models.\n",
    "  \n",
    "---\n",
    "  \n",
    "**Solution**  \n",
    "  \n",
    "```python\n",
    "In [31]: wealth = tf.ones([5,2])\n",
    "\n",
    "In [32]: print(wealth)\n",
    "tf.Tensor(\n",
    "[[1. 1.]\n",
    " [1. 1.]\n",
    " [1. 1.]\n",
    " [1. 1.]\n",
    " [1. 1.]], shape=(5, 2), dtype=float32)\n",
    "\n",
    "In [33]: wealth = tf.constant([[11, 50], [7, 2], [4, 60], [3, 0], [25, 10]])\n",
    "\n",
    "In [34]: print(wealth)\n",
    "tf.Tensor(\n",
    "[[11 50]\n",
    " [ 7  2]\n",
    " [ 4 60]\n",
    " [ 3  0]\n",
    " [25 10]], shape=(5, 2), dtype=int32)\n",
    "\n",
    "In [35]: print(tf.reduce_sum(wealth,0))\n",
    "tf.Tensor([ 50 122], shape=(2,), dtype=int32)\n",
    "\n",
    "In [36]: print(tf.reduce_sum(wealth,1))\n",
    "tf.Tensor([61  9 64  3 35], shape=(5,), dtype=int32)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced operations\n",
    "  \n",
    "In this video, we will cover a selection of advanced operations. Some will be used frequently in later chapters. Others will help you to gain intuition about complex machine learning routines.\n",
    "  \n",
    "**Overview of advanced operations**\n",
    "  \n",
    "We have already covered basic operations in TensorFlow, including `tf.add()`, `tf.multiply()`, `tf.matmul()`, and `tf.reduce_sum()`. In this lesson, we will move on to more advanced operations, including `tf.gradient()`, `tf.reshape()`, and `tf.random()`.\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Operation</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.gradient()</td>\n",
    "    <td>Computes the slope of a function at a point</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.reshape()</td>\n",
    "    <td>Reshapes a tensor (ie. 10x10 to 100x1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>tf.random()</td>\n",
    "    <td>Populates a tensor with entities drawn from a probability distribution</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "**Overview of advanced operations**\n",
    "  \n",
    "The `tf.gradient()` operation, which we'll use in conjunction with `tf.GradientTape`, computes the slope of a function at a point. The `tf.reshape()` operation changes the shape of a tensor. And the `tf.random()` module generates a tensor out of randomly-drawn values.\n",
    "  \n",
    "**Finding the optimum**\n",
    "  \n",
    "In many machine learning problems, you will need to find an optimum--that is, a minimum or maximum. You may, for instance, want to find the model parameters that minimize the loss function or maximize the objective function. Fortunately, we can do this by using the `tf.gradient()` operation, which tells us the slope of a function at a point. We start this process by passing points to the `tf.gradient()` operation until we find one where the gradient is zero. Next, we check if the gradient is increasing or decreasing at that point. If it is increasing, we have minimum. Otherwise, we have a maximum.\n",
    "  \n",
    "In many problems we want to find the optimum of a function:  \n",
    "- Minimum: Lowest value of a loss function  \n",
    "- Maximum: Highest value of objective function  \n",
    "  \n",
    "<img src='../_images/finding-the-optimum-tensorflow.png' alt='img' width='740'>\n",
    "  \n",
    "**Calculating the gradient**\n",
    "  \n",
    "The plot shows the function $y = x$. Notice that the gradient--that is, the slope at a given point, is constant. If we increase $x$ by 1 unit, $y$ also increases by 1 unit.\n",
    "  \n",
    "<img src='../_images/finding-the-optimum-tensorflow1.png' alt='img' width='740'>\n",
    "  \n",
    "**Calculating the gradient**\n",
    "  \n",
    "This is not true if we instead consider the function $y=x^2$. When $x<0$, $y$ decreases when $x$ increases. When $x>0$, $y$ increases when $x$ increases. Thus, the gradient is initially negative, but becomes positive for $x$ larger than $0$. This means that $x$ equals $0$ minimizes $y$.\n",
    "  \n",
    "<img src='../_images/finding-the-optimum-tensorflow2.png' alt='img' width='740'>\n",
    "  \n",
    "**Gradients in TensorFlow**\n",
    "  \n",
    "Let's use TensorFlow to compute the gradient. We will start by defining a variable, `x` which we initialize to `-1.0`. We will then define `y` to be $x^2$ within an instance of gradient tape. Note that we apply the `.watch()` method to an instance of gradient tape and then pass the variable `x`. This will allow us to compute $\\Delta y/\\partial x$ (*the rate of change for $y$ with respect to $x$*). Next, we compute $\\nabla y\\partial x$ (*the gradient of $y$ with respect to $x$*) using the `tape` instance of `GradientTape`. Note that $y$ is the first argument and $x$ is the second. As written, the operation computes the slope of $y$ at a point. Running the code and printing, we find that the slope is $-2$ at $x = -1$, which means that $y$ is initially decreasing in $x$, as we saw on the previous slide. Much of the differentiation you do in deep learning models will be handled by high level APIs; however, gradient tape remains an invaluable tool for building advanced and custom models.\n",
    "  \n",
    "<img src='../_images/finding-the-optimum-tensorflow3.png' alt='img' width='740'>\n",
    "  \n",
    "**Images as tensors**\n",
    "  \n",
    "We'll next consider an operation that is particularly useful for image classification problems: reshaping. The grayscale image shown has a natural representation as a matrix with values between 0 and 255. While some algorithms exploit this shape, others require you to reshape matrices into vectors before using them as inputs, as shown in the diagram.\n",
    "  \n",
    "<img src='../_images/finding-the-optimum-tensorflow4.png' alt='img' width='740'>\n",
    "  \n",
    "*Images are often represented using 8-bit color depth, which means that each pixel in the image is assigned a value ranging from 0 to 255. This range allows for a total of 256 possible values for each color channel (<span style=\"color: #E74C3C;\">red</span>, <span style=\"color: #27AE60;\">green</span>, <span style=\"color: #3498DB;\">blue</span>) in the image.*\n",
    "  \n",
    "*The 0 to 255 range is based on the binary representation of data. In an 8-bit system, a single binary digit (bit) can represent two values: 0 or 1. By combining eight of these bits, we can represent a total of $2^8 = 256$ unique values. In image processing, each pixel's color is typically represented using three color channels (<span style=\"color: #E74C3C;\">red</span>, <span style=\"color: #27AE60;\">green</span>, <span style=\"color: #3498DB;\">blue</span>), with each channel having an 8-bit value. This results in a total of 256 possible values per channel.*\n",
    "  \n",
    "*Using a range of 0 to 255 allows for efficient storage and computation of image data. It also provides a convenient way to represent the intensity or brightness of a pixel. For example, a value of 0 represents the absence of that color channel, while 255 represents the maximum intensity or full presence of that color channel.*\n",
    "  \n",
    "*This 8-bit representation is commonly used in image formats such as JPEG, PNG, and BMP. However, it's important to note that not all images have to be represented using this range. There are other color depths available, such as 16-bit or 32-bit, which can provide a wider range of colors and more precision when necessary.*\n",
    "  \n",
    "**How to reshape a grayscale image**\n",
    "  \n",
    "Now that you've seen how images can be represented as tensors, let's generate some input images and reshape them. We will create a random grayscale image by drawing numbers from the set of integers between 0 and 255. We will use these to populate a 2 by 2 matrix. We can then reshape this into a 4 by 1 vector, as shown in the diagram.\n",
    "  \n",
    "<img src='../_images/finding-the-optimum-tensorflow5.png' alt='img' width='740'>\n",
    "  \n",
    "**How to reshape a color image**\n",
    "  \n",
    "For color images, we will generate 3 such matrices to form a 2 by 2 by 3 tensor. We could then reshape the image into a 4 by 3 tensor, as shown in the diagram.\n",
    "  \n",
    "<img src='../_images/finding-the-optimum-tensorflow6.png' alt='img' width='740'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping tensors\n",
    "  \n",
    "Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images.\n",
    "  \n",
    "The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays `gray_tensor` and `color_tensor`. Reshape these arrays into 1-dimensional vectors using the `tf.reshape()` operation, which has been imported for you from `tensorflow`. Note that the shape of `gray_tensor` is 28x28 and the shape of `color_tensor` is 28x28x3.\n",
    "  \n",
    "<center><img src='../_images/reshaping-tensors-sign-language-letter-A-gray-color.png' alt='img' width='456'></center>\n",
    "  \n",
    "1. Reshape `gray_tensor` from a 28x28 matrix into a 784x1 vector named `gray_vector`.\n",
    "2. Reshape `color_tensor` from a 28x28x3 tensor into a 2352x1 vector named `color_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the image files and passing as ndarrays of proper shape\n",
    "gray_tensor = pd.read_csv('../_datasets/gray_tensor.csv').to_numpy()\n",
    "color_tensor = pd.read_csv('../_datasets/color_tensor.csv').to_numpy().reshape(28, 28, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2)  # Create a grid of 1 row and 2 columns for the subplots\n",
    "\n",
    "# Display grayscale image in the first subplot\n",
    "ax[0].imshow(gray_tensor, cmap='gray')\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Display color image in the second subplot\n",
    "ax[1].imshow(color_tensor/255.0)  # color_tensor has to be normalized\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the grayscale image tensor into a vector\n",
    "gray_vector = tf.reshape(gray_tensor, (784, 1))\n",
    "\n",
    "# Reshape the color image tensor into a vector\n",
    "color_vector = tf.reshape(color_tensor, (2352, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Notice that there are 3 times as many elements in `color_vector` as there are in `gray_vector`, since `color_tensor` has 3 color channels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration of vectorization being preformed for both `gray_vector` and `color_vector`.\n",
    "  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "A_{11} & A_{12} & A_{13} & \\ldots & A_{1n} \\\\\n",
    "A_{21} & A_{22} & A_{23} & \\ldots & A_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{m1} & A_{m2} & A_{m3} & \\ldots & A_{mn}\n",
    "\\end{bmatrix} \\to \\vec{A} =\\left[\\begin{array}{c}\n",
    "a_{1} \\\\\n",
    "a_{2} \\\\\n",
    "\\vdots \\\\\n",
    "a_{n}\n",
    "\\end{array}\\right]$  \n",
    "  \n",
    "$\\begin{bmatrix}\n",
    "B_{11} & B_{12} & B_{13} & \\ldots & B_{1n} \\\\\n",
    "B_{21} & B_{22} & B_{23} & \\ldots & B_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "B_{m1} & B_{m2} & B_{m3} & \\ldots & B_{mn}\n",
    "\\end{bmatrix} \\to \\vec{B} =\\left[\\begin{array}{c}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{n}\n",
    "\\end{array}\\right]$\n",
    "  \n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing with gradients\n",
    "You are given a loss function $y=x^2$ which you want to minimize. You can do this by computing the slope using the `tf.GradientTape()` operation at different values of $x$. If the slope is positive, you can decrease the loss by lowering $x$. If it is negative, you can decrease it by increasing $x$. This is how gradient descent works.\n",
    "  \n",
    "<center><img src='../_images/y-equals-x-squared.png' alt='img' width='456'></center>\n",
    "  \n",
    "In practice, you will use a high level `tensorflow` operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at $x$ values of $-1$, $1$, and $0$. The following operations are available: `tf.GradientTape()`, `tf.multiply()`, and `tf.Variable()`.\n",
    "  \n",
    "1. Define $x$ as a variable with the initial value `x0`.\n",
    "2. Set the loss function, $y$ equal to $x$ multiplied by $x$. Do not make use of operator overloading.\n",
    "3. Set the function to return $\\nabla y \\partial x$ (the gradient of $y$ with respect to $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n",
      "2.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(x0):\n",
    "    x = tf.Variable(x0)                 # Define x as a variable with an initial value of x0\n",
    "    with tf.GradientTape() as tape:     # A context manager is how TensorFlow handles this object\n",
    "        tape.watch(x)                   # Ensure that the tape tracks the operations involving the x tensor\n",
    "        y = tf.multiply(x, x)           # Define y using the multiply operation\n",
    "    \n",
    "    # Return the gradient of y with respect to x\n",
    "    return tape.gradient(y, x).numpy()\n",
    "\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the slope is positive at $x = 1$, which means that we can lower the loss by reducing $x$. The slope is negative at $x = -1$, which means that we can lower the loss by increasing $x$. The slope at $x = 0$ is $0$, which means that we cannot lower the loss by either increasing or decreasing $x$. This is because the loss is minimized at $x = 0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is calcualting the gradient descent for $y=x^2$ in which we can make use of the differentiation, and the general power rule derivative formula.\n",
    "\n",
    "$where.$\n",
    "  \n",
    "Power rule in calculus is a method of differentiation that is used when an algebraic expression with power needs to be differentiated. In simple words, we can say that the power rule is used to differentiate algebraic expressions of the form $x^n$, where $n$ is a real number. To differentiate $x^n$, we simply multiply the power $n$ by the expression and reduce the power by 1.\n",
    "  \n",
    "$given.$  \n",
    "  \n",
    "$\\Large \\frac{d}{dx} x^n = n x^{n - 1}$\n",
    "  \n",
    "<br></br>\n",
    "  \n",
    "**Solution**\n",
    "  \n",
    "Step 1, problem\n",
    "  \n",
    "$\\Large y=x^2$\n",
    "  \n",
    "Step 2, apply power rule  \n",
    "  \n",
    "$\\Large \\frac{dy}{dx}(x^2) = 2x^{2-1} = 2x$\n",
    "  \n",
    "Step 3, simplify  \n",
    "  \n",
    "$\\Large \\frac{dy}{dx}(x^2) = 2x$\n",
    "  \n",
    "Step 4, solution\n",
    "  \n",
    "$\\Large y = 2x$\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with image data\n",
    "  \n",
    "You are given a black-and-white image of a letter, which has been encoded as a tensor, `letter`. You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, `model`, which can be used to classify letter.\n",
    "  \n",
    "The 3x3 tensor, `letter`, and the 1x3 tensor, `model`, are available in the Python shell. You can determine whether letter is a K by multiplying `letter` by `model`, summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, `model` is a collection of weights, arranged in a tensor.\n",
    "  \n",
    "Note that the functions `tf.reshape()`, `tf.matmul()`, and `tf.reduce_sum()` have been imported from `tensorflow` and are available for use.\n",
    "  \n",
    "1. The model, `model`, is 1x3 tensor, but should be a 3x1. Reshape `model`.\n",
    "2. Perform a matrix multiplication of the 3x3 tensor, `letter`, by the 3x1 tensor, `model`.\n",
    "3. Sum over the resulting tensor, output, and assign this value to `prediction`.\n",
    "4. Print `prediction` using the `.numpy()` method to determine whether `letter` is K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = np.array([[1.0, 0, 1.0], [1., 1., 0], [1., 0, 1.]])\n",
    "model = np.array([[1., 0., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdfElEQVR4nO3df2xV9f3H8dct0FvJuLcyaG/RAiIKCkgB+VFcaI2dVQlblyVDdIKEH3OBRYSodNlk4mLjV6YmjgXNIs1EghoENnQ4LL8iVNBCM0BGBBkg4RYVe69Ud0Hu5/vH4p2FFmi559723ecjOYn39HPO/XzS3j69vfdyfM45JwAADMtI9wQAAPAasQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCY51nsTp48qXvvvVeBQEDZ2dmaNm2aTp06dcFjiouL5fP5Gm0PPPCAV1MEAHQQPq/+bcw777xTx48f1wsvvKAzZ85o6tSpGjlypJYvX97sMcXFxbr++uu1cOHCxL6uXbsqEAh4MUUAQAfR2YuT7tu3T+vWrdP777+vm2++WZL0/PPP66677tKiRYvUq1evZo/t2rWrQqGQF9MCAHRQnsSuurpa2dnZidBJUklJiTIyMrR9+3b95Cc/afbYV155RcuWLVMoFNKECRP029/+Vl27dm12fCwWUywWS9yOx+M6efKkvv/978vn8yVnQQCAlHHO6csvv1SvXr2UkZGcV9s8iV04HFZOTk7jO+rcWd27d1c4HG72uHvuuUd9+vRRr1699M9//lOPPvqo9u/frzfeeKPZYyoqKvT4448nbe4AgLbh6NGjuvrqq5NyrhbFbv78+XrqqacuOGbfvn2tnszMmTMT/z1kyBDl5eXptttu08GDB3Xttdc2eUx5ebnmzp2buB2JRNS7d+9WzwHtSyQSSfcUkELBYDDdU0AKdevWLWnnalHs5s2bp/vvv/+CY/r166dQKKQTJ0402v/NN9/o5MmTLXo9bvTo0ZKkAwcONBs7v98vv99/yeeELbx5CbArmS9FtSh2PXv2VM+ePS86rrCwUPX19aqpqdGIESMkSRs2bFA8Hk8E7FLU1tZKkvLy8loyTQAAGvHkc3Y33HCD7rjjDs2YMUM7duzQ1q1bNXv2bN19992Jd2IeO3ZMAwcO1I4dOyRJBw8e1BNPPKGamhr9+9//1l//+ldNnjxZ48aN00033eTFNAEAHYRnHyp/5ZVXNHDgQN12222666679IMf/EAvvvhi4utnzpzR/v379dVXX0mSMjMz9c477+j222/XwIEDNW/ePP30pz/V3/72N6+mCADoIDz7UHm6RKNRXsTuQIz9+OIi+DhRxxKJRJL2ujz/NiYAwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMzzPHaLFy9W3759lZWVpdGjR2vHjh0XHP/6669r4MCBysrK0pAhQ/TWW295PUUAgHGexu7VV1/V3LlztWDBAu3cuVNDhw5VaWmpTpw40eT4bdu2adKkSZo2bZp27dqlsrIylZWVac+ePV5OEwBgnM8557w6+ejRozVy5Ej98Y9/lCTF43Hl5+frV7/6lebPn3/e+IkTJ6qhoUFr165N7BszZowKCgq0ZMmSJu8jFospFoslbkejUeXn5yd5JWirPPzxRRvk8/nSPQWkUCQSUSAQSMq5PHtmd/r0adXU1KikpOR/d5aRoZKSElVXVzd5THV1daPxklRaWtrseEmqqKhQMBhMbIQOAHAuz2L32Wef6ezZs8rNzW20Pzc3V+FwuMljwuFwi8ZLUnl5uSKRSGI7evTo5U8eAGBK53RP4HL5/X75/f50TwMA0IZ59syuR48e6tSpk+rq6hrtr6urUygUavKYUCjUovEAAFwKz2KXmZmpESNGqKqqKrEvHo+rqqpKhYWFTR5TWFjYaLwkrV+/vtnxAABcEuehFStWOL/f7yorK92HH37oZs6c6bKzs104HHbOOXffffe5+fPnJ8Zv3brVde7c2S1atMjt27fPLViwwHXp0sXt3r37ku8zEok4SWwdZEPHku6fN7bUbpFIJGk/O56+Zjdx4kR9+umneuyxxxQOh1VQUKB169Yl3oRy5MgRZWT878nl2LFjtXz5cv3mN7/Rr3/9a1133XVavXq1Bg8e7OU0AQDGefo5u3SIRqMKBoPpngZSxNiPLy6Cz9l1LO3ic3YAALQVxA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYJ7nsVu8eLH69u2rrKwsjR49Wjt27Gh2bGVlpXw+X6MtKyvL6ykCAIzzNHavvvqq5s6dqwULFmjnzp0aOnSoSktLdeLEiWaPCQQCOn78eGI7fPiwl1MEAHQAnsbumWee0YwZMzR16lTdeOONWrJkibp27aqXXnqp2WN8Pp9CoVBiy83N9XKKAIAOoLNXJz59+rRqampUXl6e2JeRkaGSkhJVV1c3e9ypU6fUp08fxeNxDR8+XE8++aQGDRrU7PhYLKZYLJa4HY1Gk7MAtAs+ny/dU0AKOefSPQWkQDQaVTAYTOo5PXtm99lnn+ns2bPnPTPLzc1VOBxu8pgBAwbopZde0po1a7Rs2TLF43GNHTtWn3zySbP3U1FRoWAwmNjy8/OTug4AQPvXpt6NWVhYqMmTJ6ugoEBFRUV644031LNnT73wwgvNHlNeXq5IJJLYjh49msIZAwDaA8/+jNmjRw916tRJdXV1jfbX1dUpFApd0jm6dOmiYcOG6cCBA82O8fv98vv9lzVXAIBtnj2zy8zM1IgRI1RVVZXYF4/HVVVVpcLCwks6x9mzZ7V7927l5eV5NU0AQAfg2TM7SZo7d66mTJmim2++WaNGjdJzzz2nhoYGTZ06VZI0efJkXXXVVaqoqJAkLVy4UGPGjFH//v1VX1+vp59+WocPH9b06dO9nCYAwDhPYzdx4kR9+umneuyxxxQOh1VQUKB169Yl3rRy5MgRZWT878nlF198oRkzZigcDuvKK6/UiBEjtG3bNt14441eThMAYJzPGXsvrxdvWQXQNhj7dYVmfPt7PBKJKBAIJOWcberdmAAAeIHYAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPOIHQDAPGIHADCP2AEAzCN2AADziB0AwDxiBwAwj9gBAMwjdgAA84gdAMA8YgcAMI/YAQDMI3YAAPM8jd2WLVs0YcIE9erVSz6fT6tXr77oMZs2bdLw4cPl9/vVv39/VVZWejlFAEAH4GnsGhoaNHToUC1evPiSxh86dEjjx4/XrbfeqtraWs2ZM0fTp0/X22+/7eU0AQDG+ZxzLiV35PNp1apVKisra3bMo48+qjfffFN79uxJ7Lv77rtVX1+vdevWNXlMLBZTLBZL3I5Go8rPz0/avAG0HSn6dYU0i0ajCgaDikQiCgQCSTlnm3rNrrq6WiUlJY32lZaWqrq6utljKioqFAwGExuhAwCcq03FLhwOKzc3t9G+3NxcRaNRff31100eU15erkgkktiOHj2aiqkCANqRzumewOXy+/3y+/3pngYAoA1rU8/sQqGQ6urqGu2rq6tTIBDQFVdckaZZAQDauzYVu8LCQlVVVTXat379ehUWFqZpRgAACzyN3alTp1RbW6va2lpJ//1oQW1trY4cOSLpv6+3TZ48OTH+gQce0Mcff6xHHnlE//rXv/SnP/1Jr732mh566CEvpwkAsM55aOPGjU7SeduUKVOcc85NmTLFFRUVnXdMQUGBy8zMdP369XNLly5t0X1GIpEm75ONja39b+gYvv09HolEknbOlH3OLlW+/XwGAHuM/bpCM8x/zg4AAC8QOwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAeZ7GbsuWLZowYYJ69eoln8+n1atXX3D8pk2b5PP5ztvC4bCX0wQAGOdp7BoaGjR06FAtXry4Rcft379fx48fT2w5OTkezRAA0BF09vLkd955p+68884WH5eTk6Ps7OzkTwgA0CF5GrvWKigoUCwW0+DBg/W73/1Ot9xyS7NjY7GYYrFY4nY0Gk3FFNFGOOfSPQWkkM/nS/cU0E61qTeo5OXlacmSJVq5cqVWrlyp/Px8FRcXa+fOnc0eU1FRoWAwmNjy8/NTOGMAQHvgcyn6X2Ofz6dVq1aprKysRccVFRWpd+/eevnll5v8elPP7Ahex8Ezu46FZ3YdSyQSUSAQSMq52uSfMb9r1KhRevfdd5v9ut/vl9/vT+GMAADtTZv6M2ZTamtrlZeXl+5pAADaMU+f2Z06dUoHDhxI3D506JBqa2vVvXt39e7dW+Xl5Tp27Jj+8pe/SJKee+45XXPNNRo0aJD+85//6M9//rM2bNigf/zjH15OEwBgnKex++CDD3Trrbcmbs+dO1eSNGXKFFVWVur48eM6cuRI4uunT5/WvHnzdOzYMXXt2lU33XST3nnnnUbnAACgpVL2BpVUiUajCgaD6Z4GUsTYjy8ugjeodCzJfINKm3/NDgCAy0XsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHmexq6iokIjR45Ut27dlJOTo7KyMu3fv/+ix73++usaOHCgsrKyNGTIEL311lteThMAYJynsdu8ebNmzZql9957T+vXr9eZM2d0++23q6Ghodljtm3bpkmTJmnatGnatWuXysrKVFZWpj179ng5VQCAYT7nnEvVnX366afKycnR5s2bNW7cuCbHTJw4UQ0NDVq7dm1i35gxY1RQUKAlS5Zc9D6i0aiCwWDS5oy2LYU/vmgDfD5fuqeAFIpEIgoEAkk5V0pfs4tEIpKk7t27NzumurpaJSUljfaVlpaqurq6yfGxWEzRaLTRBgDAd6UsdvF4XHPmzNEtt9yiwYMHNzsuHA4rNze30b7c3FyFw+Emx1dUVCgYDCa2/Pz8pM4bAND+pSx2s2bN0p49e7RixYqknre8vFyRSCSxHT16NKnnBwC0f51TcSezZ8/W2rVrtWXLFl199dUXHBsKhVRXV9doX11dnUKhUJPj/X6//H5/0uYKALDH02d2zjnNnj1bq1at0oYNG3TNNddc9JjCwkJVVVU12rd+/XoVFhZ6NU0AgHGePrObNWuWli9frjVr1qhbt26J192CwaCuuOIKSdLkyZN11VVXqaKiQpL04IMPqqioSH/4wx80fvx4rVixQh988IFefPFFL6cKALDMeUhSk9vSpUsTY4qKityUKVMaHffaa6+566+/3mVmZrpBgwa5N99885LvMxKJNHu/bPY2dCzp/nljS+0WiUSS9rOT0s/ZpQKfs+tYjP344iL4nF3H0m4/ZwcAQDoQOwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGAesQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOYROwCAecQOAGCep7GrqKjQyJEj1a1bN+Xk5KisrEz79++/4DGVlZXy+XyNtqysLC+nCQAwztPYbd68WbNmzdJ7772n9evX68yZM7r99tvV0NBwweMCgYCOHz+e2A4fPuzlNAEAxnX28uTr1q1rdLuyslI5OTmqqanRuHHjmj3O5/MpFApd0n3EYjHFYrHE7Ugk0rrJol2KRqPpngIAjzjnknYuT2N3rm9D1L179wuOO3XqlPr06aN4PK7hw4frySef1KBBg5ocW1FRoccffzzpc0X7EAwG0z0FAB75/PPPk/YY97lkpvMC4vG4fvSjH6m+vl7vvvtus+Oqq6v10Ucf6aabblIkEtGiRYu0ZcsW7d27V1dfffV54899ZldfX68+ffroyJEjHeoXYTQaVX5+vo4ePapAIJDu6aRER1yzxLo70ro74pql/z4x6t27t7744gtlZ2cn5Zwpe2Y3a9Ys7dmz54Khk6TCwkIVFhYmbo8dO1Y33HCDXnjhBT3xxBPnjff7/fL7/eftDwaDHeqH41uBQKDDrbsjrlli3R1JR1yzJGVkJO9tJSmJ3ezZs7V27Vpt2bKlyWdnF9KlSxcNGzZMBw4c8Gh2AADrPH03pnNOs2fP1qpVq7RhwwZdc801LT7H2bNntXv3buXl5XkwQwBAR+DpM7tZs2Zp+fLlWrNmjbp166ZwOCzpv39ivOKKKyRJkydP1lVXXaWKigpJ0sKFCzVmzBj1799f9fX1evrpp3X48GFNnz79ku7T7/drwYIFTf5p07KOuO6OuGaJdXekdXfENUverNvTN6j4fL4m9y9dulT333+/JKm4uFh9+/ZVZWWlJOmhhx7SG2+8oXA4rCuvvFIjRozQ73//ew0bNsyraQIAjEvZuzEBAEgX/m1MAIB5xA4AYB6xAwCYR+wAAOaZiN3Jkyd17733KhAIKDs7W9OmTdOpU6cueExxcfF5lxJ64IEHUjTj1lm8eLH69u2rrKwsjR49Wjt27Ljg+Ndff10DBw5UVlaWhgwZorfeeitFM02elqzZyuWhtmzZogkTJqhXr17y+XxavXr1RY/ZtGmThg8fLr/fr/79+yfe3dxetHTNmzZtOu977fP5Eh9vag9acwk0qf0/rtN16TcTsbv33nu1d+9erV+/PvEvtcycOfOix82YMaPRpYT+7//+LwWzbZ1XX31Vc+fO1YIFC7Rz504NHTpUpaWlOnHiRJPjt23bpkmTJmnatGnatWuXysrKVFZWpj179qR45q3X0jVLNi4P1dDQoKFDh2rx4sWXNP7QoUMaP368br31VtXW1mrOnDmaPn263n77bY9nmjwtXfO39u/f3+j7nZOT49EMk681l0Cz8LhO26XfXDv34YcfOknu/fffT+z7+9//7nw+nzt27FizxxUVFbkHH3wwBTNMjlGjRrlZs2Ylbp89e9b16tXLVVRUNDn+Zz/7mRs/fnyjfaNHj3a/+MUvPJ1nMrV0zUuXLnXBYDBFs0sNSW7VqlUXHPPII4+4QYMGNdo3ceJEV1pa6uHMvHMpa964caOT5L744ouUzCkVTpw44SS5zZs3NzvGwuP6XJey7mQ8ttv9M7vq6mplZ2fr5ptvTuwrKSlRRkaGtm/ffsFjX3nlFfXo0UODBw9WeXm5vvrqK6+n2yqnT59WTU2NSkpKEvsyMjJUUlKi6urqJo+prq5uNF6SSktLmx3f1rRmzdL/Lg+Vn5+vH//4x9q7d28qpptW7f17fTkKCgqUl5enH/7wh9q6dWu6p3NZLuUSaBa/1y299FtrH9vtPnbhcPi8P1107txZ3bt3v+Df7++55x4tW7ZMGzduVHl5uV5++WX9/Oc/93q6rfLZZ5/p7Nmzys3NbbQ/Nze32TWGw+EWjW9rWrPmAQMG6KWXXtKaNWu0bNkyxeNxjR07Vp988kkqppw2zX2vo9Govv766zTNylt5eXlasmSJVq5cqZUrVyo/P1/FxcXauXNnuqfWKvF4XHPmzNEtt9yiwYMHNzuuvT+uz3Wp607GYzulF29tifnz5+upp5664Jh9+/a1+vzffU1vyJAhysvL02233aaDBw/q2muvbfV5kT4tvTwU2q8BAwZowIABidtjx47VwYMH9eyzz+rll19O48xa51IvgWaNV5d+a0qbjd28efMS/35mc/r166dQKHTeGxa++eYbnTx5UqFQ6JLvb/To0ZKkAwcOtLnY9ejRQ506dVJdXV2j/XV1dc2uMRQKtWh8W9OaNZ+ro1weqrnvdSAQSPyD6x3BqFGj2mUsWnIJtPb+uP6uVF/6rc3+GbNnz54aOHDgBbfMzEwVFhaqvr5eNTU1iWM3bNigeDyeCNilqK2tlaQ2eSmhzMxMjRgxQlVVVYl98XhcVVVVjf5v57sKCwsbjZek9evXNzu+rWnNms/VUS4P1d6/18lSW1vbrr7XrhWXQLPwvW7Nus/Vqsf2Zb29pY2444473LBhw9z27dvdu+++66677jo3adKkxNc/+eQTN2DAALd9+3bnnHMHDhxwCxcudB988IE7dOiQW7NmjevXr58bN25cupZwUStWrHB+v99VVla6Dz/80M2cOdNlZ2e7cDjsnHPuvvvuc/Pnz0+M37p1q+vcubNbtGiR27dvn1uwYIHr0qWL2717d7qW0GItXfPjjz/u3n77bXfw4EFXU1Pj7r77bpeVleX27t2briW0ypdfful27drldu3a5SS5Z555xu3atcsdPnzYOefc/Pnz3X333ZcY//HHH7uuXbu6hx9+2O3bt88tXrzYderUya1bty5dS2ixlq752WefdatXr3YfffSR2717t3vwwQddRkaGe+edd9K1hBb75S9/6YLBoNu0aZM7fvx4Yvvqq68SYyw+rluz7mQ8tk3E7vPPP3eTJk1y3/ve91wgEHBTp051X375ZeLrhw4dcpLcxo0bnXPOHTlyxI0bN851797d+f1+179/f/fwww+7SCSSphVcmueff9717t3bZWZmulGjRrn33nsv8bWioiI3ZcqURuNfe+01d/3117vMzEw3aNAg9+abb6Z4xpevJWueM2dOYmxubq6766673M6dO9Mw68vz7dvqz92+XeuUKVNcUVHReccUFBS4zMxM169fP7d06dKUz/tytHTNTz31lLv22mtdVlaW6969uysuLnYbNmxIz+Rbqan1Smr0vbP4uG7NupPx2OYSPwAA89rsa3YAACQLsQMAmEfsAADmETsAgHnEDgBgHrEDAJhH7AAA5hE7AIB5xA4AYB6xAwCYR+wAAOb9P+eAMO2HDaClAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(letter, cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]], shape=(3, 1), dtype=float64)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = tf.reshape(model, (3, 1))\n",
    "\n",
    "# Multiply letter by model\n",
    "# This operation multiplies the 3x3 letter matrix by the 3x1 model matrix, \n",
    "# resulting in a 3x1 output tensor stored in the output variable.\n",
    "output = tf.matmul(letter, model)\n",
    "print(output)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = tf.reduce_sum(output)\n",
    "print(prediction.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! Your model found that `prediction`=1.0 and correctly classified the letter as a K. In the coming chapters, you will use data to train a model, model, and then combine this with matrix multiplication, `matmul(letter, model)`, as we have done here, to make predictions about the classes of objects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
