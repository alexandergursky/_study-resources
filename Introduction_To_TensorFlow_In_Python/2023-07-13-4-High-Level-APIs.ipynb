{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level APIs\n",
    "  \n",
    "In the final chapter, you'll use high-level APIs in TensorFlow 2 to train a sign language letter classifier. You will use both the sequential and functional Keras APIs to train, validate, make predictions with, and evaluate models. You will also learn how to use the Estimators API to streamline the model definition and training process, and to avoid errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[TensorFlow Documentation](https://www.tensorflow.org)  \n",
    "[TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.20148&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)  \n",
    "[TensorFlow Estimators](https://www.tensorflow.org/guide/estimator)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>tf.constant()</td>\n",
    "    <td>Creates a constant tensor with a specified value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>tf.Variable()</td>\n",
    "    <td>Creates a mutable tensor variable that can be modified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>tf.zeros()</td>\n",
    "    <td>Creates a tensor filled with zeros.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>tf.ones()</td>\n",
    "    <td>Creates a tensor filled with ones.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>tf.zeros_like()</td>\n",
    "    <td>Creates a tensor of zeros with the same shape as another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>tf.ones_like()</td>\n",
    "    <td>Creates a tensor of ones with the same shape as another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>tf.fill()</td>\n",
    "    <td>Creates a tensor filled with a specified scalar value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>tf.add()</td>\n",
    "    <td>Performs element-wise addition of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>tf.multiply()</td>\n",
    "    <td>Performs element-wise multiplication of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>tf.matmul()</td>\n",
    "    <td>Performs matrix multiplication of two tensors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>tf.reduce_sum()</td>\n",
    "    <td>Computes the sum of elements across specified dimensions of a tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>tf.gradient()</td>\n",
    "    <td>Computes the gradients of a tensor with respect to another tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>tf.GradientTape</td>\n",
    "    <td>Records operations for automatic differentiation to compute gradients.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>tf.reshape()</td>\n",
    "    <td>Reshapes a tensor into a specified shape.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>tf.random()</td>\n",
    "    <td>Generates random values from a specified distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>tf.random().uniform()</td>\n",
    "    <td>Generates random values from a uniform distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>tf.GradientTape.watch()</td>\n",
    "    <td>Used to start tracing Tensor by the Tape</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>tf.cast()</td>\n",
    "    <td>Casts a tensor to a new datatype</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>tensorflow.keras.losses</td>\n",
    "    <td>Retrieves a Keras loss as a function</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>tensorflow.keras.losses.mse()</td>\n",
    "    <td>Computes the mean squared error between labels and predictions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>21</td>\n",
    "    <td>tensorflow.keras.losses.mae()</td>\n",
    "    <td>Computes the mean absolute error between labels and predictions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>tensorflow.keras.losses.Huber()</td>\n",
    "    <td>Computes Huber loss value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>23</td>\n",
    "    <td>tf.math.log()</td>\n",
    "    <td>Computes natural logarithm of x element-wise.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>24</td>\n",
    "    <td>tf.keras.optimizers.Adam</td>\n",
    "    <td>Optimizer that implements the Adam algorithm. Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25</td>\n",
    "    <td>tf.keras.optimizers.Adam.minimize()</td>\n",
    "    <td>Calling .minimize() takes care of both computing the gradients and applying them to the variables.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26</td>\n",
    "    <td>tf.convert_to_tensor()</td>\n",
    "    <td>Converts a numpy array or Python list to a Tensorflow tensor.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27</td>\n",
    "    <td>tf.gather()</td>\n",
    "    <td>Gathers slices from a tensor along a specified axis.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>28</td>\n",
    "    <td>tf.keras.optimizers.SGD</td>\n",
    "    <td>Optimizer that implements the Stochastic Gradient Descent algorithm.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>29</td>\n",
    "    <td>tf.keras.optimizers.RMSprop</td>\n",
    "    <td>Optimizer that implements the RMSprop algorithm.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>30</td>\n",
    "    <td>tf.keras.activations.sigmoid</td>\n",
    "    <td>Computes the sigmoid activation function.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31</td>\n",
    "    <td>tf.keras.activations.relu</td>\n",
    "    <td>Computes the ReLU activation function.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>32</td>\n",
    "    <td>tf.random.normal()</td>\n",
    "    <td>Generates random values from a normal distribution.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>33</td>\n",
    "    <td>tf.keras.layers.Dense()</td>\n",
    "    <td>A fully connected layer in a neural network.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>34</td>\n",
    "    <td>tf.keras.layers.Dropout()</td>\n",
    "    <td>Applies dropout regularization to the input.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>35</td>\n",
    "    <td>tf.keras.losses.binary_crossentropy()</td>\n",
    "    <td>Computes the binary cross-entropy loss.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>36</td>\n",
    "    <td>tf.math.confusion_matrix()</td>\n",
    "    <td>Computes the confusion matrix.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>37</td>\n",
    "    <td>pd.crosstab()</td>\n",
    "    <td>Computes a cross-tabulation of two or more factors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>38</td>\n",
    "    <td>np.hstack()</td>\n",
    "    <td>Stacks arrays in sequence horizontally (column-wise).</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: tensorflow  \n",
    "Version: 2.13.0  \n",
    "Summary: TensorFlow is an open source machine learning framework for everyone.  \n",
    "  \n",
    "Name: scikit-learn  \n",
    "Version: 1.3.0  \n",
    "Summary: A set of python modules for machine learning and data mining  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import tensorflow as tf             # TensorFlow:               Deep-Learning Neural Networks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining neural networks with Keras\n",
    "  \n",
    "In chapter 3, we saw how to define neural networks in TensorFlow, both using linear algebra and higher level Keras operations. In this lesson, we will introduce the Keras sequential API, and expand on our brief and informal introduction of the Keras functional API.\n",
    "  \n",
    "**Classifying sign language letters**\n",
    "  \n",
    "Throughout this chapter, we'll focus on using Keras to classify four letters from the Sign Language MNIST dataset: a, b, c, and d. Note that the images appear to be low resolution because each is represented by a 28x28 matrix.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras.png' alt='img' width='740'>\n",
    "  \n",
    "**The sequential API**\n",
    "  \n",
    "Now, let's say we experiment with several different architectures and select the one that makes the most accurate predictions. It has an input layer, a first hidden layer with 16 nodes, and a second hidden layer with 8 nodes. We'll have 4 output nodes, since there are 4 letters in the dataset.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras1.png' alt='img' width='740'>\n",
    "  \n",
    "**The sequential API**\n",
    "  \n",
    "A good way to construct this model in Keras is to use the sequential API. This API is simpler and makes strong assumptions about how you will construct your model. It assumes that you have an input layer, some number of hidden layers, and an output layer. All of these layers are ordered one after the other in a sequence.\n",
    "  \n",
    "- input layer\n",
    "- hidden layer\n",
    "- output layer\n",
    "- ordered in sequence\n",
    "  \n",
    "**Building a sequential model**\n",
    "  \n",
    "We'll start by importing `tensorflow`. We can then define a `keras.Sequential()` model, which we'll name model. Once we have defined this object, we can simply stack layers on top of it sequentially using the add method. Let's start by adding the first hidden layer, which is a dense layer with 16 nodes. We'll select a relu activation function and supply an `input_shape=`, which Keras requires for the first layer. This input shape is simply a tuple that contains the dimensions of our data. Since we'll be using 28 by 28 pixel images, reshaped into vector, we'll supply 28*28 comma as the input shape.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras2.png' alt='img' width='740'>\n",
    "  \n",
    "**Building a sequential model**\n",
    "  \n",
    "Next, we'll define a second hidden layer according to the desired model architecture. Finally, we specify that the model has 4 output nodes and uses a softmax activation function. If we want to check our model's architecture, we can use the `.summary()` method, which we'll return to in the upcoming exercises. The model has now been defined, but it is not yet ready to be trained. We must first perform a compilation step, where we specify the optimizer and loss function. Here, we've selected the adam optimizer and the categorical crossentropy loss function, which we'll use for classification problems with more than 2 classes.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras3.png' alt='img' width='740'>\n",
    "  \n",
    "**The functional API**\n",
    "  \n",
    "But what if you want to train two models jointly to predict the same target? The functional API is for that.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras4.png' alt='img' width='740'>\n",
    "  \n",
    "**Using the functional API**\n",
    "  \n",
    "As an example, let's say we have a set of 28x28 images and a set of 10 features of metadata. We want to use both to predict the image's class, but restrict how they interact in our model. We'll start by using the `keras.Input()` operation to define the input shapes for model 1 and model 2. Next, we define layer 1 and layer 2 as dense layers for model 1. Note that we have to pass the previous layer as an argument if we use the functional API, but did not with the sequential. You may remember that we did this in chapter 3. We were also using the functional API then.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras5.png' alt='img' width='740'>\n",
    "  \n",
    "**Using the functional API**\n",
    "  \n",
    "We now define layers 1 and 2 for model 2 and then use the add layer in keras to combine the outputs in a layer that merges the two models. Finally, we define a functional model. As inputs, it takes both the model 1 and model 2 inputs. As outputs, it takes the merged layer. The only thing left to do is `.compile()` it and train.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras6.png' alt='img' width='740'>\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sequential model in Keras\n",
    "  \n",
    "In chapter 3, we used components of the `keras` API in `tensorflow` to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the `keras.Sequential()` model API to define a neural network that can be used to classify images of sign language letters. You will also use the `.summary()` method to print the model's architecture, including the shape and number of parameters associated with each layer.\n",
    "  \n",
    "Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that `keras` has been imported from `tensorflow` for you.\n",
    "  \n",
    "1. Define a `keras.Sequential()` model named `model`.\n",
    "2. Set the first layer to be `Dense()` and to have 16 nodes and a `relu` activation.\n",
    "3. Define the second layer to be `Dense()` and to have 8 nodes and a `relu` activation.\n",
    "4. Set the output layer to have 4 nodes and use a `softmax` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_27 (Dense)            (None, 16)                12560     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12732 (49.73 KB)\n",
      "Trainable params: 12732 (49.73 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a Keras sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've defined a model, but we haven't compiled it. The compilation step in `keras` allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the `.summary()` method allows us to view the model's architecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a sequential model\n",
    "  \n",
    "In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply `keras.layers.Dropout()` to prevent overfitting. Finally, you will compile the model to use the `'adam'` `optimizer=` and the `'categorical_crossentropy'` `loss=`. You will also use a method in `keras` to summarize your model's architecture. Note that `keras` has been imported from `tensorflow` for you and a sequential `keras` model has been defined as model.\n",
    "  \n",
    "1. In the first dense layer, set the number of nodes to 16, the `activation=` to `'sigmoid'`, and the `input_shape=` to (784,).\n",
    "2. Apply `keras.layers.Dropout()` at a rate of 25% to the first layer's output.\n",
    "3. Set the output layer to be dense, have 4 nodes, and use a `'softmax'` `activation=` function.\n",
    "4. Compile the model using an `'adam'` `optimizer=` and `'categorical_crossentropy'` `loss=` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 16)                12560     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12628 (49.33 KB)\n",
      "Trainable params: 12628 (49.33 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now defined and compiled a neural network using the `keras.Sequential()` model. Notice that printing the `model.summary()` method shows the layer type, output shape, and number of parameters of each layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a multiple input model\n",
    "  \n",
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the `.summary()` method to examine the joint model's architecture.\n",
    "  \n",
    "Note that `keras` has been imported from `tensorflow` for you. Additionally, the input layers of the first and second models have been defined as `m1_inputs` and `m2_inputs`, respectively. Note that the two models have the same architecture, but one of them uses a `'sigmoid'` `activation=` in the first layer and the other uses a `'relu'`.\n",
    "  \n",
    "1. Pass model 1's input layer to its first layer and model 1's first layer to its second layer.\n",
    "2. Pass model 2's input layer to its first layer and model 2's first layer to its second layer.\n",
    "3. Use the `.add()` operation to combine the second layers of model 1 and model 2.\n",
    "4. Complete the functional model definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of input layer for each model\n",
    "m1_inputs = tf.keras.Input(shape=(784,))\n",
    "m2_inputs = tf.keras.Input(shape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 784)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 784)]                0         []                            \n",
      "                                                                                                  \n",
      " dense_32 (Dense)            (None, 12)                   9420      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_34 (Dense)            (None, 12)                   9420      ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_33 (Dense)            (None, 4)                    52        ['dense_32[0][0]']            \n",
      "                                                                                                  \n",
      " dense_35 (Dense)            (None, 4)                    52        ['dense_34[0][0]']            \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 4)                    0         ['dense_33[0][0]',            \n",
      "                                                                     'dense_35[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18944 (74.00 KB)\n",
      "Trainable params: 18944 (74.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = tf.keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = tf.keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = tf.keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = tf.keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = tf.keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = tf.keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `.summary()` method yields a new column: `Connected to`. This column tells you how layers connect to each other within the network. We can see that `dense_2`, for instance, is connected to the `input_2` layer. We can also see that the `.add()` layer, which merged the two models, connected to both `dense_1` and `dense_3`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Keras\n",
    "  \n",
    "Earlier in the chapter, we defined neural networks in Keras. In this video, we will discuss how to train and evaluate them.\n",
    "  \n",
    "**Overview of training and evaluation**\n",
    "  \n",
    "Whenever we train and evaluate a model in `tensorflow`, we typically use the same set of steps. First, we'll load and clean the data. Second, we'll define a model, specifying an architecture. Third, we'll train and validate the model. And fourth, we perform evaluation.\n",
    "  \n",
    "1. Load and clean data\n",
    "2. Define model\n",
    "3. Train and validate model\n",
    "4. Evaluate model\n",
    "  \n",
    "**How to train a model**\n",
    "  \n",
    "Let's see an example of how this works. We'll start by importing `tensorflow` and defining a `keras.Sequential()` model. We'll then `.add()` a dense layer to the model with 16 nodes and a `'relu'` `activation=` function. Note that our input shape is (784,), since our dataset consists of 28x28 images, reshaped into vectors. We next define the output layer, which has 4 nodes and a `'softmax'` `activation=` function.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras7.png' alt='img' width='740'>\n",
    "  \n",
    "**How to train a model**\n",
    "  \n",
    "We next compile the model, using the `'adam'` `optimizer=` and the `'categorical_crossentropy'` `loss=`. Finally, we train the model using the `.fit()` operation.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras8.png' alt='img' width='740'>\n",
    "  \n",
    "**The `.fit()` operation**\n",
    "  \n",
    "Notice that we only supplied two arguments to fit: features and labels. These are the only two required arguments; however, there are also many optional arguments, including `batch_size=`, `epochs=`, and `validation_split=`. We will cover each of these.\n",
    "  \n",
    "**Batch size and epochs**\n",
    "  \n",
    "Let's start with the difference between the `batch_size=` and `epochs=` parameters. The number of examples in each batch is the batch size, which is 32 by default. The number of times you train on the full set of batches is called the number of epochs. Here, the `batch_size=` is 5 and the number of `epochs=` is 2. Using multiple epochs allows the model to revisit the same batches, but with different model weights and possibly `optimizer=` parameters, since they are updated after each batch.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras9.png' alt='img' width='740'>\n",
    "  \n",
    "**Performing validation**\n",
    "  \n",
    "So what does the `validation_split=` parameter do? It divides the dataset into two parts. The first part is the train set and the second part is the validation set. Selecting a value of `0.20` will put 20% of the data in the validation set.\n",
    "  \n",
    "**Performing validation**\n",
    "  \n",
    "The benefit of using a `validation_split=` is that you can see how your model performs on both the data it was trained on, the training set, and a separate dataset it was not trained on, the validation set. Here, we can see the first 10 epochs of training. Notice that we can see the training loss and validation loss separately. If the training loss becomes substantially lower than the validation loss, this is an indication that we're overfitting. We should either terminate the training process before that point or add regularization or dropout.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras10.png' alt='img' width='740'>\n",
    "  \n",
    "**Changing the metric**\n",
    "  \n",
    "Another benefit of the high level `keras` API is that we can swap less informative metrics, such as the loss, for ones that are easily interpretable, such as the share of accurately classified examples. We can do this by supplying `'accuracy'` to the `metrics=` parameter of `model.compile()`. We then apply `.fit()` to the model again with the same settings.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras11.png' alt='img' width='740'>\n",
    "  \n",
    "**Changing the metric**\n",
    "  \n",
    "Using the accuracy metric, we can see that the model performs quite well. In just 10 epochs, it goes from an accuracy of 42% to over 99%. Notice that the model performs equally well in the validation set, which means that we're unlikely to be overfitting.\n",
    "  \n",
    "<img src='../_images/defining-neural-networks-with-keras12.png' alt='img' width='740'>\n",
    "  \n",
    "**The evaluation operation**\n",
    "  \n",
    "Finally, it is good idea to split off a test set before you begin to train and validate. You can use the `.evaluate()` operation to check performance on the test set at the end of the training process. Since you may tune model parameters in response to validation set performance, using a separate test set will provide you with further assurance that you have not overfitted. You now know how to streamline model training and validation in `keras`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Keras\n",
    "  \n",
    "In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.\n",
    "  \n",
    "Note that `keras` has been imported from `tensorflow` for you. Additionally, the features are available as `sign_language_features` and the targets are available as `sign_language_labels`.\n",
    "  \n",
    "1. Define a sequential model named `model`.\n",
    "2. Set the output layer to be dense, have 4 nodes, and use a `'softmax'` `activation=` function.\n",
    "3. Compile the model with the `'SGD'` `optimizer=` and `'categorical_crossentropy'` `loss=`.\n",
    "4. Complete the fitting operation and set the number of `epochs=` to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>148</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>37</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>142</td>\n",
       "      <td>144</td>\n",
       "      <td>145</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>152</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>160</td>\n",
       "      <td>162</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>169</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>...</td>\n",
       "      <td>181</td>\n",
       "      <td>197</td>\n",
       "      <td>195</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>191</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "      <td>193</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>97</td>\n",
       "      <td>106</td>\n",
       "      <td>117</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>175</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>160</td>\n",
       "      <td>164</td>\n",
       "      <td>168</td>\n",
       "      <td>172</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>107</td>\n",
       "      <td>106</td>\n",
       "      <td>110</td>\n",
       "      <td>111</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>102</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    1  142  143  146  148  149  149  149  150  151  ...    0   15   55   63   \n",
       "1    0  141  142  144  145  147  149  150  151  152  ...  173  179  179  180   \n",
       "2    1  156  157  160  162  164  166  169  171  171  ...  181  197  195  193   \n",
       "3    3   63   26   65   86   97  106  117  123  128  ...  175  179  180  182   \n",
       "4    1  156  160  164  168  172  175  178  180  182  ...  108  107  106  110   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0   37   61   77   65   38   23  \n",
       "1  181  181  182  182  183  183  \n",
       "2  193  191  192  198  193  182  \n",
       "3  183  183  184  185  185  185  \n",
       "4  111  108  108  102   84   70  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../_datasets/slmnist.csv', header=None)\n",
    "\n",
    "# X/y split\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features normalized \n",
    "sign_language_features = (X -  X.mean()) / (X.max() - X.min()).to_numpy()\n",
    "\n",
    "# Labels extracted, and one-hot-encoded\n",
    "sign_language_labels = pd.get_dummies(y).astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 2s 8ms/step - loss: 1.2732\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 1.0507\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.8622\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.7116\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13a305890>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784, )))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that your only measure of performance improvement was the value of the loss function in the training sample, which is not particularly informative. You will improve on this in the next exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and validation with Keras\n",
    "  \n",
    "We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation.\n",
    "\n",
    "Note that `keras` has been imported for you from `tensorflow`.\n",
    "  \n",
    "1. Set the first dense layer to have 32 nodes, use a `'sigmoid'` activation function, and have an input shape of (784,).\n",
    "2. Use the root mean square propagation optimizer, a `'categorical_crossentropy'` loss, and the accuracy metric.\n",
    "3. Set the number of epochs to 10 and use 10% of the dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 2s 14ms/step - loss: 0.9359 - accuracy: 0.6844 - val_loss: 0.5979 - val_accuracy: 0.9750\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 6ms/step - loss: 0.4327 - accuracy: 0.9822 - val_loss: 0.3115 - val_accuracy: 0.9900\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.2353 - accuracy: 0.9950 - val_loss: 0.1790 - val_accuracy: 0.9850\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9956 - val_loss: 0.1046 - val_accuracy: 0.9950\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9983 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9989 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 8ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13a3d7fd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(tf.keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `keras` API, you only needed 14 lines of code to define, compile, train, and validate a model. You may have noticed that your model performed quite well. In just 10 epochs, we achieved a classification accuracy of over 90% in the validation sample!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting detection\n",
    "  \n",
    "In this exercise, we'll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.\n",
    "  \n",
    "You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting.\n",
    "  \n",
    "Note that `keras` has been imported from `tensorflow`.\n",
    "  \n",
    "1. Define a sequential model in `keras` named model.\n",
    "2. Add a first dense layer with 1024 nodes, a `'relu'` activation, and an `input_shape=` of (784,).\n",
    "3. Set the `learning_rate=` to 0.001.\n",
    "4. Set the `.fit()` operation to iterate over the full sample 50 times and use 50% of the sample for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 25ms/step - loss: 0.2795 - accuracy: 0.9270 - val_loss: 0.0554 - val_accuracy: 0.9820\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.0211 - accuracy: 0.9950 - val_loss: 0.0169 - val_accuracy: 0.9970\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 9.5380e-04 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 7.3738e-04 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 6.2156e-04 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 5.3389e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 4.5877e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 4.0156e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 3.5745e-04 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 3.1552e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.8119e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 2.5224e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 2.2849e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 2.0976e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.9007e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 1.7632e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.6052e-04 - accuracy: 1.0000 - val_loss: 9.7131e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.5025e-04 - accuracy: 1.0000 - val_loss: 9.1201e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.3869e-04 - accuracy: 1.0000 - val_loss: 8.3791e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 1.2835e-04 - accuracy: 1.0000 - val_loss: 8.3596e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.2000e-04 - accuracy: 1.0000 - val_loss: 8.2883e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 1.1198e-04 - accuracy: 1.0000 - val_loss: 7.5049e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0476e-04 - accuracy: 1.0000 - val_loss: 7.5132e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 9.7742e-05 - accuracy: 1.0000 - val_loss: 7.0407e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 9.2403e-05 - accuracy: 1.0000 - val_loss: 6.7298e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 8.6824e-05 - accuracy: 1.0000 - val_loss: 6.2544e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 8.1537e-05 - accuracy: 1.0000 - val_loss: 6.1677e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 7.7739e-05 - accuracy: 1.0000 - val_loss: 5.9498e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 7.3011e-05 - accuracy: 1.0000 - val_loss: 5.8021e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 6.9703e-05 - accuracy: 1.0000 - val_loss: 5.6457e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 6.5560e-05 - accuracy: 1.0000 - val_loss: 5.2138e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 6.2299e-05 - accuracy: 1.0000 - val_loss: 5.0210e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 5.9269e-05 - accuracy: 1.0000 - val_loss: 5.0016e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 5.6618e-05 - accuracy: 1.0000 - val_loss: 4.8783e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 5.3701e-05 - accuracy: 1.0000 - val_loss: 4.6193e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 5.1277e-05 - accuracy: 1.0000 - val_loss: 4.5363e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 4.8770e-05 - accuracy: 1.0000 - val_loss: 4.5232e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 4.6583e-05 - accuracy: 1.0000 - val_loss: 4.2731e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 4.4732e-05 - accuracy: 1.0000 - val_loss: 4.2585e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 4.2694e-05 - accuracy: 1.0000 - val_loss: 3.9646e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 4.0942e-05 - accuracy: 1.0000 - val_loss: 4.0158e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 3.9339e-05 - accuracy: 1.0000 - val_loss: 3.8782e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 3.7688e-05 - accuracy: 1.0000 - val_loss: 3.7476e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 3.6115e-05 - accuracy: 1.0000 - val_loss: 3.7061e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 3.4592e-05 - accuracy: 1.0000 - val_loss: 3.4803e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13a4de590>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(tf.keras.layers.Dense(1024, activation='relu', input_shape=(784, )))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Finish the model compilation\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Complete the model fit operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=50, validation_split=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the validation loss, `val_loss`, was substantially higher than the training loss, `loss`. Furthermore, if `val_loss` started to increase before the training process was terminated, then we may have overfitted. When this happens, you will want to try decreasing the number of epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating models\n",
    "  \n",
    "Two models have been trained and are available: `large_model`, which has many parameters; and `small_model`, which has fewer parameters. Both models have been trained using `train_features` and `train_labels`, which are available to you. A separate test set, which consists of `test_features` and `test_labels`, is also available.\n",
    "  \n",
    "Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating `large_model` and `small_model` on both the train and test sets. For each model, you can do this by applying the `.evaluate(x, y)` method to compute the loss for features `x` and labels `y`. You will then compare the four losses generated.\n",
    "  \n",
    "1. Evaluate the small model using the train data.\n",
    "2. Evaluate the small model using the test data.\n",
    "3. Evaluate the large model using the train data.\n",
    "4. Evaluate the large model using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the small model\n",
    "small_model = tf.keras.Sequential()\n",
    "\n",
    "small_model.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(784,)))\n",
    "small_model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "small_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the large model\n",
    "large_model = tf.keras.Sequential()\n",
    "\n",
    "large_model.add(tf.keras.layers.Dense(64, activation='sigmoid', input_shape=(784,)))\n",
    "large_model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "large_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, \n",
    "                                                       beta_1=0.9, \n",
    "                                                       beta_2=0.999),\n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test; Train/test split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(sign_language_features, \n",
    "                                                                            sign_language_labels,\n",
    "                                                                            test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13a59bf90>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting models to X_train, y_train\n",
    "small_model.fit(train_features, train_labels, epochs=30, verbose=False)\n",
    "large_model.fit(train_features, train_labels, epochs=30, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 7ms/step - loss: 0.1409 - accuracy: 0.9920\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9890\n",
      "32/32 [==============================] - 1s 7ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "\n",
      " Small - Train: [0.14092232286930084, 0.9919999837875366], Test: [0.15205100178718567, 0.9890000224113464]\n",
      "Large - Train: [0.007727161981165409, 1.0], Test: [0.009323738515377045, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the small model using the train data\n",
    "small_train = small_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the small model using the test data\n",
    "small_test = small_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Evaluate the large model using the train data\n",
    "large_train = large_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evalute the large model using the test data\n",
    "large_test = large_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Print losses\n",
    "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
    "print('Large - Train: {}, Test: {}'.format(large_train, large_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the gap between the test and train set losses is high for `large_model`, suggesting that overfitting may be an issue. Furthermore, both test and train set performance is better for `large_model`. This suggests that we may want to use `large_model`, but reduce the number of training epochs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models with the Estimators API\n",
    "  \n",
    "In this video, we'll take a look at the high level Estimators API, which was elevated in importance in TensorFlow 2.0.\n",
    "  \n",
    "**What is the Estimators API?**\n",
    "  \n",
    "The Estimators API is a high level TensorFlow submodule. Relative to the core, lower-level TensorFlow APIs and the high-level Keras API, model building in the Estimator API is less flexible. This is because it enforces a set of best practices by placing restrictions on model architecture and training. The upside of using the Estimators API is that it allows for faster deployment. Models can be specified, trained, evaluated, and deployed with less code. Furthermore, there are many premade models that can be instantiated by setting a handful of model parameters.\n",
    "  \n",
    "- High level submodule \n",
    "- Less flexible \n",
    "- Faster deployment \n",
    "- Many premade models\n",
    "  \n",
    "<img src='../_images/estimator-api-keras-use.png' alt='img' width='740'>\n",
    "  \n",
    "1 Image taken from https://www.tensorflow.org/guide/premade_estimators\n",
    "  \n",
    "**Model specification and training**\n",
    "  \n",
    "So what does the typical model specification and training process look like in the Estimators API? Well, it starts with the definition of feature columns, which specify the shape and type of your data. Next, you load and transform your data within a function. The output of this function will be a dictionary object of features and your labels. The next step is to define an estimator. In this video, we'll use premade estimators, but you can also define custom estimators with different architectures. Finally, you will train the model you defined. Note that all model objects created through the Estimators API have train, evaluate, and predict operations.\n",
    "  \n",
    "1. Define feature columns\n",
    "2. Load and transform data\n",
    "3. Define an estimator\n",
    "4. Apply train operation\n",
    "  \n",
    "**Defining feature columns**\n",
    "  \n",
    "Let's step through this procedure to get a sense of how it works. We'll first define the feature columns. If we were working with the housing dataset from chapter 2, we might define a numeric feature column for size using `feature_column.numeric_column`. Note that we supplied the dictionary key, \"size,\" to the operation. We will do this for each feature column we create. We may also want a categorical feature column for the number of rooms using `feature_column.categorical_column_with_vocabulary_list`.\n",
    "  \n",
    "<img src='../_images/estimator-api-keras-use1.png' alt='img' width='740'>\n",
    "  \n",
    "**Defining feature columns**\n",
    "  \n",
    "We can then merge these into a list of features columns. Alternatively, if we were using the sign language MNIST dataset, we'd define a list containing a single vector of features.\n",
    "  \n",
    "<img src='../_images/estimator-api-keras-use2.png' alt='img' width='740'>\n",
    "  \n",
    "**Loading and transforming data**\n",
    "  \n",
    "We next need to define a function that transforms our data, puts the features in a dictionary, and returns both the features and labels. Note that we've simply taken three examples from the housing dataset for the sake of illustration. Using them, we've defined a dictionary with the keys \"size\" and \"rooms,\" which maps to the feature columns we defined. Next, we define a list or array of labels, which give the price of the house in this case, and then return the features and labels.\n",
    "  \n",
    "<img src='../_images/estimator-api-keras-use3.png' alt='img' width='740'>\n",
    "  \n",
    "**Define and train a regression estimator**\n",
    "  \n",
    "We can now define and train the estimator. But before we do that, we have to define what estimator we actually want to train. If we're predicting house prices, we may want to use a deep neural network with a regression head using `estimator.DNNRegressor`. This allows us to predict a continuous target. Note that all we had to supply was the list of feature columns and the number of nodes in each hidden layer. The rest is handled automatically. We then apply the train function, supply our input function, and train for 20 `steps=`.\n",
    "  \n",
    "<img src='../_images/estimator-api-keras-use4.png' alt='img' width='740'>\n",
    "  \n",
    "**Define and train a deep neural network**\n",
    "  \n",
    "Alternatively, if we want to instead perform a classification task with a deep neural network, we just need to change the estimator to `estimator.DNNClassifier`, add the number of classes, and then train again. You can also use linear classifiers, boosted trees, and other common options. Just check the TensorFlow Estimators documentation for a complete list. Estimators might seem confusing initially, but they're very useful once you master them.\n",
    "  \n",
    "<img src='../_images/estimator-api-keras-use5.png' alt='img' width='740'>\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to train with Estimators\n",
    "  \n",
    "For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the `estimator` API.\n",
    "  \n",
    "Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade `estimator`. Note that `feature_column` has been imported for you from `tensorflow`. Additionally, `numpy` has been imported as `np`, and the Kings County `housing` dataset is available as a `pandas` DataFrame: `housing`.\n",
    "  \n",
    "1. Complete the feature column for bedrooms and add another numeric feature column for bathrooms. Use `bedrooms` and `bathrooms` as the keys.\n",
    "2. Create a list of the feature columns, `feature_list`, in the order in which they were defined.\n",
    "3. Set `labels` to be equal to the price column in `housing`.\n",
    "4. Complete the `bedrooms` entry of the `features` dictionary and add another entry for `bathrooms`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21613, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.read_csv('../_datasets/kc_house_data.csv')\n",
    "print(housing.shape)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YIELD YIELD YIELD\n",
    "# YIELD YIELD YIELD\n",
    "# YIELD YIELD YIELD\n",
    "\n",
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = tf.feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = tf.feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "# YIELD TO tf.keras.layers.Input()\n",
    "# YIELD TO tf.keras.layers.Input()\n",
    "# YIELD TO tf.keras.layers.Input()\n",
    "def input_fn():\n",
    "    # Define the labels\n",
    "    labels = np.array(housing['price'])\n",
    "    \n",
    "    # Define the features\n",
    "    features = {'bedrooms': np.array(housing['bedrooms']),\n",
    "                'bathrooms': np.array(housing['bathrooms'])}\n",
    "    \n",
    "    return features, labels\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#E74C3C'>WARNING DEPRECATION:</span>  \n",
    "\n",
    "`bedrooms = tf.feature_column.numeric_column(\"bedrooms\")`  \n",
    "`bathrooms = tf.feature_column.numeric_column(\"bathrooms\")`  \n",
    "  \n",
    "> <span style='color:#E74C3C'>WARNING:</span>  tensorflow:From /var/folders/pf/_zjf_55d7mgb5llg516d7fyc0000gn/T/ipykernel_60816/3128264456.py:2: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.  \n",
    "Instructions for updating:  \n",
    "Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercise, we'll use the feature columns and data input function to define and train an estimator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Estimators\n",
    "  \n",
    "In the previous exercise, you defined a list of feature columns, `feature_list`, and a data input function, `input_fn()`. In this exercise, you will build on that work by defining an estimator that makes use of input data.\n",
    "  \n",
    "1. Use a deep neural network regressor with 2 nodes in both the first and second hidden layers and 1 training step.\n",
    "2. Modify the code to use a `LinearRegressor()`, remove the `hidden_units=`, and set the number of `steps=` to 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#E74C3C'>WARNING DEPRECATION:</span>  \n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION:</span>  \n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION:</span>  \n",
    "\n",
    "```python\n",
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#E74C3C'>WARNING OUTPUT:</span>  \n",
    "<span style='color:#E74C3C'>WARNING OUTPUT:</span>  \n",
    "<span style='color:#E74C3C'>WARNING OUTPUT:</span>  \n",
    "\n",
    "> INFO:tensorflow:Using default config.\n",
    "INFO:tensorflow:Using default config.\n",
    "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/pf/_zjf_55d7mgb5llg516d7fyc0000gn/T/tmp3idhkhh4\n",
    "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/pf/_zjf_55d7mgb5llg516d7fyc0000gn/T/tmp3idhkhh4\n",
    "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/pf/_zjf_55d7mgb5llg516d7fyc0000gn/T/tmp3idhkhh4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
    "graph_options {\n",
    "  rewrite_options {\n",
    "    meta_optimizer_iterations: ONE\n",
    "  }\n",
    "}\n",
    ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
    "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/pf/_zjf_55d7mgb5llg516d7fyc0000gn/T/tmp3idhkhh4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
    "graph_options {\n",
    "  rewrite_options {\n",
    "    meta_optimizer_iterations: ONE\n",
    "  }\n",
    "}\n",
    ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
    "INFO:tensorflow:Calling model_fn.\n",
    "INFO:tensorflow:Calling model_fn.\n",
    "  \n",
    "> ---------------------------------------------------------------------------\n",
    "> AttributeError                            Traceback (most recent call last)\n",
    "/Users/alexandergursky/Local_Repository/_study-resources/Introduction_To_TensorFlow_In_Python/2023-07-13-4-High-Level-APIs.ipynb Cell 42 in 3\n",
    "      1 # Define the model and set the number of steps\n",
    "      2 model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "----> 3 model.train(input_fn, steps=1)\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:360, in Estimator.train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n",
    "    357 hooks.extend(self._convert_train_steps_to_hooks(steps, max_steps))\n",
    "    359 saving_listeners = _check_listeners_type(saving_listeners)\n",
    "--> 360 loss = self._train_model(input_fn, hooks, saving_listeners)\n",
    "    361 tf.compat.v1.logging.info('Loss for final step: %s.', loss)\n",
    "    362 return self\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:1188, in Estimator._train_model(self, input_fn, hooks, saving_listeners)\n",
    "   1186   return self._train_model_distributed(input_fn, hooks, saving_listeners)\n",
    "   1187 else:\n",
    "-> 1188   return self._train_model_default(input_fn, hooks, saving_listeners)\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:1216, in Estimator._train_model_default(self, input_fn, hooks, saving_listeners)\n",
    "   1213 features, labels, input_hooks = (\n",
    "   1214     self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\n",
    "   1215 worker_hooks.extend(input_hooks)\n",
    "-> 1216 estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n",
    "   1217                                      self.config)\n",
    "   1218 global_step_tensor = tf.compat.v1.train.get_global_step(g)\n",
    "   1219 return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
    "   1220                                        hooks, global_step_tensor,\n",
    "   1221                                        saving_listeners)\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/estimator.py:1176, in Estimator._call_model_fn(self, features, labels, mode, config)\n",
    "   1173   kwargs['config'] = config\n",
    "   1175 tf.compat.v1.logging.info('Calling model_fn.')\n",
    "-> 1176 model_fn_results = self._model_fn(features=features, **kwargs)\n",
    "   1177 tf.compat.v1.logging.info('Done calling model_fn.')\n",
    "   1179 if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py:1159, in DNNRegressorV2.__init__.._model_fn(features, labels, mode, config)\n",
    "   1157 def _model_fn(features, labels, mode, config):\n",
    "   1158   \"\"\"Call the defined shared dnn_model_fn_v2.\"\"\"\n",
    "-> 1159   return dnn_model_fn_v2(\n",
    "   1160       features=features,\n",
    "   1161       labels=labels,\n",
    "   1162       mode=mode,\n",
    "   1163       head=head,\n",
    "   1164       hidden_units=hidden_units,\n",
    "   1165       feature_columns=tuple(feature_columns or []),\n",
    "   1166       optimizer=optimizer,\n",
    "   1167       activation_fn=activation_fn,\n",
    "   1168       dropout=dropout,\n",
    "   1169       config=config,\n",
    "   1170       batch_norm=batch_norm)\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py:570, in dnn_model_fn_v2(***failed resolving arguments***)\n",
    "    566 # In TRAIN mode, create optimizer and assign global_step variable to\n",
    "    567 # optimizer.iterations to make global_step increased correctly, as Hooks\n",
    "    568 # relies on global step as step counter.\n",
    "    569 if mode == ModeKeys.TRAIN:\n",
    "--> 570   optimizer = optimizers.get_optimizer_instance_v2(optimizer)\n",
    "    571   optimizer.iterations = tf.compat.v1.train.get_or_create_global_step()\n",
    "    573 # Create EstimatorSpec.\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/optimizers.py:127, in get_optimizer_instance_v2(opt, learning_rate)\n",
    "    125 if opt in six.iterkeys(_OPTIMIZER_CLS_NAMES_V2):\n",
    "    126   if not learning_rate:\n",
    "--> 127     if _optimizer_has_default_learning_rate(_OPTIMIZER_CLS_NAMES_V2[opt]):\n",
    "    128       return _OPTIMIZER_CLS_NAMES_V2[opt]()\n",
    "    129     else:\n",
    "\n",
    "> File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow_estimator/python/estimator/canned/optimizers.py:90, in _optimizer_has_default_learning_rate(opt)\n",
    "     89 def _optimizer_has_default_learning_rate(opt):\n",
    "---> 90   signature = inspect.getargspec(opt.__init__)\n",
    "     91   default_name_to_value = dict(zip(signature.args[::-1], signature.defaults))\n",
    "     92   return 'learning_rate' in default_name_to_value\n",
    "\n",
    "> AttributeError: module 'inspect' has no attribute 'getargspec'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#E74C3C'>WARNING DEPRECATION CODE:</span>  \n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION CODE:</span>  \n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION CODE:</span>  \n",
    "\n",
    "```python\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)\n",
    "```\n",
    "\n",
    "<span style='color:#E74C3C'>WARNING NULL EXERCISE SUGGESTION:</span>  \n",
    "<span style='color:#E74C3C'>WARNING NULL EXERCISE SUGGESTION:</span>  \n",
    "<span style='color:#E74C3C'>WARNING NULL EXERCISE SUGGESTION:</span>  \n",
    "Note that you have other premade `estimator` options, such as `BoostedTreesRegressor()`, and can also create your own custom estimators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#E74C3C'>WARNING DEPRECATION ESTIMATORS:</span>  \n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION ESTIMATORS:</span>  \n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION ESTIMATORS:</span>  \n",
    "```python\n",
    "model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "```\n",
    "\n",
    "> <span style='color:#E74C3C'>WARNING:</span>  WARNING:tensorflow:From /var/folders/pf/_zjf_55d7mgb5llg516d7fyc0000gn/T/ipykernel_60816/3006336195.py:2: DNNRegressorV2.__init__ (from tensorflow_estimator.python.estimator.canned.dnn) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.keras instead.\n",
    "  \n",
    "> <span style='color:#E74C3C'>WARNING:</span>  ValueError: Received a feature column from TensorFlow v1, but this is a TensorFlow v2 Estimator. Please either use v2 feature columns (accessible via tf.feature_column.* in TF 2.x) with this Estimator, or switch to a v1 Estimator for use with v1 feature columns accessible via tf.compat.v1.estimator.* and tf.compat.v1.feature_column.*, respectively.\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION:</span>  \n",
    "Change in the inspect module in Python 3.11, which removed the `getargspec()` function. This change affects the compatibility of TensorFlow with Python 3.11.\n",
    "\n",
    "<span style='color:#E74C3C'>WARNING DEPRECATION:</span>  \n",
    "Warning: Estimators are not recommended for new code. Estimators run v1.Session-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.\n",
    "  \n",
    "<span style='color:#E74C3C'>CRITICAL DEPRECATION WARNING REFER TO DOCUMENTATION: NOTEBOOK>RESOURCE SECTION>Links>Tensorflow Estimators</span>  \n",
    "  \n",
    "<span style='color:#E74C3C'>CRITICAL DEPRECATION WARNING REFER TO DOCUMENTATION: NOTEBOOK>RESOURCE SECTION>Links>Tensorflow Estimators</span>  \n",
    "  \n",
    "<span style='color:#E74C3C'>CRITICAL DEPRECATION WARNING REFER TO DOCUMENTATION: NOTEBOOK>RESOURCE SECTION>Links>Tensorflow Estimators</span>  \n",
    "  \n",
    "<span style='color:#E74C3C'>CRITICAL DEPRECATION WARNING REFER TO DOCUMENTATION: NOTEBOOK>RESOURCE SECTION>Links>Tensorflow Estimators</span>  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Complete\n",
    "  \n",
    "You've now completed this course on the fundamentals of the TensorFlow API in Python. In this final video, we'll review what you've learned, talk about two useful TensorFlow extensions, and then wrap-up with a discussion of the transition to TensorFlow 2.0.\n",
    "  \n",
    "What you learned\n",
    "  \n",
    "In chapter 1, you learned low-level, basic, and advanced operations in TensorFlow. You learned how to define and manipulate variables and constants. You also learned the graph-based computational model that underlies TensorFlow and how it can be used to compute gradients and solve arbitrary optimization problems. In chapter 2, you learned how to load and transform data for use in your TensorFlow projects. You also saw how to use predefined and custom loss functions. We ended with a discussion of how to train models, and when and how to divide the training into batches.\n",
    "  \n",
    "What you learned\n",
    "  \n",
    "In chapter 3, we moved on to training neural networks. You learned how to define neural network architecture in TensorFlow, both using low-level linear algebra operations and high-level Keras API operations. We talked about how to select activation functions and optimizers, and, ultimately, how to train models. In chapter 4, you learned how to make full use of the Keras API to train models in TensorFlow. We discussed the training and validation process and also introduced the high-level Estimators API, which can be used to streamline the production process.\n",
    "  \n",
    "TensorFlow extensions\n",
    "  \n",
    "In addition to what we covered, there are also a two important TensorFlow extensions that did not fit into the course, but may be worthwhile to explore on your own. The first is TensorFlow Hub, which allows users to import pretrained models that can then be used to perform transfer learning. This will be particularly useful when you want to train an image classifier with a small number of images, but want to make use of a feature-extractor trained on a much larger set of different images. TensorFlow Probability is another exciting extension, which is also currently available as a standalone module. One benefit of using TensorFlow Probability is that it provides additional statistical distributions that can be used for random number generation. It also enables you to incorporate trainable statistical distributions into your models. Finally, TensorFlow Probability provides an extended set of optimizers that are commonly used in statistical research. This gives you additional tools beyond what the core TensorFlow module provides.\n",
    "  \n",
    "TensorFlow 2.0\n",
    "  \n",
    "Finally, I will say a few words about the difference between TensorFlow 2 and TensorFlow 1. If you primarily develop in 1, you may have noticed that you do not need to define static graphs or enable eager execution. This is done automatically in 2. Furthermore, TensorFlow 2 has substantially tighter integration with Keras. In fact, the core functionality of the TensorFlow 1 train module is handled by `tf.keras` operations in 2. In addition to the centrality of Keras, the Estimators API also plays a more important role in TensorFlow 2. Finally, TensorFlow 2 also allows you to use static graphs, but they are available through the `tf.function` operation.\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
