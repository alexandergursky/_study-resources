{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "  \n",
    "You will now get exposure to different types of features. You will modify existing features and create new ones. Also, you will treat the missing data accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Scikit-Learn Documentation](https://scikit-learn.org/stable/)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>sklearn.ensemble.RandomForestRegressor</td>\n",
    "    <td>Create a Random Forest Regressor model.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>sklearn.model_selection.KFold</td>\n",
    "    <td>Split dataset into K consecutive folds for cross-validation.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>sklearn.model_selection.KFold.split</td>\n",
    "    <td>Generate indices to split data into training and test sets using KFold.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>sklearn.metrics.mean_squared_error</td>\n",
    "    <td>Calculate the mean squared error between true and predicted values.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>numpy.mean</td>\n",
    "    <td>Compute the arithmetic mean along a specified axis.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>numpy.std</td>\n",
    "    <td>Compute the standard deviation along a specified axis.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>pandas.concat</td>\n",
    "    <td>Concatenate DataFrames along a particular axis.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>pandas.to_datetime</td>\n",
    "    <td>Convert the argument to datetime.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>sklearn.preprocessing.LabelEncoder</td>\n",
    "    <td>Encode labels with a numeric value.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>DataFrame.map</td>\n",
    "    <td>Apply a function to each element of a Series.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>DataFrame.sample</td>\n",
    "    <td>Randomly sample rows from a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>DataFrame.drop_duplicates</td>\n",
    "    <td>Remove duplicate rows from a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>sklearn.impute.SimpleImputer</td>\n",
    "    <td>Impute missing values using a specified strategy.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: scikit-learn  \n",
    "Version: 1.3.0  \n",
    "Summary: A set of python modules for machine learning and data mining  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Setting a standard style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "  \n",
    "Once we know the properties of the input data and have a reliable validation scheme, it's time to start building prediction models.\n",
    "  \n",
    "**Solution workflow**\n",
    "  \n",
    "Recall from the previous chapter the solution workflow for the competitions. We've already covered the first three blocks. Let's now consider the modeling stage.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Modeling stage**\n",
    "  \n",
    "This stage is the longest one in the competition, and kind of feels like a marathon.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle1.png' alt='img' width='740'></center>\n",
    "  \n",
    "During the modeling loop we pre-process data, create new features, enhance models, apply different tricks and iterate over and over again. The majority of the ideas and experiments will not work, but the goal is to find a subsample of actions which improve both local validation and Public Leaderboard scores.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle2.png' alt='img' width='740'></center>\n",
    "  \n",
    "So, after any change we should look at the validation score. If we observe an improvement on local validation, then we keep our change, otherwise, discard it. The important rule is to tweak only a single thing at a time, because changing multiple things does not allow us to detect what actually works and what doesn't.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Feature engineering**\n",
    "  \n",
    "This particular chapter is devoted to feature engineering. It is the process of creating new features. It helps our Machine Learning models to get the additional information and consequently to better predict the target variable.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle4.png' alt='img' width='740'></center>\n",
    "  \n",
    "The ideas for new features can come from prior experience working with similar data. Another source is EDA. Having looked at the data, we could potentially generate ideas for new valuable features. One more source is domain knowledge of the problem we're solving. It allows us to use ideas and approaches that work for this particular domain.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Feature types**\n",
    "  \n",
    "There is a number of different feature types. The most popular include: Numerical features. It's usual numbers, measures and counts. For example, price, number of bedrooms and so on. Categorical features. It's some group the observation belongs to. For example, country names, marital status and so on. Date features include various date and time information. Coordinates describe geospatial data. Text features contain different descriptions, addresses and so on. Finally, images include some visual data for each observation.\n",
    "  \n",
    "- Numerical\n",
    "- Categorical\n",
    "- Datetime\n",
    "- Coordinates\n",
    "- Text\n",
    "- Images\n",
    "  \n",
    "**Creating features**\n",
    "  \n",
    "There are some situations when we need to generate features for train and test independently and for each validation split in the k-fold cross-validation. However, in the majority of cases features are created for train and test sets simultaneously. For this purpose, we concatenate train and test DataFrames into a single DataFrame using `pandas`' `.concat()` method. Then we generate some new features. And split our DataFrame back to the train and test. We could use the `.isin()` method to find the original train and test ids, respectively.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Arithmetical features**\n",
    "  \n",
    "The simplest engineered features are arithmetical features. We just take two numerical features, apply arithmetical operations to them and obtain new features. Let's consider a subsample from two sigma connect dataset with only number of bathrooms and bedrooms in the apartments, together with the price. Then, for example, we could generate such features as price per one bedroom. Or the overall number of bedrooms and bathrooms. And so on.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Datetime features**\n",
    "  \n",
    "Another type of the data we will speak about in this lesson, is datetime. Let's look at the demand forecasting data. It contains item sales for each date. To generate features from this date, firstly, we convert the date column to datetime object using `pandas`' `to_datetime()` method. Then, we could use the `.dt` attribute and obtain any date feature we'd like.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle8.png' alt='img' width='740'></center>\n",
    "  \n",
    "For example, we could start with the year number. Using `.dt` attribute and proceeding with `.year` attribute. Then, for example, month number. January is encoded as 1, February as 2 and so on to December encoded as 12. We can also get a consecutive number of the week during the year. And various possibilities for day features. Like a consecutive number of the day during the year, month and week. Note that day of the week encodes Monday as 0, Tuesday as 1 proceeding to Sunday as 6.\n",
    "  \n",
    "<center><img src='../_images/feature-engineering-kaggle9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "All right, let's get some practical experience creating new numerical and date features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetical features\n",
    "  \n",
    "To practice creating new features, you will be working with a subsample from the Kaggle competition called \"House Prices: Advanced Regression Techniques\". The goal of this competition is to predict the price of the house based on its properties. It's a regression problem with Root Mean Squared Error as an evaluation metric.\n",
    "  \n",
    "Your goal is to create new features and determine whether they improve your validation score. To get the validation score from 5-fold cross-validation, you're given the `get_kfold_rmse()` function. Use it with the `train` DataFrame, available in your workspace, as an argument.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Create a new feature representing the total area (basement, 1st and 2nd floors) of the house. The columns `\"TotalBsmtSF\"`, `\"FirstFlrSF\"` and `\"SecondFlrSF\"` give the areas of the basement, 1st and 2nd floors, respectively.\n",
    "2. Create a new feature representing the area of the garden. It is a difference between the total area of the property (`\"LotArea\"`) and the first floor area (`\"FirstFlrSF\"`).\n",
    "3. Create a new feature representing the total number of bathrooms in the house. It is a sum of full bathrooms (`\"FullBath\"`) and half bathrooms (`\"HalfBath\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Model instantiation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "def get_kfold_rmse(train):\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        train = train.fillna(0)\n",
    "        feats = [x for x in train.columns if x not in ['Id', 'SalePrice', 'RoofStyle', 'CentralAir']]\n",
    "        \n",
    "        fold_train, fold_test = train.loc[train_index], train.loc[test_index]\n",
    "\n",
    "        # Fit the data and make predictions\n",
    "        # Create a Random Forest object\n",
    "        rf = RandomForestRegressor(n_estimators=10, min_samples_split=10, random_state=123)\n",
    "\n",
    "        # Train a model\n",
    "        rf.fit(X=fold_train[feats], y=fold_train['SalePrice'])\n",
    "\n",
    "        # Get predictions for the test set\n",
    "        pred = rf.predict(fold_test[feats])\n",
    "    \n",
    "        fold_score = mean_squared_error(fold_test['SalePrice'], pred)\n",
    "        mse_scores.append(np.sqrt(fold_score))\n",
    "        \n",
    "    return round(np.mean(mse_scores) + np.std(mse_scores), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../_datasets/house_prices_train.csv')\n",
    "test = pd.read_csv('../_datasets/house_prices_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE before feature engineering: 36029.39\n",
      "RMSE with total area: 35073.2\n",
      "RMSE with garden area: 34413.55\n",
      "RMSE with number of bathromms: 34506.78\n"
     ]
    }
   ],
   "source": [
    "# Look at the initial RMSE\n",
    "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
    "\n",
    "# Find the total area of the house\n",
    "train['totalArea'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
    "\n",
    "# Look at the updated RMSE\n",
    "print('RMSE with total area:', get_kfold_rmse(train))\n",
    "\n",
    "# Find the area of the garden\n",
    "train['GardenArea'] = train['LotArea'] - train['1stFlrSF']\n",
    "print('RMSE with garden area:', get_kfold_rmse(train))\n",
    "\n",
    "# Find total number of bathrooms\n",
    "train['TotalBath'] = train['FullBath'] + train['HalfBath']\n",
    "print('RMSE with number of bathromms:', get_kfold_rmse(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You've created three new features. Here you see that house area improved the RMSE by almost 1,000. Adding garden area improved the RMSE by another 600. However, with the total number of bathrooms, the RMSE has increased. It means that you keep the new area features, but do not add \"TotalBath\" as a new feature. Let's now work with the datetime features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features\n",
    "  \n",
    "You've built some basic features using numerical variables. Now, it's time to create features based on date and time. You will practice on a subsample from the Taxi Fare Prediction Kaggle competition data. The data represents information about the taxi rides and the goal is to predict the price for each ride.\n",
    "  \n",
    "Your objective is to generate date features from the pickup datetime. Recall that it's better to create new features for `train` and `test` data simultaneously. After the features are created, split the data back into the `train` and `test` DataFrames. Here it's done using `pandas`' `.isin()` method.\n",
    "  \n",
    "The `train` and `test` DataFrames are already available in your workspace.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Concatenate the `train` and `test` DataFrames into a single DataFrame `taxi`.\n",
    "2. Convert the \"pickup_datetime\" column to a datetime object.\n",
    "3. Create the day of week (using `.dayofweek` attribute) and hour (using `.hour` attribute) features from the `\"pickup_datetime\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 8)\n",
      "(9914, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:21 UTC</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:16 UTC</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00 UTC</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:42 UTC</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00 UTC</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  fare_amount          pickup_datetime  pickup_longitude  \\\n",
       "0   0          4.5  2009-06-15 17:26:21 UTC        -73.844311   \n",
       "1   1         16.9  2010-01-05 16:52:16 UTC        -74.016048   \n",
       "2   2          5.7  2011-08-18 00:35:00 UTC        -73.982738   \n",
       "3   3          7.7  2012-04-21 04:30:42 UTC        -73.987130   \n",
       "4   4          5.3  2010-03-09 07:51:00 UTC        -73.968095   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0        40.721319         -73.841610         40.712278                1  \n",
       "1        40.711303         -73.979268         40.782004                1  \n",
       "2        40.761270         -73.991242         40.750562                2  \n",
       "3        40.733143         -73.991567         40.758092                1  \n",
       "4        40.768008         -73.956655         40.783762                1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../_datasets/taxi_train_chapter_4.csv')\n",
    "test = pd.read_csv('../_datasets/taxi_test_chapter_4.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29914, 8)\n",
      "id                     int64\n",
      "fare_amount          float64\n",
      "pickup_datetime       object\n",
      "pickup_longitude     float64\n",
      "pickup_latitude      float64\n",
      "dropoff_longitude    float64\n",
      "dropoff_latitude     float64\n",
      "passenger_count        int64\n",
      "dtype: object\n",
      "id                                 int64\n",
      "fare_amount                      float64\n",
      "pickup_datetime      datetime64[ns, UTC]\n",
      "pickup_longitude                 float64\n",
      "pickup_latitude                  float64\n",
      "dropoff_longitude                float64\n",
      "dropoff_latitude                 float64\n",
      "passenger_count                    int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-06-15 17:26:21+00:00</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>2010-01-05 16:52:16+00:00</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2011-08-18 00:35:00+00:00</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2012-04-21 04:30:42+00:00</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2010-03-09 07:51:00+00:00</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  fare_amount           pickup_datetime  pickup_longitude  \\\n",
       "0   0          4.5 2009-06-15 17:26:21+00:00        -73.844311   \n",
       "1   1         16.9 2010-01-05 16:52:16+00:00        -74.016048   \n",
       "2   2          5.7 2011-08-18 00:35:00+00:00        -73.982738   \n",
       "3   3          7.7 2012-04-21 04:30:42+00:00        -73.987130   \n",
       "4   4          5.3 2010-03-09 07:51:00+00:00        -73.968095   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \\\n",
       "0        40.721319         -73.841610         40.712278                1   \n",
       "1        40.711303         -73.979268         40.782004                1   \n",
       "2        40.761270         -73.991242         40.750562                2   \n",
       "3        40.733143         -73.991567         40.758092                1   \n",
       "4        40.768008         -73.956655         40.783762                1   \n",
       "\n",
       "   dayofweek  hour  \n",
       "0          0    17  \n",
       "1          1    16  \n",
       "2          3     0  \n",
       "3          5     4  \n",
       "4          1     7  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate train and test together\n",
    "taxi = pd.concat([train, test])\n",
    "print(taxi.shape)\n",
    "print(taxi.dtypes)\n",
    "\n",
    "# Convert pickup date to datetime object\n",
    "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
    "print(taxi.dtypes)\n",
    "\n",
    "# Create a day of week feature\n",
    "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "# Create an hour feature\n",
    "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
    "\n",
    "# Split back into train and test\n",
    "new_train = taxi[taxi['id'].isin(train['id'])]\n",
    "new_test = taxi[taxi['id'].isin(test['id'])]\n",
    "\n",
    "new_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you know how to perform feature engineering for train and test DataFrames simultaneously. Having considered numerical and datetime features, move forward to master feature engineering for categorical ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features\n",
    "  \n",
    "We started working with numerical features in the previous lesson. In this lesson, we will generate some new features from categorical variables.\n",
    "  \n",
    "**Label encoding**\n",
    "  \n",
    "Consider the example of a categorical feature on the slide. The majority of machine learning models does not handle string values and categorical features automatically. So, before passing the data to the model we need to pre-process the categorical features into some meaningful numbers. There are lots of ways to encode categorical features. We'll consider two of the most popular options. The first one is label encoding. The idea is to map each category into the integer number. In this case, A is mapped into 0, B is mapped into 1 and so on.\n",
    "  \n",
    "<center><img src='../_images/categorical-features-kaggle.png' alt='img' width='740'></center>\n",
    "  \n",
    "To apply label encoding we will use `LabelEncoder` from `sklearn`. Firstly, create the object of this class. Then call the `.fit_transform()` method on the column needed. df is an example DataFrame from the previous slide. So, now we have label encoded categories! The problem with Label encoding is that we implicitly assume that there is a ranking dependency between the categories. For example, category C has label 2 which is much higher than category A with label 0. Such an approach is harmful to linear models, although it still works for tree-based models.\n",
    "  \n",
    "<center><img src='../_images/categorical-features-kaggle1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**One-Hot encoding**\n",
    "  \n",
    "To overcome the problem of ranking dependency between the categories, we could use one-hot encoding. In this type of encoding, we create a separate column for each of the categories. So, in this example, we created 4 columns instead of a single initial one. Then we set 1 for the corresponding category value and 0 for all other categories.\n",
    "  \n",
    "<center><img src='../_images/categorical-features-kaggle2.png' alt='img' width='740'></center>\n",
    "  \n",
    "There are multiple ways to implement one-hot encoding. We will consider `pandas`' `.get_dummies()` method. Let's call it on a column to be encoded specifying the `prefix=` parameter that will assign column names. Then we drop the initial categorical column, because it is not needed anymore. Lastly, we concatenate the original features with the one-hot encoded feature into a single DataFrame. The resulting DataFrame has the expected structure. The drawback of such approach arises if the feature has a lot of different categories. For example, if we have a feature with 1,000 different categories, we'll have to create 1,000 new columns.\n",
    "  \n",
    "<center><img src='../_images/categorical-features-kaggle3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Binary Features**\n",
    "  \n",
    "One special case of categorical features is binary features. It relates to categorical variables that have only two possible values. For example, Yes-No answers or whether some property is On or Off. For such features we always apply label encoding, substituting the first category with zero and the second category with one.\n",
    "  \n",
    "<center><img src='../_images/categorical-features-kaggle4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Other encoding approaches**\n",
    "  \n",
    "There is a long list of other categorical features encoders.\n",
    "  \n",
    "<center><img src='../_images/categorical-features-kaggle5.png' alt='img' width='740'></center>\n",
    "  \n",
    "The most widely used at Kaggle is target encoder. We will learn more about it in the next lesson.\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "But for now, let's get some practical experience with label and one-hot encoders!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "  \n",
    "Let's work on categorical variables encoding. You will again work with a subsample from the House Prices Kaggle competition.\n",
    "  \n",
    "Your objective is to encode categorical features `\"RoofStyle\"` and `\"CentralAir\"` using label encoding. The `train` and `test` DataFrames are already available in your workspace.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Concatenate `train` and `test` DataFrames into a single DataFrame `houses`.\n",
    "2. Create a `LabelEncoder` object without arguments and assign it to `le`.\n",
    "3. Create new label-encoded features for `\"RoofStyle\"` and `\"CentralAir\"` using the same le object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofStyle_enc</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>CentralAir_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gable</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gable</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gable</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gable</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gable</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RoofStyle  RoofStyle_enc CentralAir  CentralAir_enc\n",
       "0     Gable              1          Y               1\n",
       "1     Gable              1          Y               1\n",
       "2     Gable              1          Y               1\n",
       "3     Gable              1          Y               1\n",
       "4     Gable              1          Y               1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train = pd.read_csv('../_datasets/house_prices_train.csv')\n",
    "test = pd.read_csv('../_datasets/house_prices_test.csv')\n",
    "\n",
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Create new features\n",
    "houses['RoofStyle_enc'] = le.fit_transform(houses['RoofStyle'])\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
    "\n",
    "# Look at new features\n",
    "houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right! You can see that categorical variables have been label encoded. However, as you already know, label encoder is not always a good choice for categorical variables. Let's go further and apply One-Hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "  \n",
    "The problem with label encoding is that it implicitly assumes that there is a ranking dependency between the categories. So, let's change the encoding method for the features `\"RoofStyle\"` and `\"CentralAir\"` to one-hot encoding. Again, the train and test DataFrames from House Prices Kaggle competition are already available in your workspace.\n",
    "  \n",
    "Recall that if you're dealing with binary features (categorical features with only two categories) it is suggested to apply label encoder only.\n",
    "  \n",
    "Your goal is to determine which of the mentioned features is not binary, and to apply one-hot encoding only to this one.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Determine the distribution of `\"RoofStyle\"` and `\"CentralAir\"` features using `pandas`' `.value_counts()` method.\n",
    "2. Question  \n",
    "Which of the features is binary?  \n",
    "- [ ] \"RoofStyle\".\n",
    "- [x] \"CentralAir\".\n",
    "3. As long as `\"CentralAir\"` is a binary feature, encode it with a label encoder (0 - for one class and 1 - for another class).\n",
    "4. For the categorical feature `\"RoofStyle\"` let's use the one-hot encoder. Firstly, create one-hot encoded features using the `.get_dummies()` method. Then they are concatenated to the initial `houses` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoofStyle\n",
      "Gable      2310\n",
      "Hip         551\n",
      "Gambrel      22\n",
      "Flat         20\n",
      "Mansard      11\n",
      "Shed          5\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "CentralAir\n",
      "Y    2723\n",
      "N     196\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Look at feature distributions\n",
    "print(houses['RoofStyle'].value_counts(), '\\n')\n",
    "print(houses['CentralAir'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofStyle_Flat</th>\n",
       "      <th>RoofStyle_Gable</th>\n",
       "      <th>RoofStyle_Gambrel</th>\n",
       "      <th>RoofStyle_Hip</th>\n",
       "      <th>RoofStyle_Mansard</th>\n",
       "      <th>RoofStyle_Shed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gable</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gable</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gable</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gable</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gable</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RoofStyle  RoofStyle_Flat  RoofStyle_Gable  RoofStyle_Gambrel  \\\n",
       "0     Gable           False             True              False   \n",
       "1     Gable           False             True              False   \n",
       "2     Gable           False             True              False   \n",
       "3     Gable           False             True              False   \n",
       "4     Gable           False             True              False   \n",
       "\n",
       "   RoofStyle_Hip  RoofStyle_Mansard  RoofStyle_Shed  \n",
       "0          False              False           False  \n",
       "1          False              False           False  \n",
       "2          False              False           False  \n",
       "3          False              False           False  \n",
       "4          False              False           False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encode binary 'CentralAir' feature\n",
    "le = LabelEncoder()\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
    "\n",
    "# Create One-Hot encoded features\n",
    "ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')\n",
    "\n",
    "# Concatenate OHE features to houses\n",
    "houses = pd.concat([houses, ohe], axis=1)\n",
    "\n",
    "# Look at OHE features\n",
    "houses[[col for col in houses.columns if 'RoofStyle' in col]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you've mastered one-hot encoding as well! The one-hot encoded features look as expected. Remember to drop the initial string column, because models will not handle it automatically. OK, we're done with simple categorical encoders. Let's move to the target encoder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding\n",
    "  \n",
    "We eventually come to one of the secret sauces of Kaggle competitions. It's called target encoding.\n",
    "  \n",
    "**High cardinality categorical features**\n",
    "  \n",
    "To begin with, let's discuss high cardinality categorical features. These are categorical features that have a large number of category values (at least over 10 different category values). A label encoder would encode each category with a separate number. In case of high-cardinality, it means that we'll have a feature with lots of unordered integer numbers. Another option is a one-hot encoder. In this case, we have to create a large number of new features for each category value. So, the best alternative to the two methods above is target encoding. As a label encoder, it creates only a single column, but it also introduces the correlation between the categories and the target variable.\n",
    "  \n",
    "- Label encoder provides only distinct number for each category\n",
    "- One-hot encoder creates a new feature for each category value\n",
    "  \n",
    "**Mean target encoding**\n",
    "  \n",
    "There are various options for the encoding function. We will consider the most frequently used on Kaggle: the mean target encoding. Say we have a binary classification problem with a single categorical feature. On the left is our train data with known labels. On the right is our test data on which we'd like to make predictions.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle.png' alt='img' width='740'></center>\n",
    "  \n",
    "To apply mean target encoding to a particular feature we need to perform the following steps: First, we calculate the mean target value for each category on the whole train data. Then we apply these statistics to the corresponding category in the test data. Next, we divide the train data into folds. For each fold, we calculate the target mean on all the folds except for this particular one. It's called 'out-of-fold' data. Further, out-of-fold statistics are applied to this particular fold. This prevents overfitting to the train set. Now, both train and test data have this new feature. So, we can add this mean target encoded feature to our model.\n",
    "  \n",
    "1. Calculate mean on the train, apply to the test\n",
    "2. Split train into K folds, Calculate mean on (K-1) folds, apply to the K-th fold\n",
    "3. Add mean target encoded feature to the model\n",
    "  \n",
    "**Calculate mean on the train**\n",
    "  \n",
    "To encode categories in the test data, we simply take the whole train data and calculate mean target values for each category.\n",
    "  \n",
    "In this case, for category A it equals 0.66 (2 positive values out of 3 observations).\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle1.png' alt='img' width='740'></center>\n",
    "  \n",
    "And for category B it equals 0.25 (1 positive value out of 4 observations).\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Test encoding**\n",
    "  \n",
    "These statistics are applied to the corresponding category in the test data. As a result, we've obtained a new feature.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Train encoding using out-of-fold**\n",
    "  \n",
    "Now, we need to calculate this mean target encoded feature for the train data. As we said, we'll be using out-of-fold statistics. Let's split the train data into 2 folds: one and two.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle4.png' alt='img' width='740'></center>\n",
    "  \n",
    "Take fold number 1. We calculate the target mean out of this fold, so using only fold number 2 observations.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle5.png' alt='img' width='740'></center>\n",
    "  \n",
    "That's why category A obtains 0 and category B obtains 0.5.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle6.png' alt='img' width='740'></center>\n",
    "  \n",
    "Now we calculate out-of-fold target means for the second fold using only the first fold observations.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle7.png' alt='img' width='740'></center>\n",
    "  \n",
    "Thus, category A obtains 1 and category B obtains 0. We now have this mean encoded category in both the train and test data. So, we can use it as a new feature and pass to our model.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Practical guides**\n",
    "  \n",
    "Before moving to practice, let's discuss some practical tips that are always applied together with mean target encoding.\n",
    "  \n",
    "The first one is smoothing. Initially, for a specific category, we took a simple mean. However, if we had some rare categories with only one or two values, they would get a strict 0 or 1 mean encoding. It could lead to overfitting. That's why we introduce regularization. We first calculate the global mean. It is the target mean value for the whole train data. Then assume that we add alpha new observations with this global mean to each category. Now, if the category is large, we will trust the mean encoding, otherwise, we will stick to the global mean. Alpha is a hyperparameter we have to specify manually. Usually, values from 5 to 10 work pretty well by default.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle9.png' alt='img' width='740'></center>\n",
    "  \n",
    "Another practical advice is about new categories in the test data. In such case, we do not know what is the target mean value for this category. That's why new category values are simply filled in with a target global mean.\n",
    "  \n",
    "Take a look at the example. In the initial setting category A would get 0.5 and category B -- one third. However, with alpha equals 5, category A gets about 0.43 and category B -- about 0.38. While the new category C in the test data gets the global mean, that equals 0.4.\n",
    "  \n",
    "<center><img src='../_images/target-encoding-kaggle10.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "All right, let's transform these theoretical considerations into the Python code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean target encoding\n",
    "  \n",
    "First of all, you will create a function that implements mean target encoding. Remember that you need to develop the two following steps:\n",
    "  \n",
    "1. Calculate the mean on the train, apply to the test\n",
    "2. Split train into K folds. Calculate the out-of-fold mean for each fold, apply to this particular fold\n",
    "  \n",
    "Each of these steps will be implemented in a separate function: `test_mean_target_encoding()` and `train_mean_target_encoding()`, respectively.\n",
    "  \n",
    "The final function `mean_target_encoding()` takes as arguments: the `train` and `test` DataFrames, the name of the categorical column to be encoded, the name of the target column and a smoothing parameter alpha. It returns two values: a new feature for train and test DataFrames, respectively.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. You need to add smoothing to avoid overfitting. So, add α parameter to the denominator in `train_statistics` calculations.\n",
    "2. You need to treat new categories in the test data. So, pass a global mean as an argument to the `.fillna()` method.\n",
    "3. To calculate the train mean encoded feature you need to use out-of-fold statistics, splitting train into several folds. Specify the train and test indices for each validation split to access it.\n",
    "4. Finally, you just calculate train and test target mean encoded features and return them from the function. So, return `train_feature` and `test_feature` obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
    "    # Calculate global mean on the train data\n",
    "    global_mean = train[target].mean()\n",
    "    \n",
    "    # Group by the categorical feature and calculate its properties\n",
    "    train_groups = train.groupby(categorical)\n",
    "    category_sum = train_groups[target].sum()\n",
    "    category_size = train_groups.size()\n",
    "    \n",
    "    # Calculate smoothed mean target statistics\n",
    "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
    "    \n",
    "    # Apply statistics to the test data and fill new categories\n",
    "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
    "    return test_feature.values\n",
    "\n",
    "\n",
    "def train_mean_target_encoding(train, target, categorical, alpha=5):\n",
    "    # Create 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5,random_state=123, shuffle=True)\n",
    "    train_feature = pd.Series(index=train.index, dtype='float')\n",
    "    \n",
    "    # For each folds split\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "        \n",
    "        # Calculate out-of-fold statistics and apply to cv_test\n",
    "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, \n",
    "                                                    categorical, alpha)\n",
    "        \n",
    "        # Save new feature for this particular fold\n",
    "        train_feature.iloc[test_index] = cv_test_feature\n",
    "    return train_feature.values\n",
    "\n",
    "\n",
    "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
    "    # Get the train feature\n",
    "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
    "    \n",
    "    # Get the test feature\n",
    "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
    "    \n",
    "    # Return new features to add to the model\n",
    "    return train_feature, test_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you are equipped with a function that performs mean target encoding of any categorical feature. Move on to learn how to implement mean target encoding for the K-fold cross-validation using the `mean_target_encoding()` function you've just built!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "  \n",
    "You will work with a binary classification problem on a subsample from Kaggle playground competition. The objective of this competition is to predict whether a famous basketball player Kobe Bryant scored a basket or missed a particular shot.\n",
    "  \n",
    "Train data is available in your workspace as `bryant_shots` DataFrame. It contains data on 10,000 shots with its properties and a target variable `\"shot\\_made\\_flag\"` -- whether shot was scored or not.\n",
    "  \n",
    "One of the features in the data is `\"game_id\"` -- a particular game where the shot was made. There are 541 distinct games. So, you deal with a high-cardinality categorical feature. Let's encode it using a target mean!\n",
    "  \n",
    "Suppose you're using 5-fold cross-validation and want to evaluate a mean target encoded feature on the local validation.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. To achieve this, you need to repeat encoding procedure for the `\"game_id\"` categorical feature inside each folds split separately. Your goal is to specify all the missing parameters for the `mean_target_encoding()` function call inside each folds split.\n",
    "2. Recall that the train and test parameters expect the `train` and `test` DataFrames.\n",
    "3. While the target and categorical parameters expect names of the target variable and categorical feature to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "       game_id  shot_made_flag  game_id_enc\n",
      "5245  20400027             0.0      0.47706\n",
      "       game_id  shot_made_flag  game_id_enc\n",
      "3780  20200842             0.0     0.343787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       game_id  shot_made_flag  game_id_enc\n",
      "4649  20300500             1.0     0.311673\n",
      "       game_id  shot_made_flag  game_id_enc\n",
      "9333  20601057             1.0     0.285334\n",
      "       game_id  shot_made_flag  game_id_enc\n",
      "8442  20600340             1.0     0.421983\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "bryant_shots = pd.read_csv('../_datasets/bryant_shots.csv')\n",
    "print(bryant_shots.shape)\n",
    "\n",
    "# Create 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "# For each folds split\n",
    "for train_index, test_index in kf.split(bryant_shots):\n",
    "    cv_train, cv_test = bryant_shots.iloc[train_index].copy(), bryant_shots.iloc[test_index].copy()\n",
    "    \n",
    "    # Create mean target encoded feature\n",
    "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(\n",
    "        train=cv_train,\n",
    "        test=cv_test,\n",
    "        target='shot_made_flag',\n",
    "        categorical='game_id',\n",
    "        alpha=5\n",
    "    )\n",
    "    \n",
    "    # Look at the encoding\n",
    "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))\n",
    "\n",
    "print(bryant_shots.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You could see different game encodings for each validation split in the output. The main conclusion you should make: while using local cross-validation, you need to repeat mean target encoding procedure inside each folds split separately. Go on to try other problem types beyond binary classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond binary classification\n",
    "  \n",
    "Of course, binary classification is just a single special case. Target encoding could be applied to any target variable type:\n",
    "  \n",
    "- For **binary classification** usually mean target encoding is used\n",
    "- For **regression** mean could be changed to median, quartiles, etc.\n",
    "- For **multi-class classification** with $N$ classes we create $N$ features with target mean for each category in one vs. all fashion\n",
    "  \n",
    "The `mean_target_encoding()` function you've created could be used for any target type specified above. Let's apply it for the regression problem on the example of House Prices Kaggle competition.\n",
    "  \n",
    "Your goal is to encode a categorical feature `\"RoofStyle\"` using mean target encoding. The `train` and `test` DataFrames are already available in your workspace.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Specify all the missing parameters for the `mean_target_encoding()` function call. Target variable name is `\"SalePrice\"`. Set α hyperparameter to 10.\n",
    "2. Recall that the `train=` and `test=` parameters expect the `train` and `test` DataFrames.\n",
    "3. While the `target` and `categorical` parameters expect names of the target variable and feature to be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofStyle_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gable</td>\n",
       "      <td>171565.947836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hip</td>\n",
       "      <td>217594.645131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Gambrel</td>\n",
       "      <td>164152.950424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Flat</td>\n",
       "      <td>188703.563431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Mansard</td>\n",
       "      <td>180775.938759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Shed</td>\n",
       "      <td>188267.663242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RoofStyle  RoofStyle_enc\n",
       "0        Gable  171565.947836\n",
       "1          Hip  217594.645131\n",
       "98     Gambrel  164152.950424\n",
       "133       Flat  188703.563431\n",
       "362    Mansard  180775.938759\n",
       "1053      Shed  188267.663242"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../_datasets/house_prices_train.csv')\n",
    "test = pd.read_csv('../_datasets/house_prices_test.csv')\n",
    "\n",
    "# Create mean target encoded feature\n",
    "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
    "                                                                     test=test, \n",
    "                                                                     target='SalePrice',\n",
    "                                                                     categorical='RoofStyle',\n",
    "                                                                     alpha=10)\n",
    "# Look at the encoding\n",
    "test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you observe that houses with the `Hip` roof are the most pricy, while houses with the `Gambrel` roof are the cheapest. It's exactly the goal of target encoding: you've encoded categorical feature in such a manner that there is now a correlation between category values and target variable. We're done with categorical encoders. Now it's time to talk about the missing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "  \n",
    "We've considered some feature types and how to create new features for them. In this lesson, we will deal with the missing data.\n",
    "  \n",
    "**Missing data**\n",
    "  \n",
    "Some machine learning algorithms like XGBoost or LightGBM can treat missing data without any preprocessing. However, it's always a good idea to implement your own missing value imputation in order to improve the model. For example, consider the data presented on the slide. Let's assume that we need to solve the binary classification problem with labels 0 and 1. We have one categorical feature and one numerical feature. We'll consider how to deal with the missing values in order to impute gaps in the data. For example, observations with IDs 4 and 5 have missing values. Note that they are denoted as `'NaN'` values in `pandas` DataFrames.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Impute missing data**\n",
    "  \n",
    "For the numerical features the simplest method is mean or median imputing. It means that we fill each missing value with the mean or median of the available observations.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle1.png' alt='img' width='740'></center>\n",
    "  \n",
    "In this example, we would change the missing value to 4.72. However, imputation with mean or median just assigns an average observation to the missing value. So, we lose the information that this value was actually missing. To emphasize that the data was missing sometimes special constant values are used.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle2.png' alt='img' width='740'></center>\n",
    "  \n",
    "For example, -999. It's not a good choice for linear models but works perfectly for tree-based models.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle3.png' alt='img' width='740'></center>\n",
    "  \n",
    "To impute the categorical features we again have two choices. Either to fill in the most frequent category in the data,\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle4.png' alt='img' width='740'></center>\n",
    "  \n",
    "In this example it would be category A. Or create a new category for the missing values. It again allows the model to get information that this observation had missing value.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle5.png' alt='img' width='740'></center>\n",
    "  \n",
    "For example, create a new category `'MISS'` and fill in the missing value.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle6.png' alt='img' width='740'></center>\n",
    "  \n",
    "1. Numerical data\n",
    "- Mean/median imputation\n",
    "- Constant value imputation\n",
    "  \n",
    "2. Categorical data\n",
    "- Most frequent category imputation\n",
    "- New category imputation\n",
    "  \n",
    "**Find missing data**\n",
    "  \n",
    "Let df be the `pandas` DataFrame from the example table presented on the previous slides. The `pandas`' method `.isnull()` returns the DataFrame with Booleans as cell values. If the value is missing, it returns `True`. If the value is present, it returns `False`. Therefore, we could call the `.sum()` method on this DataFrame and obtain the number of missing values in each column. In this case, we have one missing categorical feature and one missing numerical feature.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Numerical missing data**\n",
    "  \n",
    "Let's now consider Python implementation. Again we will use the scikit-learn package. Import `SimpleImputer` from the `impute` module. To impute numerical data, we could create an object of this class. For mean imputing, we set the `strategy=` parameter to `'mean'`. For constant imputing, we set the `strategy=` to `'constant'` and specify the filling value (in this example, -999). Finally, we impute the value applying the `.fit_transform()` method to the selected columns. Note that we could select multiple columns to be imputed simultaneously. For this purpose, just pass the list of columns to be imputed. Note that even if we want to impute a single column, we have to use double brackets.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Categorical missing data**\n",
    "  \n",
    "Imputation of categorical missing data is absolutely similar. We again could use two different strategies: the most frequent category or constant category. In this case, for example, category `'MISS'` for the missing data. Then we apply the selected imputer to the list of columns we'd like to impute.\n",
    "  \n",
    "<center><img src='../_images/missing-data-kaggle9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "So, now you know the approaches to impute missing data. Let's polish them on practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing data\n",
    "  \n",
    "Let's impute missing data on a real Kaggle dataset. For this purpose, you will be using a data subsample from the Kaggle \"Two sigma connect: rental listing inquiries\" competition.\n",
    "  \n",
    "Before proceeding with any imputing you need to know the number of missing values for each of the features. Moreover, if the feature has missing values, you should explore the type of this feature.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Read the `\"twosigma_train.csv\"` file using `pandas`.\n",
    "2. Find the number of missing values in each column.\n",
    "3. Select the columns with the missing values and look at the head of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 0\n",
      "bathrooms          0\n",
      "bedrooms           0\n",
      "building_id       13\n",
      "latitude           0\n",
      "longitude          0\n",
      "manager_id         0\n",
      "price             32\n",
      "interest_level     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53a5b119ba8f7b61d4e010512e0dfc85</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c5c8a357cba207596b04d1afd1e4f130</td>\n",
       "      <td>5465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3ba40552e2120b0acfc3cb5730bb2aa</td>\n",
       "      <td>2850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28d9ad350afeaab8027513a3e52ac8d5</td>\n",
       "      <td>3275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3350.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        building_id   price\n",
       "0  53a5b119ba8f7b61d4e010512e0dfc85  3000.0\n",
       "1  c5c8a357cba207596b04d1afd1e4f130  5465.0\n",
       "2  c3ba40552e2120b0acfc3cb5730bb2aa  2850.0\n",
       "3  28d9ad350afeaab8027513a3e52ac8d5  3275.0\n",
       "4                               NaN  3350.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataframe\n",
    "twosigma = pd.read_csv('../_datasets/twosigma_rental_train_null.csv')\n",
    "\n",
    "# find the number of missing values in each column\n",
    "print(twosigma.isnull().sum())\n",
    "\n",
    "twosigma[['building_id', 'price']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, you've found out that `'building_id'` and `'price'` columns have missing values. Looking at the head of the DataFrame, we may conclude that `'price'` is a numerical feature, while `'building_id'` is a categorical feature that is encoding buildings as hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data\n",
    "  \n",
    "You've found that \"price\" and \"building_id\" columns have missing values in the Rental Listing Inquiries dataset. So, before passing the data to the models you need to impute these values.\n",
    "  \n",
    "Numerical feature \"price\" will be encoded with a mean value of non-missing prices.\n",
    "  \n",
    "Imputing categorical feature \"building_id\" with the most frequent category is a bad idea, because it would mean that all the apartments with a missing \"building_id\" are located in the most popular building. The better idea is to impute it with a new category.\n",
    "  \n",
    "The DataFrame `rental_listings` with competition data is read for you.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Create a `SimpleImputer` object with \"mean\" `strategy=`. Impute missing prices with the mean value.\n",
    "2. Create an imputer with \"constant\" `strategy=`. Use \"MISSING\" as `fill_value=`. Impute missing buildings with a constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create mean imputer\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Price imputation\n",
    "twosigma[['price']] = mean_imputer.fit_transform(twosigma[['price']])\n",
    "\n",
    "# Create constant inputer\n",
    "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "\n",
    "# building_id imputation\n",
    "twosigma[['building_id']] = constant_imputer.fit_transform(twosigma[['building_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "bathrooms         0\n",
       "bedrooms          0\n",
       "building_id       0\n",
       "latitude          0\n",
       "longitude         0\n",
       "manager_id        0\n",
       "price             0\n",
       "interest_level    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twosigma.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now our data is ready to be passed to any Machine Learning model. Move on to the next chapter to build and improve your models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
