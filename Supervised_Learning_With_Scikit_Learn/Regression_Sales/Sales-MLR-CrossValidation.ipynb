{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99894062 0.99909245 0.9990103  0.99896344 0.99889153 0.99903953]\n"
     ]
    }
   ],
   "source": [
    "# Multiple Linear Regression Machine Learning In Python\n",
    "\n",
    "# Demostration of MLR, cross-validation, evaluating cross-validation.\n",
    "\n",
    "# y = a1x1 + a2x2 + a3x3 + ... + anxn + b\n",
    "# y = target\n",
    "# x = single feature\n",
    "# a,b = parameters/coefficients of the model - slope,intercept\n",
    "\n",
    "# How do we choose a and b?\n",
    "# - Define an error function for any given line\n",
    "# - Choose the line that minimizes the error function\n",
    "# Error function = lost function = cost function\n",
    "# Regression minimizes a loss function to choose a coefficient 'a', for each feature and the intercept 'b'. If we allow these coefficients to become too large = overfitting.\n",
    "\n",
    "\n",
    "# __________Terms__________\n",
    "# Residual:                     is the difference between the expected results from a model and the true values from data.\n",
    "# Variance:                     is the variability in the expected results (predictions) of a given data point between different runs of the model.\n",
    "# R-squared:                    is the absolute amount of variation as a proportion of total variation. quantifies the amount of variance in the target variable that is explained by the features. Ranges from 0 to 1, 0=low, 1=high\n",
    "# Mean Squared Error (MSE):     measures the amount of error in statistical models. It assesses the average squared difference between the observed and predicted values. When a model has no error, the MSE equals zero. As model error increases, its value increases.\n",
    "# Root Mean Squared Error(RMSE):Root mean square error or root mean square deviation is one of the most commonly used measures for evaluating the quality of predictions. It shows how far predictions fall from measured true values using Euclidean distance.\n",
    "# RSS:                          residual sum of squares, The residual sum of squares (RSS) is the absolute amount of explained variation.\n",
    "# Ordinary Least Squares(OLS):  Goal is to Minimize RSS. A Common technique for estimating coefficients of linear regression equations which describe the relationship\n",
    "#                               between one or more independent quantitative variables and a dependent variable (simple or multiple linear regression).\n",
    "#                               OLS estimators minimize the sum of the squared errors (a difference between observed values and predicted values).\n",
    "# - Advantages of OLS:          OLS is the most efficient linear regression estimator when the assumptions hold true. \n",
    "#                               Another benefit of satisfying these assumptions is that as the sample size increases to infinity, the coefficient estimates converge on the actual population parameters.\n",
    "# - Disadvantages of OLS:       As with OLS, a large data set is necessary in order to obtain reliable results. \n",
    "#                               The regression results are sensitive to functional form if the error term is not adequately interpreted, which can lead to widely varying conclusions depending on how the regression is initially set up.\n",
    "# Cross Fold Validation:        Folds the training data over in nth folds. In 5-fold the data would be spliced in 5ths, then 4 would be used to compare on the 5th, iterates 5 times to use each block/fold as a validation. More folds = higher computational expense.\n",
    "#                               Cross-validation is a vital approach to evaluating a model. It maximizes the amount of data that is available to the model, as the model is not only trained but also tested on all of the available data.\n",
    "#                               By using cross-validation, we can see how performance varies depending on how the data is split.\n",
    "# Hyperparameter:               Variable used to to optimize model parameters.\n",
    "# Regularization:               Penalizes large coefficients.\n",
    "# - Ridge Regression:           Ridge penalizes large positive or negative coefficients. contains the hyperprameter Alpha which is simular to Kappa in KNN. Alpha controls model complexity.\n",
    "#                               When Alpha = 0 we are preforming OLS (Can lead to overfitting). A very high Alpha can lead extreme penalization of coefficients ie. underfitting.\n",
    "# - Lasso Regression:           Can be used to select feature importance, as it actually shrinks the coefficients of least importance to 0. The features not reduced will be selected by Lasso.\n",
    "\n",
    "\n",
    "# pip3 install pandas\n",
    "# pip3 install scikit-learn\n",
    "# pip3 install numpy\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "\n",
    "# Importing the 'advertising_and_sales_clean.csv' dataset as a pandas dataframe\n",
    "sales_df = pd.read_csv('../../_datasets/advertising_and_sales_clean.csv')\n",
    "\n",
    "# Removing column \"influencer\"\n",
    "sales_df = sales_df.drop(\"influencer\", axis=1)\n",
    "\n",
    "# Create X and y arrays, X represents the features, y represents the target.\n",
    "X = sales_df.drop('sales', axis=1).values\n",
    "y = sales_df['sales'].values\n",
    "\n",
    "#Â Create a KFold object\n",
    "# KFold() is splitting data into 6 folds/blocks, randomizing the data before batching it into blocks, and setting seed to 5\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
    "\n",
    "# Instantiate the model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Compute 6-fold cross-validation scores\n",
    "cv_results = cross_val_score(reg, X, y, cv=kf)\n",
    "\n",
    "# Print the n number of R^2 scores from each block/fold\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9989896443678249\n",
      "6.608118371529651e-05\n",
      "[0.99889767 0.99908583]\n"
     ]
    }
   ],
   "source": [
    "# Print the mean\n",
    "print(np.mean(cv_results))\n",
    "\n",
    "# Print the standard deviation\n",
    "print(np.std(cv_results))\n",
    "\n",
    "# Print the 95% confidence interval\n",
    "# 0.975 to get a two-sided confidence interval. This gives 2.5% of the probability in the upper tail and 2.5% in the lower tail.\n",
    "print(np.quantile(cv_results, [0.025, 0.975]))\n",
    "\n",
    "# An average score of 0.9989896443678249 with a low standard deviation is very high for a model out of the box."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52d43516e08ba44a91236334f3ff506a57085b07359b42e8a57478a41bcad1ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
