{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF and similarity scores\n",
    "  \n",
    "Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "  \n",
    "[SpaCy Documentation](https://spacy.io)  \n",
    "[Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  \n",
    "[List of LaTeX mathematical symbols](https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols)  \n",
    "[Classical ML Equations in LaTeX](https://blmoistawinde.github.io/ml_equations_latex/)  \n",
    "[SpaCy en_core_web_lg Documentation](https://spacy.io/models/en)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 11:38:12.724119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import re                           # Regular Expressions:      Text manipulation\n",
    "import spacy                        # Spatium Cython:           Natural Language Processing\n",
    "from pprint import pprint           # Pretty Print:             Advanced printing operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building tf-idf document vectors\n",
    "  \n",
    "In the last chapter, we learned about n-gram modeling.\n",
    "  \n",
    "**n-gram modeling**\n",
    "  \n",
    "In n-gram modeling, the weight of a dimension for the vector representation of a document is dependent on the number of times the word corresponding to the dimension occurs in the document. Let's say we have a document that has the word 'human' occurring 5 times. Then, the dimension of its vector representation corresponding to 'human' would have the value 5. Think about a matrix of vocabulary terms, where 'human' is an instance in the matrix.\n",
    "  \n",
    "**Motivation**\n",
    "  \n",
    "However, some words occur very commonly across all the documents in the corpus. As a result, the vector representations get more characterized by these dimensions. Consider a corpus of documents on the Universe. Let's say there is a particular document on Jupiter where the word 'jupiter' and 'universe' both occur about 20 times. However, 'jupiter' rarely figures in the other documents whereas 'universe' is just as common. We could argue that although both *jupiter* and *universe* occur 20 times, *jupiter* should be given a larger weight on account of its exclusivity. In other words, the word 'jupiter' characterizes the document more than 'universe'.\n",
    "  \n",
    "**Applications**\n",
    "  \n",
    "Weighting words this way has a huge number of applications. They can be used to automatically detect stopwords for the corpus instead of relying on a generic list. They're used in search algorithms to determine the ranking of pages containing the search query and in recommender systems as we will soon find out. In a lot of cases, this kind of weighting also generates better performance during predictive modeling.\n",
    "  \n",
    "- Automatically detect stopwords  \n",
    "- Search algorithms  \n",
    "- Recommendation systems  \n",
    "- Better performance for predictive modeling in some cases  \n",
    "  \n",
    "**Term frequency-inverse document frequency**\n",
    "  \n",
    "The weighting mechanism we've described is known as term frequency-inverse document frequency or tf-idf for short. It is based on the idea that the weight of a term in a document should be proportional to its frequency and an inverse function of the number of documents in which it occurs.\n",
    "  \n",
    "$formula.$\n",
    "  \n",
    "$\\Large w_{i, j} = \\text{tf}_{i, j} \\cdot \\log (\\frac{N}{\\text{df}_{i}})$\n",
    "  \n",
    "$where.$\n",
    "  \n",
    "$w_{i,j}$ = Weight of term $i$ in document $j$  \n",
    "$tf_{i,j}$ = Term frequency of term $i$ in document $j$  \n",
    "$N$ = Number of documents in the corpus  \n",
    "$df_{i}$ = Number of documents containing term $i$  \n",
    "\n",
    "\n",
    "**Mathematical formula**\n",
    "  \n",
    "Therefore, let's say the word 'library' occurs in a document 5 times. There are 20 documents in the corpus and 'library' occurs in 8 of them. Then, the tf-idf weight of 'library' in the vector representation of this document will be 5 times log of 20 by 8 which is approximately 2. In general, higher the tf-idf weight, more important is the word in characterizing the document. A high tf-idf weight for a word in a document may imply that the word is relatively exclusive to that particular document or that the word occurs extremely commonly in the document, or both.\n",
    "  \n",
    "$example.$\n",
    "  \n",
    "$\\Large w_{library, document} = \\text{5}_{library, document} \\cdot \\log (\\frac{20}{\\text{8}_{library}}) \\approx 2$\n",
    "  \n",
    "**tf-idf using `scikit-learn`**\n",
    "  \n",
    "Generating vectors that use tf-idf weighting is almost identical to what we've already done so far. Instead of using `CountVectorizer`, we use the `TfidfVectorizer` class of `scikit-learn`. The parameters and methods it has is almost identical to `CountVectorizer`. The only difference is that `TfidfVectorizer` assigns weights using the tf-idf formula from before and has extra parameters related to inverse document frequency which we will not cover in this course. Here, we can see how using `TfidfVectorizer` is almost identical to using `CountVectorizer` for a corpus. However, notice that the weights are non-integer and reflect values calculated by the tf-idf formula.\n",
    "  \n",
    "<img src='../_images/tf-idf-sklearn-tfidfvect.png' alt='img' width='740'>\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf weight of commonly occurring words\n",
    "  \n",
    "The word `bottle` occurs 5 times in a particular document `D` and also occurs in every document of the corpus. What is the tf-idf weight of `bottle` in `D`?\n",
    "  \n",
    "Possible Answers\n",
    "\n",
    "- [x] 0\n",
    "- [ ] 1\n",
    "- [ ] Not defined\n",
    "- [ ] 5\n",
    "  \n",
    "Correct! In fact, the tf-idf weight for `bottle` in every document will be 0. This is because the inverse document frequency is constant across documents in a corpus and since `bottle` occurs in every document, its value is log(1), which is 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf vectors for TED talks\n",
    "  \n",
    "In this exercise, you have been given a corpus `ted` which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
    "  \n",
    "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript.\n",
    "  \n",
    "1. Import `TfidfVectorizer` `from sklearn`.\n",
    "2. Create a `TfidfVectorizer` object. Name it `vectorizer`.\n",
    "3. Generate `tfidf_matrix` for `ted` using the `.fit_transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../_datasets/ted.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the transcript feature\n",
    "ted = df['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29158)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to generate tf-idf vectors for a given corpus of text. You can use these vectors to perform predictive modeling just like we did with `CountVectorizer`. In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "  \n",
    "We now know how to compute vectors out of text documents. With this representation in mind, let us now explore techniques that will allow us to determine how similar two vectors and consequentially two documents, are to each other. More specifically, we will learn about the cosine similarity score which is one of the most popularly used similarity metrics in NLP.\n",
    "  \n",
    "**Mathematical formula**\n",
    "  \n",
    "Very simply put, the cosine similarity score of two vectors is the cosine of the angle between the vectors. Mathematically, it is the ratio of the dot product of the vectors and the product of the magnitude of the two vectors. Let's walk through what this formula really means.\n",
    "  \n",
    "$formula.$\n",
    "  \n",
    "$\\Large Cosine(x,y) = \\frac{x \\cdot y}{|x||y|}$\n",
    "  \n",
    "<img src='../_images/cosine-distance-simularity-score.png' alt='img' width='740'>\n",
    "  \n",
    "**The dot product**\n",
    "  \n",
    "The dot product is computed by summing the product of values across corresponding dimensions of the vectors. Let's say we have two $n$-dimensional vectors $V$ and $W$ as shown. Then, the dot product here would be $V_1$ times $W_1$ plus $V_2$ times $W_2$ and so on until $V_n$ times $W_n$. As an example, consider two vectors $A$ and $B$. By applying the formula, we see that the dot product comes to 37.\n",
    "  \n",
    "Consider two vectors,  \n",
    "$\\Large V = (v_1, v_2, \\dots, v_n), W = (w_1, w_2, \\dots, w_n)$\n",
    "  \n",
    "Then the dot product of $V$ and $W$ is,  \n",
    "$\\Large V \\cdot W = (v_1 \\times w_1) + (v_2 \\times w_2) + \\dots + (v_n \\times w_n)$\n",
    "  \n",
    "$example.$\n",
    "  \n",
    "$A = (4, 7, 1)$  \n",
    "$B = (5, 2, 3)$  \n",
    "  \n",
    "$A • B = (4*5) + (7*2) + (1*3)$  \n",
    "$20 + 14 + 3 = 37$  \n",
    "  \n",
    "<img src='../_images/cosine-distance-simularity-score1.png' alt='img' width='400'>\n",
    "  \n",
    "**Magnitude of a vector**\n",
    "  \n",
    "The magnitude of a vector is essentially the length of the vector. Mathematically, it is defined as the square root of the sum of the squares of values across all the dimensions of a vector. Therefore, for an $n$-dimensional vector $V$ the magnitude, mod $V$ is computed as the square root of $V_1$ square plus $V_2$ square and so on until $V_n$ square. Consider the vector $A$ from before. Using the above formula, we compute its magnitude to be root 66.\n",
    "  \n",
    "For any vector,  \n",
    "$\\Large V = (v_1, v_2, \\dots, v_n)$\n",
    "  \n",
    "The magnitude is defined as,  \n",
    "$\\Large \\Vert V \\Vert = \\sqrt{(v_1)^2 + (v_2)^2 + \\dots + (v_n)^2}$\n",
    "  \n",
    "$example.$\n",
    "  \n",
    "$A = (4, 7, 1)$  \n",
    "$B = (5, 2, 3)$  \n",
    "  \n",
    "$\\Large \\Vert A \\Vert = \\sqrt{(4)^2 + (7)^2 + (1)^2}$  \n",
    "$\\Large \\Vert A \\Vert = \\sqrt{16 + 49 + 1} \\approx \\sqrt{66}$  \n",
    "  \n",
    "$\\Large \\Vert B \\Vert = \\sqrt{(5)^2 + (2)^2 + (3)^2}$  \n",
    "$\\Large \\Vert B \\Vert = \\sqrt{25 + 4 + 9} \\approx \\sqrt{38}$  \n",
    "  \n",
    "<img src='../_images/cosine-distance-simularity-score2.png' alt='img' width='400'>\n",
    "  \n",
    "**The cosine score**\n",
    "  \n",
    "We are now in a position to compute the cosine similarity score of $A$ and $B$. It is the dot product, which is 37, divided by the product of the magnitudes of $A$ and $B$, which are root 66 and root 38 respectively. The value comes out to be approximately 0.738, which is the value of the cosine($\\cos$) of the angle($\\measuredangle$) theta($\\theta$) between the two vectors $\\text{represented as} \\cos(\\theta) \\measuredangle \\vec{A},\\vec{B}$.\n",
    "  \n",
    "$example.$\n",
    "  \n",
    "$A = (4, 7, 1)$  \n",
    "$B = (5, 2, 3)$  \n",
    "  \n",
    "$\\Large Cosine(A,B) = \\frac{A \\cdot B}{|A||B|}$  \n",
    "  \n",
    "$\\Large Cosine(A,B) = \\frac{37}{(\\sqrt{66}) \\times (\\sqrt{38})} \\approx 0.738$  \n",
    "  \n",
    "<img src='../_images/cosine-distance-simularity-score3.png' alt='img' width='390'>  \n",
    "  \n",
    "**Cosine Score: points to remember**\n",
    "  \n",
    "Since the cosine score is simply the cosine of the angle between two vectors, its value is bounded between -1 and 1. However, in NLP, document vectors almost always use non-negative weights. Therefore, cosine scores vary between 0 and 1 where 0 indicates no similarity and 1 indicates that the documents are identical. Finally, since the cosine score ignores the magnitude of the vectors, it is fairly robust to document length. This may be an advantage or a disadvantage depending on the use case.\n",
    "  \n",
    "- Value between -1 and 1  \n",
    "- In NLP, value between 0 (no similarity) and 1 (same)  \n",
    "- Robust to document length  \n",
    "  \n",
    "**Implementation using scikit-learn**\n",
    "  \n",
    "Scikit-learn offers a `cosine_similarity()` function that outputs a similarity matrix containing the pairwise cosine scores for a set of vectors. You can import `cosine_similarity` `from sklearn.metrics.pairwise`. However, remember that `cosine_similarity()` takes in 2-D arrays as arguments. Passing in 1-D arrays will throw an error. Let us compute the cosine similarity scores of vectors $A$ and $B$ from before. We see that we get the same answer of 0.738 from before.\n",
    "  \n",
    "<img src='../_images/cosine-distance-simularity-score4.png' alt='img' width='740'>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range of cosine scores\n",
    "  \n",
    "Which of the following is a possible cosine score for a pair of document vectors?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [x] 0.86\n",
    "- [ ] -0.52\n",
    "- [ ] 2.36\n",
    "- [ ] -1.32\n",
    "  \n",
    "Great job! Since document vectors use only non-negative weights, the cosine score lies between 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing dot product\n",
    "  \n",
    "In this exercise, we will learn to compute the dot product between two vectors, `A` = (1, 3) and `B` = (-2, 2), using the `numpy` library. More specifically, we will use the `np.dot()` function to compute the dot product of two `numpy` arrays.\n",
    "  \n",
    "1. Initialize `A` (1,3) and `B` (-2,2) as `numpy` arrays using `np.array()`.\n",
    "2. Compute the dot product using `np.dot()` and passing `A` and `B` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Initialize numpy vectors\n",
    "A = np.array([1, 3])\n",
    "B = np.array([-2, 2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! The dot product of the two vectors is 1 * -2 + 3 * 2 = 4, which is indeed the output produced. We will not be using `np.dot()` too much in this course but it can prove to be a helpful function while computing dot products between two standalone vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity matrix of a corpus\n",
    "  \n",
    "In this exercise, you have been given a `corpus`, which is a list containing five sentences. The `corpus` is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
    "  \n",
    "```python\n",
    "corpus:\n",
    " ['The sun is the largest celestial body in the solar system', \n",
    " 'The solar system consists of the sun and eight revolving planets', \n",
    " 'Ra was the Egyptian Sun God', \n",
    " 'The Pyramids were the pinnacle of Egyptian architecture', \n",
    " 'The quick brown fox jumps over the lazy dog']\n",
    "```\n",
    "  \n",
    "Remember, the value corresponding to the $i$-th row and $j$-th column of a similarity matrix denotes the similarity score for the $i$-th and $j$-th vector.\n",
    "  \n",
    "1. Initialize an instance of `TfidfVectorizer`. Name it `tfidf_vectorizer`.\n",
    "2. Using `.fit_transform()`, generate the tf-idf vectors for `corpus`. Name it `tfidf_matrix`.\n",
    "3. Use `.cosine_similarity()` and pass `tfidf_matrix` to compute the cosine similarity matrix `cosine_sim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The sun is the largest celestial body in the solar system', \n",
    "          'The solar system consists of the sun and eight revolving planets', \n",
    "          'Ra was the Egyptian Sun God', \n",
    "          'The Pyramids were the pinnacle of Egyptian architecture', \n",
    "          'The quick brown fox jumps over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus, datatype is a csr_matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders. From our similarity matrix, we see that the first and the second sentence are the most similar. Also the fifth sentence has, on average, the lowest pairwise cosine scores. This is intuitive as it contains entities that are not present in the other sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a plot line based recommender\n",
    "  \n",
    "In this lesson, we will use tf-idf vectors and cosine scores to build a recommender system that suggests movies based on overviews.\n",
    "  \n",
    "**Movie recommender**\n",
    "  \n",
    "We've a dataset containing movie overviews. Here, we can see two movies, Shanghai Triad and Cry, the Beloved Country and their overviews.\n",
    "  \n",
    "<img src='../_images/building-a-plot-line-based-recommender.png' alt='img' width='740'>\n",
    "  \n",
    "**Movie recommender**\n",
    "  \n",
    "Our task is to build a system that takes in a movie title and outputs a list of movies that has similar plot lines. For instance, if we passed in 'The Godfather', we could expect output like this. Notice how a lot of the movies listed here have to do with crime and gangsters, just like The Godfather.\n",
    "  \n",
    "<img src='../_images/building-a-plot-line-based-recommender1.png' alt='img' width='740'>\n",
    "  \n",
    "**Steps**\n",
    "  \n",
    "Following are the steps involved. The first step, as always, is to preprocess movie overviews. The next step is to generate the tf-idf vectors for our overviews. Finally, we generate a cosine similarity matrix which contains the pairwise similarity scores of every movie with every other movie. Once the cosine similarity matrix is computed, we can proceed to build the recommender function.\n",
    "  \n",
    "1. Text preprocessing  \n",
    "2. Generate tf-idf vectors  \n",
    "3. Generate cosine similarity matrix  \n",
    "4. Recommender function  \n",
    "  \n",
    "**The recommender function**\n",
    "  \n",
    "We will build a recommender function as part of this course. Let's take a look at how it works. The recommender function takes a movie title, the cosine similarity matrix and an indices series as arguments. The indices series is a reverse mapping of movie titles with their indices in the original dataframe. The function extracts the pairwise cosine similarity scores of the movie passed in with every other movie. Next, it sorts these scores in descending order. Finally, it outputs the titles of movies corresponding to the highest similarity scores. Note that the function ignores the highest similarity score of 1. This is because the movie most similar to a given movie is the movie itself!\n",
    "  \n",
    "1. Function arguments: takes a movie title, cosine similarity matrix, and indices series  \n",
    "2. Extract pair-wise cosine similarity scores for the movie  \n",
    "3. Sort the scores in descending order  \n",
    "4. Output titles corresponding to highest similarity scores  \n",
    "5. Ignore the highest similarity score of 1 (score < 1)  \n",
    "  \n",
    "**Generating tf-idf vectors**\n",
    "  \n",
    "Let's say we already have the preprocessed movie overviews as '`movie_plots`'. We already know how to generate the tf-idf vectors.\n",
    "  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create Vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of tf-idf vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(<dataset>)\n",
    "```\n",
    "  \n",
    "**Generating cosine similarity matrix**\n",
    "  \n",
    "Generating the cosine similarity matrix is also extremely simple. We simply pass in `tfidf_matrix` as both the first and second argument of `cosine_similarity`. This generates a matrix that contains the pairwise similarity score of every movie with every other movie. The value corresponding to the ith row and the $j$-th column is the cosine similarity score of movie $i$ with movie $j$. Notice that the diagonal elements of this matrix is 1. This is because, as stated earlier, the cosine similarity score of movie $k$ with itself is 1.\n",
    "  \n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create Vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of tf-idf vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(<dataset>)\n",
    "\n",
    "# Generate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "```\n",
    "  \n",
    "**The `linear_kernel` function**\n",
    "  \n",
    "The magnitude of a tf-idf vector is always 1. Recall from the previous lesson that the cosine score is computed as the ratio of the dot product and the product of the magnitude of the vectors. Since the magnitude is 1, the cosine score of two tf-idf vectors is equal to their dot product! This fact can help us greatly improve the speed of computation of our cosine similarity matrix as we do not need to compute the magnitudes while working with tf-idf vectors. Therefore, while working with tf-idf vectors, we can use the `linear_kernel` function which computes the pairwise dot product of every vector with every other vector.\n",
    "  \n",
    "- Magnitude of a tf-idf vector is always 1  \n",
    "- Cosine score between two tf-idf vectors is their dot product  \n",
    "- Considering both facts can greatly improve the speed of computation time  \n",
    "- Use `linear_kernel` function in place of `cosine_similarity` to consider both facts  \n",
    "  \n",
    "**Generating cosine similarity matrix**\n",
    "  \n",
    "Let us replace the `cosine_similarity` function with `linear_kernel`. As you can see, the output remains the same but it takes significantly lesser time to compute.\n",
    "  \n",
    "<img src='../_images/building-a-plot-line-based-recommender2.png' alt='img' width='740'>\n",
    "  \n",
    "**The get_recommendations function**\n",
    "  \n",
    "The recommender function and the indices series described earlier will be built in the exercises. You can use this function to generate recommendations using the cosine similarity matrix.\n",
    "  \n",
    "<img src='../_images/building-a-plot-line-based-recommender3.png' alt='img' width='740'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "In the exercises, you will build recommendation systems of your own and see them in action. Let's practice!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing `linear_kernel` and `cosine_similarity`\n",
    "  \n",
    "In this exercise, you have been given `tfidf_matrix` which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using `cosine_similarity` and then, using `linear_kernel`.\n",
    "  \n",
    "We will then compare the computation times for both functions.\n",
    "  \n",
    "1. Compute the cosine similarity matrix for `tfidf_matrix` using `cosine_similarity`.\n",
    "2. Compute the cosine similarity matrix for `tfidf_matrix` using `linear_kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.006113767623901367 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
      "Time taken: 0.003468036651611328 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how both `linear_kernel` and `cosine_similarity` produced the same result. However, `linear_kernel` took a smaller amount of time to execute. When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to `linear_kernel` to improve performance. \n",
    "  \n",
    "*NOTE: In case, you see `linear_kernel` taking more time, it's because the dataset we're dealing with is extremely small and Python's time module is incapable of capture such minute time differences accurately*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The recommender function\n",
    "  \n",
    "In this exercise, we will build a recommender function `get_recommendations()`, as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
    "  \n",
    "You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console.\n",
    "  \n",
    "1. Get index of the movie that matches the title by using the title key of indices.\n",
    "2. Extract the ten most similar movies from `sim_scores` and store it back in `sim_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(820, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>49026</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Following the death of District Attorney Harve...</td>\n",
       "      <td>The Legend Ends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>414</td>\n",
       "      <td>Batman Forever</td>\n",
       "      <td>The Dark Knight of Gotham City confronts a das...</td>\n",
       "      <td>Courage now, truth always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>268</td>\n",
       "      <td>Batman</td>\n",
       "      <td>The Dark Knight of Gotham City begins his war ...</td>\n",
       "      <td>Have you ever danced with the devil in the pal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>364</td>\n",
       "      <td>Batman Returns</td>\n",
       "      <td>Having defeated the Joker, Batman now faces th...</td>\n",
       "      <td>The Bat, the Cat, the Penguin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>415</td>\n",
       "      <td>Batman &amp; Robin</td>\n",
       "      <td>Along with crime-fighting partner Robin and ne...</td>\n",
       "      <td>Strength. Courage. Honor. And loyalty.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>14919</td>\n",
       "      <td>Batman: Mask of the Phantasm</td>\n",
       "      <td>An old flame of Bruce Wayne's strolls into tow...</td>\n",
       "      <td>The Dark Knight fights to save Gotham city fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2661</td>\n",
       "      <td>Batman</td>\n",
       "      <td>The Dynamic Duo faces four super-villains who ...</td>\n",
       "      <td>He's Here Big As Life In A Real Bat-Epic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>Batman Begins</td>\n",
       "      <td>Driven by tragedy, billionaire Bruce Wayne ded...</td>\n",
       "      <td>Evil fears the knight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>40662</td>\n",
       "      <td>Batman: Under the Red Hood</td>\n",
       "      <td>Batman faces his ultimate challenge as the mys...</td>\n",
       "      <td>Dare to Look Beneath the Hood.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>69735</td>\n",
       "      <td>Batman: Year One</td>\n",
       "      <td>Two men come to Gotham City: Bruce Wayne after...</td>\n",
       "      <td>A merciless crime turns a man into an outlaw.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>123025</td>\n",
       "      <td>Batman: The Dark Knight Returns, Part 1</td>\n",
       "      <td>Batman has not been seen for ten years. A new ...</td>\n",
       "      <td>Old heroes never die. They just get darker.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>142061</td>\n",
       "      <td>Batman: The Dark Knight Returns, Part 2</td>\n",
       "      <td>Batman has stopped the reign of terror that Th...</td>\n",
       "      <td>Justice Returns... Vengeance Returns... Redemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>209112</td>\n",
       "      <td>Batman v Superman: Dawn of Justice</td>\n",
       "      <td>Fearing the actions of a god-like Super Hero l...</td>\n",
       "      <td>Justice or revenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>949</td>\n",
       "      <td>Heat</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>A Los Angeles Crime Saga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>11860</td>\n",
       "      <td>Sabrina</td>\n",
       "      <td>An ugly duckling having undergone a remarkable...</td>\n",
       "      <td>You are cordially invited to the most surprisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>45325</td>\n",
       "      <td>Tom and Huck</td>\n",
       "      <td>A mischievous young boy, Tom Sawyer, witnesses...</td>\n",
       "      <td>The Original Bad Boys.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0      id                                    title  \\\n",
       "0            0   49026                    The Dark Knight Rises   \n",
       "1            1     414                           Batman Forever   \n",
       "2            2     268                                   Batman   \n",
       "3            3     364                           Batman Returns   \n",
       "4            4     415                           Batman & Robin   \n",
       "5            5   14919             Batman: Mask of the Phantasm   \n",
       "6            6    2661                                   Batman   \n",
       "7            7     272                            Batman Begins   \n",
       "8            8   40662               Batman: Under the Red Hood   \n",
       "9            9   69735                         Batman: Year One   \n",
       "10          10  123025  Batman: The Dark Knight Returns, Part 1   \n",
       "11          11  142061  Batman: The Dark Knight Returns, Part 2   \n",
       "12          12  209112       Batman v Superman: Dawn of Justice   \n",
       "14          14    8844                                  Jumanji   \n",
       "15          15   15602                         Grumpier Old Men   \n",
       "16          16   31357                        Waiting to Exhale   \n",
       "17          17   11862              Father of the Bride Part II   \n",
       "18          18     949                                     Heat   \n",
       "19          19   11860                                  Sabrina   \n",
       "20          20   45325                             Tom and Huck   \n",
       "\n",
       "                                             overview  \\\n",
       "0   Following the death of District Attorney Harve...   \n",
       "1   The Dark Knight of Gotham City confronts a das...   \n",
       "2   The Dark Knight of Gotham City begins his war ...   \n",
       "3   Having defeated the Joker, Batman now faces th...   \n",
       "4   Along with crime-fighting partner Robin and ne...   \n",
       "5   An old flame of Bruce Wayne's strolls into tow...   \n",
       "6   The Dynamic Duo faces four super-villains who ...   \n",
       "7   Driven by tragedy, billionaire Bruce Wayne ded...   \n",
       "8   Batman faces his ultimate challenge as the mys...   \n",
       "9   Two men come to Gotham City: Bruce Wayne after...   \n",
       "10  Batman has not been seen for ten years. A new ...   \n",
       "11  Batman has stopped the reign of terror that Th...   \n",
       "12  Fearing the actions of a god-like Super Hero l...   \n",
       "14  When siblings Judy and Peter discover an encha...   \n",
       "15  A family wedding reignites the ancient feud be...   \n",
       "16  Cheated on, mistreated and stepped on, the wom...   \n",
       "17  Just when George Banks has recovered from his ...   \n",
       "18  Obsessive master thief, Neil McCauley leads a ...   \n",
       "19  An ugly duckling having undergone a remarkable...   \n",
       "20  A mischievous young boy, Tom Sawyer, witnesses...   \n",
       "\n",
       "                                              tagline  \n",
       "0                                     The Legend Ends  \n",
       "1                        Courage now, truth always...  \n",
       "2   Have you ever danced with the devil in the pal...  \n",
       "3                      The Bat, the Cat, the Penguin.  \n",
       "4              Strength. Courage. Honor. And loyalty.  \n",
       "5   The Dark Knight fights to save Gotham city fro...  \n",
       "6            He's Here Big As Life In A Real Bat-Epic  \n",
       "7                              Evil fears the knight.  \n",
       "8                      Dare to Look Beneath the Hood.  \n",
       "9       A merciless crime turns a man into an outlaw.  \n",
       "10        Old heroes never die. They just get darker.  \n",
       "11  Justice Returns... Vengeance Returns... Redemp...  \n",
       "12                                 Justice or revenge  \n",
       "14          Roll the dice and unleash the excitement!  \n",
       "15  Still Yelling. Still Fighting. Still Ready for...  \n",
       "16  Friends are the people who let you be yourself...  \n",
       "17  Just When His World Is Back To Normal... He's ...  \n",
       "18                           A Los Angeles Crime Saga  \n",
       "19  You are cordially invited to the most surprisi...  \n",
       "20                             The Original Bad Boys.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv('../_datasets/movie_metadata.csv').dropna()\n",
    "print(metadata.shape)\n",
    "metadata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Series has the title as the index, and the feature as the titles original index\n",
      "title\n",
      "The Dark Knight Rises       0\n",
      "Batman Forever              1\n",
      "Batman                      2\n",
      "Batman Returns              3\n",
      "Batman & Robin              4\n",
      "                         ... \n",
      "Braindead                1002\n",
      "Glory                    1003\n",
      "Manhattan                1005\n",
      "Miller's Crossing        1006\n",
      "Dead Poets Society       1007\n",
      "Length: 820, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "print('The Series has the title as the index, and the feature as the titles original index')\n",
    "print(indices)\n",
    "\n",
    "# Function to calculate the recomendation, given a movie title, the text based cosine matrix, and index of the movie to select\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwise similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this recommender function in our toolkit, we are now in a very good place to build the rest of the components of our recommendation engine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot recommendation engine\n",
    "  \n",
    "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a `get_recommendations()` function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you.\n",
    "  \n",
    "You have also been given a `movie_plots` Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
    "  \n",
    "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises.\n",
    "  \n",
    "1. Initialize a `TfidfVectorizer` with English `stop_words`. Name it `tfidf`.\n",
    "2. Construct `tfidf_matrix` by fitting and transforming the movie plot data using `.fit_transform()`.\n",
    "3. Generate the cosine similarity matrix `cosine_sim` using `tfidf_matrix`. Don't use `cosine_similarity()`!\n",
    "4. Use `get_recommendations()` to generate recommendations for `'The Dark Knight Rises'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the target\n",
    "movie_plots = metadata['overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                              Batman Forever\n",
      "2                                      Batman\n",
      "8                  Batman: Under the Red Hood\n",
      "3                              Batman Returns\n",
      "9                            Batman: Year One\n",
      "10    Batman: The Dark Knight Returns, Part 1\n",
      "11    Batman: The Dark Knight Returns, Part 2\n",
      "5                Batman: Mask of the Phantasm\n",
      "7                               Batman Begins\n",
      "4                              Batman & Robin\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Generate recommendations\n",
    "print(get_recommendations(\"The Dark Knight Rises\", cosine_sim, indices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've just built your very first recommendation system. Notice how the recommender correctly identifies `'The Dark Knight Rises'` as a Batman movie and recommends other Batman movies as a result. This sytem is, of course, very primitive and there are a host of ways in which it could be improved. One method would be to look at the cast, crew and genre in addition to the plot to generate recommendations. We will not be covering this in this course but you have all the tools necessary to accomplish this. Do give it a try!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TED talk recommender\n",
    "  \n",
    "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a `get_recommendations()` function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you.\n",
    "  \n",
    "You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
    "  \n",
    "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce.  \n",
    "  \n",
    "1. Initialize a `TfidfVectorizer` with English `stop_words=`. Name it tfidf.\n",
    "2. Construct `tfidf_matrix` by fitting and transforming transcripts.\n",
    "3. Generate the cosine similarity matrix `cosine_sim` using `tfidf_matrix`.\n",
    "4. Use `get_recommendations()` to generate recommendations for `'5 ways to kill your dreams'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'title', 'url', 'transcript'], dtype='object')\n",
      "Index(['title', 'url', 'transcript'], dtype='object')\n",
      "(499, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 top time-saving tech tips</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_10_top_t...</td>\n",
       "      <td>I've noticed something interesting about socie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who am I? Think again</td>\n",
       "      <td>https://www.ted.com/talks/hetain_patel_who_am_...</td>\n",
       "      <td>Hetain Patel: (In Chinese)Yuyu Rau: Hi, I'm He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Awoo\"</td>\n",
       "      <td>https://www.ted.com/talks/sofi_tukker_awoo\\n</td>\n",
       "      <td>(Music)Sophie Hawley-Weld: OK, you don't have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What I learned from 2,000 obituaries</td>\n",
       "      <td>https://www.ted.com/talks/lux_narayan_what_i_l...</td>\n",
       "      <td>Joseph Keller used to jog around the Stanford ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why giving away our wealth has been the most s...</td>\n",
       "      <td>https://www.ted.com/talks/bill_and_melinda_gat...</td>\n",
       "      <td>Chris Anderson: So, this is an interview with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                       10 top time-saving tech tips   \n",
       "1                              Who am I? Think again   \n",
       "2                                             \"Awoo\"   \n",
       "3               What I learned from 2,000 obituaries   \n",
       "4  Why giving away our wealth has been the most s...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.ted.com/talks/david_pogue_10_top_t...   \n",
       "1  https://www.ted.com/talks/hetain_patel_who_am_...   \n",
       "2       https://www.ted.com/talks/sofi_tukker_awoo\\n   \n",
       "3  https://www.ted.com/talks/lux_narayan_what_i_l...   \n",
       "4  https://www.ted.com/talks/bill_and_melinda_gat...   \n",
       "\n",
       "                                          transcript  \n",
       "0  I've noticed something interesting about socie...  \n",
       "1  Hetain Patel: (In Chinese)Yuyu Rau: Hi, I'm He...  \n",
       "2  (Music)Sophie Hawley-Weld: OK, you don't have ...  \n",
       "3  Joseph Keller used to jog around the Stanford ...  \n",
       "4  Chris Anderson: So, this is an interview with ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted = pd.read_csv('../_datasets/ted_clean.csv', index_col=0)\n",
    "print(ted.columns)\n",
    "\n",
    "# Dropping an un-used import-export error column\n",
    "ted = ted.drop('Unnamed: 0.1', axis=1)\n",
    "\n",
    "# Drop the name of the index, it was 'Unnamed: 0' before (what a \"clean\" dataset haha)\n",
    "ted = ted.rename_axis(None)\n",
    "\n",
    "print(ted.columns)\n",
    "print(ted.shape)\n",
    "ted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendation function for ted talks\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    talk_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return ted['title'].iloc[talk_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(ted.index, index=ted['title']).drop_duplicates()\n",
    "\n",
    "# Extracting the text required to make the matrix\n",
    "transcripts = ted['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453             Success is a continuous journey\n",
      "157                        Why we do what we do\n",
      "494                   How to find work you love\n",
      "149          My journey into movies that matter\n",
      "447                        One Laptop per Child\n",
      "230             How to get your ideas to spread\n",
      "497         Plug into your hard-wired happiness\n",
      "495    Why you will fail to have a great career\n",
      "179             Be suspicious of simple stories\n",
      "53                          To upgrade is human\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Generate recommendations\n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully built a TED talk recommender. This recommender works surprisingly well despite being trained only on a small subset of TED talks. In fact, three of the talks recommended by our system is also recommended by the official TED website as talks to watch next after `'5 ways to kill your dreams'`!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond n-grams: word embeddings\n",
    "  \n",
    "We have covered a lot of ground in the last 4 chapters. However, before we bid adieu, we will cover one advanced topic that has a large number of applications in NLP.\n",
    "  \n",
    "**The problem with BoW and tf-idf**\n",
    "  \n",
    "Consider the three sentences, \n",
    "  \n",
    "\"I am happy\"   \n",
    "\"I am joyous\"  \n",
    "\"I am sad\"  \n",
    "  \n",
    "Now if we were to compute the similarities, \"I am happy\" and \"I am joyous\" would have the same score as \"I am happy\" and \"I am sad\", regardless of how we vectorize it. This is because 'happy', 'joyous' and 'sad' are considered to be completely different words. However, we know that happy and joyous are more similar to each other than sad. This is something that the vectorization techniques we've covered so far simply cannot capture.\n",
    "  \n",
    "**Word embeddings**\n",
    "  \n",
    "Word embedding is the process of mapping words into an $n$-dimensional vector space. These vectors are usually produced using deep learning models and huge amounts of data. The techniques used are beyond the scope of this course. However, once generated, these vectors can be used to discern how similar two words are to each other. Consequently, they can also be used to detect synonyms and antonyms. Word embeddings are also capable of capturing complex relationships. For instance, it can be used to detect that the words \"king\" and \"queen\" relate to each other the same way as \"man\" and \"woman\". Or that \"France\" and \"Paris\" are related in the same way as \"Russia\" and \"Moscow\". One last thing to note is that word embeddings are not trained on user data; they are dependent on the pre-trained `spacy` model you're using and are independent of the size of your dataset.\n",
    "  \n",
    "- Mapping words into an $n$-dimensional vector space  \n",
    "- Produced using deep learning and huge amounts of data  \n",
    "- Discern how similar two words are to each other  \n",
    "- Used to detect synonyms and antonyms  \n",
    "- Captures complex relationships (ie. King, Queen -> Man, Woman)  \n",
    "- Dependent on spacy model; independent of dataset you use  \n",
    "  \n",
    "**Word embeddings using `spaCy`**\n",
    "  \n",
    "Generating word embeddings is easy using spaCy's pre-trained models. As usual, we load the `spacy` model and create the `doc` object for our string. Note that it is advisable to load larger `spacy` models while working with word vectors. This is because the `en_core_web_sm` model does not technically ship with word vectors but context specific tensors, which tend to give relatively poorer results. We generate word vectors for each word by looping through the tokens and accessing the `.vector` attribute. The truncated output is as shown.\n",
    "  \n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_we_lg')\n",
    "doc = nlp('<string>')\n",
    "\n",
    "# Generate word vectors for each token\n",
    "for token in doc:\n",
    "    print(token.vector)\n",
    "\n",
    "out[1] : <ndarray_displayed>\n",
    "```\n",
    "  \n",
    "**Word similarities**\n",
    "  \n",
    "We can compute how similar two words are to each other by using the `.similarity()` method of a `spacy` token. Let's say we want to compute how similar \"happy\", \"joyous\" and \"sad\" are to each other. We define a `doc` containing the three words. We then use a nested loop to calculate the similarity scores between each pair of words. As expected, \"happy\" and \"joyous\" are more similar to each other than they are to sad.\n",
    "  \n",
    "<img src='../_images/beyond-n-grams-word-embeddings.png' alt='img' width='740'>\n",
    "  \n",
    "**Document similarities**\n",
    "  \n",
    "`Spacy` also allows us to directly compute the similarity between two documents by using the average of the word vectors of all the words in a particular document. Let's consider the three sentences from before. We create `doc` objects for the sentences. Like `spacy` tokens, docs also have a `.similarity` method. Therefore, we can compute the similarity between two docs as follows. As expected, \"I am happy\" is more similar to \"I am joyous\" than it is to \"I am sad\". Note that the similarity scores are high in both cases because all sentences share 2 out of their three words, \"I\" and \"am\".\n",
    "  \n",
    "<img src='../_images/beyond-n-grams-word-embeddings1.png' alt='img' width='740'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "With this, we come to an end of this lesson. Let's now practice our new found skills in the last set of exercises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Before using word embedding through spaCy, you need to download `en_core_web_lg` model  \n",
    "> Terminal: `python3 -m spacy download en_core_web_lg`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating word vectors\n",
    "  \n",
    "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as `sent` and has been printed to the console for your convenience.\n",
    "  \n",
    "1. Create a `Doc` object `doc` for `sent`.\n",
    "2. In the nested loop, compute the similarity between `token1` and `token2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m704.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from en-core-web-lg==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_lg\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.3184410631656647\n",
      "I apples 0.1975560337305069\n",
      "I and -0.0979200005531311\n",
      "I orange 0.06804359704256058\n",
      "like I 0.3184410631656647\n",
      "like like 1.0\n",
      "like apples 0.29574331641197205\n",
      "like and 0.24359610676765442\n",
      "like orange 0.3216366171836853\n",
      "apples I 0.1975560337305069\n",
      "apples like 0.29574331641197205\n",
      "apples apples 1.0\n",
      "apples and 0.24472734332084656\n",
      "apples orange 0.5736395120620728\n",
      "and I -0.0979200005531311\n",
      "and like 0.24359610676765442\n",
      "and apples 0.24472734332084656\n",
      "and and 1.0\n",
      "and orange 0.2520448565483093\n",
      "orange I 0.06804359704256058\n",
      "orange like 0.3216366171836853\n",
      "orange apples 0.5736395120620728\n",
      "orange and 0.2520448565483093\n",
      "orange orange 1.0\n"
     ]
    }
   ],
   "source": [
    "sent = 'I like apples and orange'\n",
    "\n",
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "    for token2 in doc: \n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apples orange 0.5736395120620728`  \n",
    "`orange apples 0.5736395120620728`  \n",
    "  \n",
    "Notice how the words `'apples'` and `'oranges'` have the highest pairwaise similarity score. This is expected as they are both fruits and are more related to each other than any other pair of words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing similarity of Pink Floyd songs\n",
    "  \n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as `hopes`, `hey` and `mother` respectively.\n",
    "  \n",
    "Your task is to compute the pairwise similarity between `mother` and `hopes`, and `mother` and `hey`.\n",
    "  \n",
    "1. Create `Doc` objects for `mother`, `hopes` and `hey`.\n",
    "2. Compute the similarity between `mother` and `hopes`.\n",
    "3. Compute the similarity between `mother` and `hey`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mother do you think they'll drop the bomb?\n",
      "Mother do you think they'll like this song?\n",
      "Mother do you think they'll try to break my balls?\n",
      "Ooh, ah\n",
      "Mother should I build the wall?\n",
      "Mother should I run for President?\n",
      "Mother should I trust the government\n",
      "\n",
      "Beyond the horizon of the place we lived when we were young\n",
      "In a world of magnets and miracles\n",
      "Our thoughts strayed constantly and without boundary\n",
      "The ringing of the division bell had begun\n",
      "Along the Long Road and on down the Causeway\n",
      "Do they still\n",
      "\n",
      "Hey you, out there in the cold\n",
      "Getting lonely, getting old\n",
      "Can you feel me?\n",
      "Hey you, standing in the aisles\n",
      "With itchy feet and fading smiles\n",
      "Can you feel me?\n",
      "Hey you, don't help them to bury the light\n",
      "Don't give in without a fight\n",
      "Hey you out there\n"
     ]
    }
   ],
   "source": [
    "with open('../_datasets/mother.txt', 'r') as f:\n",
    "    mother = f.read()\n",
    "    \n",
    "with open('../_datasets/hopes.txt', 'r') as f:\n",
    "    hopes = f.read()\n",
    "    \n",
    "with open('../_datasets/hey.txt', 'r') as f:\n",
    "    hey = f.read()\n",
    "\n",
    "print(mother[:250])     # Displaying the first 250 chars in the string\n",
    "print(hopes[:250])      # Displaying the first 250 chars in the string\n",
    "print(hey[:250])        # Displaying the first 250 chars in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5779929666352768\n",
      "0.9465446706762218\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that 'Mother' and 'Hey You' have a similarity score of 0.947 whereas 'Mother' and 'High Hopes' has a score of only 0.578. This is probably because 'Mother' and 'Hey You' were both songs from the same album 'The Wall' and were penned by Roger Waters. On the other hand, 'High Hopes' was a part of the album 'Division Bell' with lyrics by David Gilmour and his wife, Penny Samson. Treat yourself by listening to these songs. They're some of the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
