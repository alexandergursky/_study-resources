{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram models\n",
    "  \n",
    "Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "  \n",
    "[SpaCy Documentation](https://spacy.io)  \n",
    "[Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  \n",
    "[Scikit-learn CountVectorizer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import re                           # Regular Expressions:      Text manipulation\n",
    "import spacy                        # Spatium Cython:           Natural Language Processing\n",
    "from pprint import pprint           # Pretty Print:             Advanced printing operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a bag of words model\n",
    "  \n",
    "In this chapter, we will cover vectorization which is, as you may recall, the process of converting text into vectors.\n",
    "  \n",
    "**Recap of data format for ML algorithms**\n",
    "  \n",
    "Recall that for any ML algorithm to run properly, data fed into it must be in tabular form and all the training features must be numerical. This is clearly not the case for textual data. In this lesson, we will learn a technique called bag of words that converts text documents into vectors.\n",
    "  \n",
    "**Bag of words model**\n",
    "  \n",
    "The bag of words model is a procedure of extracting word tokens from a text document (henceforth, we will refer to this as just document), computing the frequency of these word tokens and constructing a word vector based on these frequencies and the vocabulary of the entire corpus of documents. This is best explained with the help of an example.\n",
    "  \n",
    "**Bag of words model**  \n",
    "  \n",
    "- Extract word tokens  \n",
    "- Compute frequency of word tokens  \n",
    "- Construct a word vector out of these frequencies and vocabulary of corpus  \n",
    "  \n",
    "**Bag of words model example**\n",
    "  \n",
    "Consider a corpus of three documents:  \n",
    "- \"The lion is the king of the jungle.\"  \n",
    "- \"Lions have an average lifespan of 15 years.\"  \n",
    "- \"The lion is an endangered species.\"  \n",
    "  \n",
    "**Bag of words model example**\n",
    "  \n",
    "We now extract the unique word tokens that occur in this corpus of documents. This will be the vocabulary of our model. In this example, the following 15 word tokens will constitute our vocabulary. Since there are 15 words in our vocabulary, our word vectors will have 15 dimensions and each dimension's value will correspond to the frequency of the word token corresponding to that dimension. For instance, the second dimension will correspond to the number of times the second word in the vocabulary, an, occurs in the document. Let's now convert our documents into word vectors using this bag of words model. The lion is the king of the jungle is converted to the following vector. Similarly, the other two sentences have the following word vector representations.\n",
    "  \n",
    "<img src='../_images/building-a-bag-of-words-model.png' alt='img' width='740'>\n",
    "  \n",
    "**Text preprocessing**\n",
    "  \n",
    "As we were constructing this model, you may have noticed how text preprocessing would have been extremely useful in creating arguably better models. We would usually want \"Lions\" and \"lion\" to mean the same thing and therefore, counted as the same thing. The same applies to 'the' with different cases. We would also want to remove punctuations and stopwords as they are extremely common and don't really contribute much to the character of the document. Performing text preprocessing usually leads to smaller vocabularies, which is a good thing. While working with vectorization, it is routine to form word vectors running into thousands of dimensions and keeping this to a minimum helps improve performance.\n",
    "  \n",
    "**Text preprocessing**  \n",
    "  \n",
    "- Example: Lions, lion -> lion  \n",
    "- Example: The, the -> the  \n",
    "- No punctuations  \n",
    "- No stopwords  \n",
    "- Leads to smaller vocabularies  \n",
    "- Reducing number of dimensions helps to improve model performance  \n",
    "  \n",
    "**Bag of words model using sklearn**\n",
    "  \n",
    "To construct the bag of words model in Python, we will use the `scikit-learn` library. We will use the corpus from before, consisting of the three sentences on lions. Let's ignore text preprocessing for now.\n",
    "  \n",
    "<img src='../_images/building-a-bag-of-words-model1.png' alt='img' width='740'>\n",
    "  \n",
    "**Bag of words model using sklearn**\n",
    "  \n",
    "We import the `CountVectorizer` class from `sklearn.feature_extraction.text`. This is the class that will help us build our bag of words model. Next, we instantiate a `CountVectorizer` object `vectorizer`. We finally create our matrix of word vectors by passing `corpus` to the `.fit_transform()` method of `vectorizer`. This is stored in `bow_matrix`. \n",
    "  \n",
    "This `bow_matrix` is a sparse matrix and we can print out its 2D array form using `bow_matrix.toarray()`. This gives us the following output. Notice how this is different from the word vectors we generated. This is because `CountVectorizer` automatically lowercases words and ignores single character tokens such as 'a'. Also, it doesn't necessarily index the vocabulary in alphabetical order. We will learn how to map the vocabulary to the indices in the exercises. We can now use this `bow_matrix` as our training features in ML models.\n",
    "  \n",
    "<img src='../_images/building-a-bag-of-words-model2.png' alt='img' width='740'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "We've covered a lot of theory in this lesson. Let us practice this in the exercises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors with a given vocabulary\n",
    "  \n",
    "You have been given a corpus of documents and you have computed the vocabulary of the corpus to be the following:  \n",
    "  \n",
    "**Vocabulary**: a, an, and, but, can, come, evening, forever, go, i, men, may, on, the, women\n",
    "  \n",
    "Which of the following corresponds to the bag of words vector for the document \"men may come and men may go but i go on forever\"?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [x] (0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0)  \n",
    "- [ ] (0, 1, 0, 1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 2, 0)  \n",
    "- [ ] (2, 1, 0, 0, 2, 1, 0, 0, 0, 1)  \n",
    "- [ ] (0, 0, 1, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1)  \n",
    "  \n",
    "That is, indeed, the correct answer. Each value in the vector corresponds to the frequency of the corresponding word in the vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW model for movie taglines\n",
    "  \n",
    "In this exercise, you have been provided with a `corpus` of more than 7000 movie tag lines. Your job is to generate the bag of words representation `bow_matrix` for these taglines. For this exercise, we will ignore the text preprocessing step and generate `bow_matrix` directly.\n",
    "  \n",
    "We will also investigate the shape of the resultant `bow_matrix`. The first five taglines in `corpus` have been printed to the console for you to examine.\n",
    "  \n",
    "```python\n",
    "1            Roll the dice and unleash the excitement!\n",
    "2    Still Yelling. Still Fighting. Still Ready for...\n",
    "3    Friends are the people who let you be yourself...\n",
    "4    Just When His World Is Back To Normal... He's ...\n",
    "5                             A Los Angeles Crime Saga\n",
    "Name: tagline, dtype: object\n",
    "```\n",
    "  \n",
    "1. Import the `CountVectorizer` class from `sklearn`.\n",
    "2. Instantiate a `CountVectorizer` object. Name it `vectorizer`.\n",
    "3. Using `.fit_transform()`, generate `bow_matrix` for `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>still yelling. still fighting. still ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>friends are the people who let you be yourself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>just when his world is back to normal... he's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>949</td>\n",
       "      <td>Heat</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>a los angeles crime saga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title  \\\n",
       "1   8844                      Jumanji   \n",
       "2  15602             Grumpier Old Men   \n",
       "3  31357            Waiting to Exhale   \n",
       "4  11862  Father of the Bride Part II   \n",
       "5    949                         Heat   \n",
       "\n",
       "                                            overview  \\\n",
       "1  When siblings Judy and Peter discover an encha...   \n",
       "2  A family wedding reignites the ancient feud be...   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   \n",
       "4  Just when George Banks has recovered from his ...   \n",
       "5  Obsessive master thief, Neil McCauley leads a ...   \n",
       "\n",
       "                                             tagline  \n",
       "1          roll the dice and unleash the excitement!  \n",
       "2  still yelling. still fighting. still ready for...  \n",
       "3  friends are the people who let you be yourself...  \n",
       "4  just when his world is back to normal... he's ...  \n",
       "5                           a los angeles crime saga  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('../_datasets/movie_overviews.csv').dropna()   # Load df\n",
    "movies['tagline'] = movies['tagline'].str.lower()                   # Standardization\n",
    "print(movies.shape)                                                 # Dataframe shape\n",
    "movies.head()                                                       # Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033,)\n",
      "1               roll the dice and unleash the excitement!\n",
      "2       still yelling. still fighting. still ready for...\n",
      "3       friends are the people who let you be yourself...\n",
      "4       just when his world is back to normal... he's ...\n",
      "5                                a los angeles crime saga\n",
      "                              ...                        \n",
      "9091                        kingsglaive: final fantasy xv\n",
      "9093    what happens in vegas, stays in vegas. unless ...\n",
      "9095    decorated officer. devoted family man. defendi...\n",
      "9097                      a god incarnate. a city doomed.\n",
      "9098              the band you know. the story you don't.\n",
      "Name: tagline, Length: 7033, dtype: object\n"
     ]
    }
   ],
   "source": [
    "corpus = movies['tagline']  # Corpus instantiation\n",
    "print(corpus.shape)         # Shape of the pd.Series\n",
    "print(corpus)               # Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 6614)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix, datatype is csr_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to generate a Bag-of-Words(BoW) representation for a given corpus of documents. Notice that the word vectors created have more than 6600 dimensions. However, most of these dimensions have a value of zero since most words do not occur in a particular tagline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing dimensionality and preprocessing\n",
    "  \n",
    "In this exercise, you have been provided with a `lem_corpus` which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed.\n",
    "  \n",
    "Your job is to generate the bag of words representation `bow_lem_matrix` for these lemmatized taglines and compare its shape with that of `bow_matrix` obtained in the previous exercise. The first five lemmatized taglines in `lem_corpus` have been printed to the console for you to examine.\n",
    "  \n",
    "1. Import the `CountVectorizer` class from `sklearn`.\n",
    "2. Instantiate a `CountVectorizer` object. Name it `vectorizer`.\n",
    "3. Using `.fit_transform()`, generate `bow_lem_matrix` for `lem_corpus`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Breakdown of the Anonymous function in successor cell\n",
    "  \n",
    "1. Applies a lambda function to each row of the corpus. \n",
    "2. For each token in the processed row, the lambda function checks if its lemma (base form) is not in a set of stopwords.\n",
    "3. If the lemma is not in the set of stopwords and is composed entirely of alphabetic characters it is added to a list comprehension.\n",
    "4. Finally, the lemmas are joined into a single string using a space as the separator.\n",
    "  \n",
    "```python\n",
    "# Creating the lemmatized corpus\n",
    "lem_corpus = corpus.apply(lambda row: ' '.join(\n",
    "    [t.lemma_ for t in nlp(row) if t.lemma_ not in stopwords and t.lemma_.isalpha()])\n",
    "    )\n",
    "```\n",
    "Datatype: pd.Series  \n",
    "Shape: (7033,)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                            roll dice unleash excitement\n",
       "2                                   yell fight ready love\n",
       "3                            friend people let let forget\n",
       "4                              world normal surprise life\n",
       "5                                  los angeles crime saga\n",
       "                              ...                        \n",
       "9091                         kingsglaive final fantasy xv\n",
       "9093                       happen vegas stay vegas happen\n",
       "9095    decorate officer devote family man defend hono...\n",
       "9097                              god incarnate city doom\n",
       "9098                                      band know story\n",
       "Name: tagline, Length: 7033, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading pre-trained model, Object Oriented Programming (OOP) instance\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Creating the stopwords, datatype is a Set\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating the lemmatized corpus\n",
    "lem_corpus = corpus.apply(lambda row: ' '.join(\n",
    "    [t.lemma_ for t in nlp(row) if t.lemma_ not in stopwords and t.lemma_.isalpha()])\n",
    "    )\n",
    "\n",
    "lem_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 4941)\n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix, datatype is a csr_matrix\n",
    "print(bow_lem_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the number of features have reduced significantly from 6614 to 4941 (a 25.2948292% reduction) for pre-processed movie taglines. The reduced number of dimensions on account of text preprocessing usually leads to better performance when conducting machine learning and it is a good idea to consider it. However, as mentioned in a previous lesson, the final decision always depends on the nature of the application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping feature indices with feature names\n",
    "  \n",
    "In the lesson video, we had seen that `CountVectorizer` doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary.\n",
    "  \n",
    "We will use the same three sentences on lions from the video. The sentences are available in a list named `corpus` and has already been printed to the console.\n",
    "  \n",
    "`['The lion is the king of the jungle', 'Lions have lifespans of a decade', 'The lion is an endangered species']`\n",
    "  \n",
    "1. Instantiate a `CountVectorizer` object. Name it `vectorizer`.\n",
    "2. Using `.fit_transform()`, generate `bow_matrix` for `corpus`.\n",
    "3. Using the `.get_feature_names_out()` method, map the column names to the corresponding word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>decade</th>\n",
       "      <th>endangered</th>\n",
       "      <th>have</th>\n",
       "      <th>is</th>\n",
       "      <th>jungle</th>\n",
       "      <th>king</th>\n",
       "      <th>lifespans</th>\n",
       "      <th>lion</th>\n",
       "      <th>lions</th>\n",
       "      <th>of</th>\n",
       "      <th>species</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  decade  endangered  have  is  jungle  king  lifespans  lion  lions  of  \\\n",
       "0   0       0           0     0   1       1     1          0     1      0   1   \n",
       "1   0       1           0     1   0       0     0          1     0      1   1   \n",
       "2   1       0           1     0   1       0     0          0     1      0   0   \n",
       "\n",
       "   species  the  \n",
       "0        0    3  \n",
       "1        0    0  \n",
       "2        1    1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the list of Lion sentences\n",
    "sentences = ['The lion is the king of the jungle',\n",
    "             'Lions have lifespans of a decade', \n",
    "             'The lion is an endangered species']\n",
    "\n",
    "# Create CountVectorizer object, Scikit-learn OOP object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors, datatype is a csr_matrix\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary\n",
    "bow_df.columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df.shape)\n",
    "bow_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the column names refer to the token whose frequency is being recorded. Therefore, since the first column name is `an`, the first feature represents the number of times the word 'an' occurs in a particular sentence. `get_feature_names_out()` essentially gives us a list which represents the mapping of the feature indices to the feature name in the vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a BoW Naive Bayes classifier\n",
    "  \n",
    "In this lesson, we will walk through a machine learning problem that utilizes feature engineering techniques we've learned, to arrive at a desired result.\n",
    "  \n",
    "**Spam filtering**\n",
    "  \n",
    "Let's take a look at the spam filtering problem. We're given a dataset of messages that have been labelled as spam or ham. Here, you can see a typical spam and ham message. Our task is to train an ML model that can predict the label given a particular text.\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <th>Message</th>\n",
    "      <th>Label</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>WINNER!!! As a valued Wa1mart customer you have been selected to WIN $900 dollars and a FREE trip to the BAHAMAS! Enter your social security number to claim your prize!</td>\n",
    "      <td>Spam</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Hey Alexander, I wanted to reach out to see if you have finished your notebook on Natural Language Processing?</td>\n",
    "      <td>Ham</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "\n",
    "**Steps**\n",
    "  \n",
    "There are 3 steps involved. The first is to preprocess the text. Next, we proceed to build the bag-of-words model. Finally, we conduct predictive modeling using the generated BoW vectors. Note that although we use the term 'modeling' in the context of both BoW and machine learning, they mean two different things.\n",
    "  \n",
    "1. Text processing\n",
    "2. Building a Bag-of-Words representation/model\n",
    "3. Machine Learning\n",
    "  \n",
    "**Text preprocessing using `CountVectorizer`**\n",
    "  \n",
    "We've already learned how to conduct text preprocessing using `spaCy`. However, it is also possible to do this using `CountVectorizer`. `CountVectorizer` takes in a number of arguments to perform preprocessing. \n",
    "  \n",
    "The `lowercase=` argument, when set to `True`, converts words to lowercase. The `strip_accents=` argument can convert accented characters according to unicode or ASCII mapping. Passing in a `stopwords=` argument will lead to `CountVectorizer` ignoring stopwords. You can pass in a custom list or the string 'english' to use scikit-learn's list of English stopwords. You can specify tokenization using a regular expression as the value of the `token_pattern=` argument. Tokenization can also be specified using a `tokenizer=` argument. \n",
    "  \n",
    "Here, you can pass a function that takes a string as an argument and returns a list of tokens. This way, `CountVectorizer` allows usage of spaCy's tokenization techniques. `CountVectorizer` cannot perform certain steps such as lemmatization automatically. This is where `spaCy` is useful. Although it performs tokenization and preprocessing, CountVectorizer's main job is to convert a corpus into a matrix of numerical vectors.\n",
    "  \n",
    "**Building the BoW model**\n",
    "  \n",
    "As usual, we import `CountVectorizer` from `scikit-learn`. We then instantiate a `CountVectorizer` object called vectorizer. We perform accent stripping using ASCII mapping and remove English stopwords. We also set the `lowercase=` argument to False. This is because spam messages usually tend to abuse all-capital words and we might want to preserve this information for the ML step. The dataset has been already been loaded into the dataframe `df`. We split this dataset into training and test sets using scikit-learn's `train_test_split()` function.\n",
    "  \n",
    "<img src='../_images/building-the-bow-model.png' alt='img' width='740'>\n",
    "  \n",
    "**Building the BoW model**\n",
    "  \n",
    "We now fit the vectorizer on the training set and transform it into its bag-of-words representation. We can perform both these steps together using the `.fit_transform()` method. Next, we transform the test set into its BoW representation. Note, that we do not fit the vectorizer with the test data. It is possible that there are some words in the test data that is not in the vocabulary of the vectorizer. In such cases, `CountVectorizer` simply ignores these words.\n",
    "  \n",
    "<img src='../_images/building-the-bow-model1.png' alt='img' width='740'>\n",
    "  \n",
    "**Training the Naive Bayes classifier**\n",
    "  \n",
    "We're now in a good position to train an ML model. We will use the Multinomial Naive Bayes classifier for this task. We import the `MultinomialNB` class from `scikit-learn` and create an object named `clf`. We then fit the training BoW vectors and their corresponding labels to `clf`. We can now test the performance of our model. We compute the accuracy of the model on the test set using `clf.score`. In this case, our model registered an accuracy of 76% on the test set.\n",
    "  \n",
    "<img src='../_images/building-the-bow-model2.png' alt='img' width='740'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "We've covered a lot of ground in building a spam filter in this lesson. In the exercises, we will perform similar steps to perform sentiment analysis on movie reviews. Let's practice!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW vectors for movie reviews\n",
    "  \n",
    "In this exercise, you have been given two `pandas` Series, `X_train` and `X_test`, which consist of movie reviews. They represent the training and the test review data respectively. Your task is to preprocess the reviews and generate BoW vectors for these two sets using `CountVectorizer`.\n",
    "  \n",
    "Once we have generated the BoW vector matrices `X_train_bow` and `X_test_bow`, we will be in a very good position to apply a machine learning model to it and conduct sentiment analysis.\n",
    "  \n",
    "1. Import `CountVectorizer` from the `sklearn` library.\n",
    "2. Instantiate a `CountVectorizer` object named `vectorizer`. Ensure that all words are converted to lowercase and english stopwords are removed.\n",
    "3. Using `X_train`, fit vectorizer and then use it to transform `X_train` to generate the set of BoW vectors `X_train_bow`.\n",
    "4. Transform `X_test` using `vectorizer` to generate the set of BoW vectors `X_test_bow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this anime series starts out great interesting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some may go for a film like this but i most as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i ve seen this piece of perfection during the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this movie is likely the worst movie i ve ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it ll soon be 10 yrs since this movie was rele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  this anime series starts out great interesting...          0\n",
       "1  some may go for a film like this but i most as...          0\n",
       "2  i ve seen this piece of perfection during the ...          1\n",
       "3  this movie is likely the worst movie i ve ever...          0\n",
       "4  it ll soon be 10 yrs since this movie was rele...          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews = pd.read_csv('../_datasets/movie_reviews_clean.csv')\n",
    "print(movie_reviews.shape)\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X/y split\n",
    "X = movie_reviews['review']\n",
    "y = movie_reviews['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 14934)\n",
      "(250, 14934)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Create a CounterVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a good idea of preprocessing text and transforming them into their bag-of-words representation using `CountVectorizer`. In this exercise, you have set `lowercase=True`. However, note that this is the default value of `lowercase=` and passing it explicitly is not necessary. Also, note that both `X_train_bow` and `X_test_bow` have 14,934 features. There were words present in `X_test` that were not in `X_train`. `CountVectorizer` chose to ignore them in order to ensure that the dimensions of both sets remain the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the sentiment of a movie review\n",
    "  \n",
    "In the previous exercise, you generated the bag-of-words representations for the training and test movie review data. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews.\n",
    "  \n",
    "In case you don't recall, the training and test BoW vectors are available as `X_train_bow` and `X_test_bow` respectively. The corresponding labels are available as `y_train` and `y_test` respectively. Also, for you reference, the original movie review dataset is available as `df`.\n",
    "  \n",
    "1. Instantiate an object of `MultinomialNB`. Name it `clf`.\n",
    "2. Fit `clf` using `X_train_bow` and `y_train`.\n",
    "3. Measure the accuracy of `clf` using `X_test_bow` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.792\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is {:.3f}\".format(accuracy))\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = 'The movie was terrible. The music was underwhelming and the acting mediocre.'\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is {}\".format(prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully performed basic sentiment analysis. Note that the accuracy of the classifier is 79.2%. Considering the fact that it was trained on only 750 reviews, this is reasonably good performance. The classifier also correctly predicts the sentiment of a mini negative review which we passed into it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building n-gram models\n",
    "  \n",
    "We already know how to build bag-of-words representations of our documents and use it to conduct various machine learning tasks.\n",
    "  \n",
    "**BoW shortcomings**\n",
    "  \n",
    "Consider the following mini reviews. One is a positive review which states that the movie was good and not boring. The other is negative; commenting that the movie was not good and boring. If we were to construct BoW vectors for these reviews, we would get identical vectors since both reviews contain exactly the same words. And here in lies the biggest shortcoming of the bag-of-words model: context of the words is lost. In this example, the position of the word 'not' changes the entire sentiment of the review. Therefore, in this lesson, we will study techniques that will allow us to model this.\n",
    "  \n",
    "  <table>\n",
    "    <tr>\n",
    "      <th>Review</th>\n",
    "      <th>Label</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>The movie was good and not boring.</td>\n",
    "      <td>Positive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>The moview was not good and boring.</td>\n",
    "      <td>Negative</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  \n",
    "- Exactly the same BoW representation.  \n",
    "- Context of the words are lost.  \n",
    "- Sentiment dependent on the position of \"not\".  \n",
    "  \n",
    "**n-grams**\n",
    "  \n",
    "An n-gram is a contiguous sequence of n elements (or words) in a given document. The bag-of-words model that we've explored so far is nothing but an n-gram model where $n$ is equal to one. Let's now explore n-grams when $n$ is greater than one. Consider the sentence \"For you a thousand times over\". If we set $n$ to 2, then the n-grams (called bi-grams in this case) would be \"for you\", \"you a\", \"a thousand\", \"thousand times\" and \"times over\".\n",
    "  \n",
    "<img src='../_images/n-grams-introduction-nlp.png' alt='img' width='740'>\n",
    "  \n",
    "**n-grams**\n",
    "  \n",
    "Similarly, for $n$ equal to 3, the n-grams (or tri-grams) will be for you a, you a thousand, a thousand times, thousand times over. Therefore, we can use these n-grams to capture more context and account for cases like 'not'.\n",
    "  \n",
    "<img src='../_images/n-grams-introduction-nlp1.png' alt='img' width='740'>\n",
    "  \n",
    "**Applications**\n",
    "  \n",
    "Apart from capturing more context, n-grams have a host of other useful applications. They are used in sentence completion, spelling correction and machine translation correction. In all these cases, the model computes the probability of n words occurring contiguously to perform the above processes.\n",
    "  \n",
    "- Sentence completion\n",
    "- Spelling correction\n",
    "- Machine translation correction\n",
    "  \n",
    "**Building n-gram models using scikit-learn**\n",
    "  \n",
    "Building these n-gram models using scikit-learn is extremely simple, now that we know how to use `CountVectorizer`. `CountVectorizer` takes in an argument `ngram_range=` which is a tuple containing the lower and upper bound for the range of n-values. For instance, passing 2,2 as the `ngram_range=` will generate only bi-grams. On the other hand, passing in 1,3 will generate n-grams where $n$ is equal to 1, 2 and 3.\n",
    "  \n",
    "<img src='../_images/n-grams-introduction-nlp2.png' alt='img' width='740'>\n",
    "  \n",
    "**Shortcomings**\n",
    "  \n",
    "While on the surface, it may seem lucrative to generate n-grams of high orders to capture more and more context, it comes with caveats. We've already seen that the BoW vectors run into thousands of dimensions. Adding higher order n-grams increases the number of dimensions even more and while performing machine learning, leads to a problem known as the curse of dimensionality. Additionally, n-grams for $n$ greater than 3 become exceedingly rare to find in multiple documents. So that feature becomes effectively useless. For these reasons, it is often a good idea to restrict yourself to n-grams where $n$ is small.\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Great! Let's now build these advanced n-gram models and discover more insights in the exercises."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram models for movie tag lines\n",
    "  \n",
    "In this exercise, we have been provided with a `corpus` of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model.\n",
    "  \n",
    "We will then compare the number of features generated for each model.\n",
    "  \n",
    "1. Generate an n-gram model with n-grams up to n=1. Name it `ng1`\n",
    "2. Generate an n-gram model with n-grams up to n=2. Name it `ng2`\n",
    "3. Generate an n-Gram Model with n-grams up to n=3. Name it `ng3`\n",
    "4. Print the number of features for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1               roll the dice and unleash the excitement!\n",
       "2       still yelling. still fighting. still ready for...\n",
       "3       friends are the people who let you be yourself...\n",
       "4       just when his world is back to normal... he's ...\n",
       "5                                a los angeles crime saga\n",
       "                              ...                        \n",
       "9091                        kingsglaive: final fantasy xv\n",
       "9093    what happens in vegas, stays in vegas. unless ...\n",
       "9095    decorated officer. devoted family man. defendi...\n",
       "9097                      a god incarnate. a city doomed.\n",
       "9098              the band you know. the story you don't.\n",
       "Name: tagline, Length: 7033, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1, 1))  # Bag-of-Words\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1, 2))  # Bi-grams\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))  # Tri-grams\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have {}, {} and {} features respectively\".format(ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to generate n-gram models containing higher order n-grams. Notice that `ng2` has over 37,000 features whereas `ng3` has over 76,000 features. This is much greater than the 6,000 dimensions obtained for `ng1`. As the n-gram range increases, so does the number of features, leading to increased computational costs and a problem known as the curse of dimensionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order n-grams for sentiment analysis\n",
    "  \n",
    "Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task.\n",
    "  \n",
    "The n-gram training reviews are available as `X_train_ng`. The corresponding test reviews are available as `X_test_ng`. Finally, use `y_train` and `y_test` to access the training and test sentiment classes respectively.\n",
    "  \n",
    "1. Define an instance of `MultinomialNB`. Name it `clf_ng`\n",
    "2. Fit the classifier on `X_train_ng` and `y_train`.\n",
    "3. Measure accuracy on `X_test_ng` and `y_test` the using `.score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_ng = ng_vectorizer.fit_transform(X_train)\n",
    "X_test_ng = ng_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.796\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Define an instance of MultinomialNB\n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is {:.3f}\".format(accuracy))\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = 'The movie was not good. The plot had several holes and the acting lacked panache'\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is {}\".format(prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now adept at performing sentiment analysis using text. Notice how this classifier performs *slightly* better than the BoW version (from 79.2% to 79.6%). Also, it succeeds at correctly identifying the sentiment of the mini-review as negative. In the next chapter, we will learn more complex methods of vectorizing textual data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing performance of n-gram models\n",
    "  \n",
    "You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3.\n",
    "  \n",
    "We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation.\n",
    "  \n",
    "1. Initialize a `CountVectorizer` object such that it generates uni-grams.\n",
    "2. Initialize a `CountVectorizer` object such that it generates n-grams up to n=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 0.757 seconds to complete. The accuracy on the test-set is 0.75. \n",
      "The n-gram representation had 12347 features.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(movie_reviews['review'],\n",
    "                                                    movie_reviews['sentiment'], \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=movie_reviews['sentiment'])\n",
    "\n",
    "# Generating 1-grams, Uni-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print the accuracy, time and number of dimensions\n",
    "print(\"The program took {:.3f} seconds to complete. The accuracy on the test-set is {:.2f}. \".format(time.time() - start_time, clf.score(test_X, test_y)))\n",
    "print(\"The n-gram representation had {} features.\".format(train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 3.244 seconds to complete. The accuracy on the test-set is 0.77. \n",
      "The n-gram representation had 178240 features.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Starting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(movie_reviews['review'],\n",
    "                                                    movie_reviews['sentiment'], \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=movie_reviews['sentiment'])\n",
    "\n",
    "# Generating 3-grams, Tri-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print the accuracy, time and number of dimensions\n",
    "print(\"The program took {:.3f} seconds to complete. The accuracy on the test-set is {:.2f}. \".format(time.time() - start_time, clf.score(test_X, test_y)))\n",
    "print(\"The n-gram representation had {} features.\".format(train_X.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program took around 0.757 seconds in the case of the unigram model and more than 4.28 times longer for the higher order n-gram model. The unigram model had over 12,000 features whereas the n-gram model for upto n=3 had over 178,000! Despite taking higher computation time and generating more features, the classifier only performs marginally better in the latter case, producing an accuracy of 77% in comparison to the 75% for the unigram model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
