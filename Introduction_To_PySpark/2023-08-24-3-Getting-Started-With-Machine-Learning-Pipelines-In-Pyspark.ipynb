{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with machine learning pipelines in Pyspark\n",
    "  \n",
    "PySpark has built-in, cutting-edge machine learning routines, along with utilities to create full machine learning pipelines. You'll learn about them in this chapter.\n",
    "  \n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   \n",
    "      /_/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pyspark.SparkContext()</td>\n",
    "    <td>Creates a SparkContext, the entry point to using Spark functionality.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>pyspark.SparkContext().version</td>\n",
    "    <td>Returns the version of the SparkContext.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>pyspark.SparkContext().stop()</td>\n",
    "    <td>Stops the SparkContext, terminating the connection to the Spark cluster.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>pyspark.sql.SparkSession</td>\n",
    "    <td>Creates a SparkSession, the entry point to using Spark SQL functionality.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.getOrCreate()</td>\n",
    "    <td>Returns an existing SparkSession or creates a new one.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.appName</td>\n",
    "    <td>Sets the name of the application for the SparkSession.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.getOrCreate</td>\n",
    "    <td>Returns an existing SparkSession or creates a new one.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>pyspark.sql.SparkSession.read</td>\n",
    "    <td>Creates a DataFrameReader for reading data into a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format</td>\n",
    "    <td>Sets the input format for reading data into a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.option('inferSchema', 'True')</td>\n",
    "    <td>Sets the option to infer the schema of the DataFrame from the data.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.option('header', 'True')</td>\n",
    "    <td>Sets the option to treat the first row as the header in the DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.load()</td>\n",
    "    <td>Loads data into a DataFrame using the specified format.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>pyspark.sql.SparkSession.createOrReplaceTempView</td>\n",
    "    <td>Creates or replaces a temporary view of a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>pyspark.sql.SparkSession.catalog.listTables()</td>\n",
    "    <td>Returns a list of tables in the catalog.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>pyspark.sql.SparkSession.sql()</td>\n",
    "    <td>Executes a SQL query and returns the result as a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>pyspark.sql.SparkSession.sql().toPandas()</td>\n",
    "    <td>Converts a DataFrame to a Pandas DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>pyspark.sql.SparkSession.sql().toPandas().head()</td>\n",
    "    <td>Returns the first n rows of a Pandas DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>pyspark.sql.SparkSession.createDataFrame()</td>\n",
    "    <td>Creates a DataFrame from a Pandas DataFrame or an RDD.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>pyspark.sql.SparkSession.read.csv()</td>\n",
    "    <td>Reads a CSV file and returns a DataFrame.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: scikit-learn  \n",
    "Version: 1.3.0  \n",
    "Summary: A set of python modules for machine learning and data mining  \n",
    "  \n",
    "Name: hyperopt  \n",
    "Version: 0.2.7  \n",
    "Summary: Distributed Asynchronous Hyperparameter Optimization  \n",
    "  \n",
    "Name: TPOT  \n",
    "Version: 0.12.1  \n",
    "Summary: Tree-based Pipeline Optimization Tool  \n",
    "  \n",
    "Name: pyspark  \n",
    "Version: 3.4.1  \n",
    "Summary: Apache Spark Python API  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import pyspark                      # Apache Spark:             Cluster Computing\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
