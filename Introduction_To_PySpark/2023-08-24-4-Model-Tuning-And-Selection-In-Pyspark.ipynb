{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning and selection in Pyspark\n",
    "  \n",
    "In this last chapter, you'll apply what you've learned to create a model that predicts which flights will be delayed.\n",
    "  \n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   \n",
    "      /_/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pyspark.SparkContext()</td>\n",
    "    <td>Create a new SparkContext instance, the entry point to using Spark functionality.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>pyspark.SparkContext().version</td>\n",
    "    <td>Retrieve the version of the SparkContext.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>pyspark.SparkContext().stop()</td>\n",
    "    <td>Stop the SparkContext, releasing associated resources.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>pyspark.sql.SparkSession</td>\n",
    "    <td>Create a new SparkSession instance, offering an entry point for DataFrame and SQL functionality.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.getOrCreate()</td>\n",
    "    <td>Retrieve an existing SparkSession or create a new one if none exists.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.appName</td>\n",
    "    <td>Set the application name for the SparkSession.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.getOrCreate</td>\n",
    "    <td>Get an existing SparkSession or create a new one if none exists.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>pyspark.sql.SparkSession.read</td>\n",
    "    <td>Create a DataFrameReader for reading data in various formats.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format</td>\n",
    "    <td>Specify the input data format when reading data using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.option('inferSchema', 'True')</td>\n",
    "    <td>Specify options, such as inferring schema from data, when reading data using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.option('header', 'True')</td>\n",
    "    <td>Specify options, such as reading headers from data, when reading data using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.load()</td>\n",
    "    <td>Load data into a DataFrame based on specified options using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>pyspark.sql.SparkSession.createOrReplaceTempView</td>\n",
    "    <td>Create or replace a temporary view of a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>pyspark.sql.SparkSession.catalog.listTables()</td>\n",
    "    <td>List the tables available in the catalog.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>pyspark.sql.SparkSession.sql()</td>\n",
    "    <td>Execute a SQL query and return the result as a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>pyspark.sql.SparkSession.sql().toPandas()</td>\n",
    "    <td>Convert the result of a SQL query to a Pandas DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>pyspark.sql.SparkSession.sql().toPandas().head()</td>\n",
    "    <td>Retrieve the first few rows of a Pandas DataFrame obtained from a SQL query result.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>pyspark.sql.SparkSession.createDataFrame()</td>\n",
    "    <td>Create a DataFrame from a list or RDD.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>pyspark.sql.SparkSession.read.csv()</td>\n",
    "    <td>Read data from a CSV file and load it into a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>pyspark.sql.SparkSession.table</td>\n",
    "    <td>Create a DataFrame representing a table in the catalog.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>21</td>\n",
    "    <td>pyspark.sql.SparkSession.filter</td>\n",
    "    <td>Filter rows of a DataFrame based on a condition.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>pyspark.sql.SparkSession.select</td>\n",
    "    <td>Select columns from a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>23</td>\n",
    "    <td>pyspark.sql.SparkSession.selectExpr</td>\n",
    "    <td>Select columns using SQL expressions from a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>24</td>\n",
    "    <td>pyspark.sql.SparkSession.printSchema</td>\n",
    "    <td>Print the schema of a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25</td>\n",
    "    <td>pyspark.sql.SparkSession.withColumn</td>\n",
    "    <td>Add or replace a column in a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26</td>\n",
    "    <td>pyspark.sql.types.IntegerType</td>\n",
    "    <td>Create an IntegerType column type for use in DataFrame schema.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27</td>\n",
    "    <td>pyspark.sql.functions.col</td>\n",
    "    <td>Reference a column in a DataFrame based on its name.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>28</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy</td>\n",
    "    <td>Group rows in a DataFrame based on specified columns.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>29</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.min</td>\n",
    "    <td>Compute the minimum value of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>30</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.max</td>\n",
    "    <td>Compute the maximum value of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.avg</td>\n",
    "    <td>Compute the average value of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>32</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.sum</td>\n",
    "    <td>Compute the sum of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>33</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.count</td>\n",
    "    <td>Compute the count of rows for grouped columns.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>34</td>\n",
    "    <td>pyspark.sql.functions.stddev</td>\n",
    "    <td>Compute the standard deviation of specified columns in a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>35</td>\n",
    "    <td>pyspark.sql.SparkSession.withColumnRenamed</td>\n",
    "    <td>Rename a column in a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>36</td>\n",
    "    <td>pyspark.sql.SparkSession.join</td>\n",
    "    <td>Join two DataFrames based on specified columns.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>37</td>\n",
    "    <td>pyspark.ml.feature.StringIndexer</td>\n",
    "    <td>Convert categorical strings to numerical indices using StringIndexer.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>38</td>\n",
    "    <td>pyspark.ml.feature.OneHotEncoder</td>\n",
    "    <td>Encode categorical indices as one-hot vectors using OneHotEncoder.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>39</td>\n",
    "    <td>pyspark.ml.feature.VectorAssembler</td>\n",
    "    <td>Combine multiple columns into a single feature vector using VectorAssembler.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>40</td>\n",
    "    <td>pyspark.ml.Pipeline</td>\n",
    "    <td>Construct a ML pipeline by assembling a sequence of transformers and an estimator.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>41</td>\n",
    "    <td>pyspark.sql.SparkSession.randomSplit</td>\n",
    "    <td>Randomly split a DataFrame into training and testing datasets.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: pyspark  \n",
    "Version: 3.4.1  \n",
    "Summary: Apache Spark Python API  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import pyspark                      # Apache Spark:             Cluster Computing\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is logistic regression?\n",
    "  \n",
    "The model you'll be fitting in this chapter is called a logistic regression. This model is very similar to a linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event.\n",
    "  \n",
    "To use this as a classification algorithm, all you have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, you classify that observation as a 'yes' (in this case, the flight being late), if it's below, you classify it as a 'no'!\n",
    "  \n",
    "You'll tune this model by testing different values for several hyperparameters. A hyperparameter is just a value in the model that's not estimated from the data, but rather is supplied by the user to maximize performance. For this course it's not necessary to understand the mathematics behind all of these values - what's important is that you'll try out a few different choices and pick the best one.\n",
    "  \n",
    "---\n",
    "  \n",
    "Why do you supply hyperparameters?\n",
    "  \n",
    "1. Possible Answers\n",
    "  \n",
    "- [ ] They explain information about the data.\n",
    "- [x] They improve model performance.\n",
    "- [ ] They improve model fitting speed.\n",
    "  \n",
    "Great job! You supply hyperparameters to optimize your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
