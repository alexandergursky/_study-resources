{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning and selection in Pyspark\n",
    "  \n",
    "In this last chapter, you'll apply what you've learned to create a model that predicts which flights will be delayed.\n",
    "  \n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   \n",
    "      /_/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pyspark.SparkContext()</td>\n",
    "    <td>Create a new SparkContext instance, the entry point to using Spark functionality.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>pyspark.SparkContext().version</td>\n",
    "    <td>Retrieve the version of the SparkContext.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>pyspark.SparkContext().stop()</td>\n",
    "    <td>Stop the SparkContext, releasing associated resources.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>pyspark.sql.SparkSession</td>\n",
    "    <td>Create a new SparkSession instance, offering an entry point for DataFrame and SQL functionality.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.getOrCreate()</td>\n",
    "    <td>Retrieve an existing SparkSession or create a new one if none exists.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.appName</td>\n",
    "    <td>Set the application name for the SparkSession.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>pyspark.sql.SparkSession.builder.getOrCreate</td>\n",
    "    <td>Get an existing SparkSession or create a new one if none exists.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>pyspark.sql.SparkSession.read</td>\n",
    "    <td>Create a DataFrameReader for reading data in various formats.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format</td>\n",
    "    <td>Specify the input data format when reading data using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.option('inferSchema', 'True')</td>\n",
    "    <td>Specify options, such as inferring schema from data, when reading data using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.option('header', 'True')</td>\n",
    "    <td>Specify options, such as reading headers from data, when reading data using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>pyspark.sql.SparkSession.read.format.load()</td>\n",
    "    <td>Load data into a DataFrame based on specified options using the DataFrameReader.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>pyspark.sql.SparkSession.createOrReplaceTempView</td>\n",
    "    <td>Create or replace a temporary view of a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>pyspark.sql.SparkSession.catalog.listTables()</td>\n",
    "    <td>List the tables available in the catalog.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>pyspark.sql.SparkSession.sql()</td>\n",
    "    <td>Execute a SQL query and return the result as a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>pyspark.sql.SparkSession.sql().toPandas()</td>\n",
    "    <td>Convert the result of a SQL query to a Pandas DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>pyspark.sql.SparkSession.sql().toPandas().head()</td>\n",
    "    <td>Retrieve the first few rows of a Pandas DataFrame obtained from a SQL query result.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>pyspark.sql.SparkSession.createDataFrame()</td>\n",
    "    <td>Create a DataFrame from a list or RDD.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>pyspark.sql.SparkSession.read.csv()</td>\n",
    "    <td>Read data from a CSV file and load it into a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>pyspark.sql.SparkSession.table</td>\n",
    "    <td>Create a DataFrame representing a table in the catalog.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>21</td>\n",
    "    <td>pyspark.sql.SparkSession.filter</td>\n",
    "    <td>Filter rows of a DataFrame based on a condition.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>pyspark.sql.SparkSession.select</td>\n",
    "    <td>Select columns from a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>23</td>\n",
    "    <td>pyspark.sql.SparkSession.selectExpr</td>\n",
    "    <td>Select columns using SQL expressions from a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>24</td>\n",
    "    <td>pyspark.sql.SparkSession.printSchema</td>\n",
    "    <td>Print the schema of a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25</td>\n",
    "    <td>pyspark.sql.SparkSession.withColumn</td>\n",
    "    <td>Add or replace a column in a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26</td>\n",
    "    <td>pyspark.sql.types.IntegerType</td>\n",
    "    <td>Create an IntegerType column type for use in DataFrame schema.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27</td>\n",
    "    <td>pyspark.sql.functions.col</td>\n",
    "    <td>Reference a column in a DataFrame based on its name.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>28</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy</td>\n",
    "    <td>Group rows in a DataFrame based on specified columns.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>29</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.min</td>\n",
    "    <td>Compute the minimum value of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>30</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.max</td>\n",
    "    <td>Compute the maximum value of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.avg</td>\n",
    "    <td>Compute the average value of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>32</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.sum</td>\n",
    "    <td>Compute the sum of specified columns for grouped rows.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>33</td>\n",
    "    <td>pyspark.sql.SparkSession.groupBy.count</td>\n",
    "    <td>Compute the count of rows for grouped columns.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>34</td>\n",
    "    <td>pyspark.sql.functions.stddev</td>\n",
    "    <td>Compute the standard deviation of specified columns in a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>35</td>\n",
    "    <td>pyspark.sql.SparkSession.withColumnRenamed</td>\n",
    "    <td>Rename a column in a DataFrame.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>36</td>\n",
    "    <td>pyspark.sql.SparkSession.join</td>\n",
    "    <td>Join two DataFrames based on specified columns.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>37</td>\n",
    "    <td>pyspark.ml.feature.StringIndexer</td>\n",
    "    <td>Convert categorical strings to numerical indices using StringIndexer.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>38</td>\n",
    "    <td>pyspark.ml.feature.OneHotEncoder</td>\n",
    "    <td>Encode categorical indices as one-hot vectors using OneHotEncoder.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>39</td>\n",
    "    <td>pyspark.ml.feature.VectorAssembler</td>\n",
    "    <td>Combine multiple columns into a single feature vector using VectorAssembler.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>40</td>\n",
    "    <td>pyspark.ml.Pipeline</td>\n",
    "    <td>Construct a ML pipeline by assembling a sequence of transformers and an estimator.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>41</td>\n",
    "    <td>pyspark.sql.SparkSession.randomSplit</td>\n",
    "    <td>Randomly split a DataFrame into training and testing datasets.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: pyspark  \n",
    "Version: 3.4.1  \n",
    "Summary: Apache Spark Python API  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import pyspark                      # Apache Spark:             Cluster Computing\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is logistic regression?\n",
    "  \n",
    "The model you'll be fitting in this chapter is called a logistic regression. This model is very similar to a linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event.\n",
    "  \n",
    "To use this as a classification algorithm, all you have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, you classify that observation as a 'yes' (in this case, the flight being late), if it's below, you classify it as a 'no'!\n",
    "  \n",
    "You'll tune this model by testing different values for several hyperparameters. A hyperparameter is just a value in the model that's not estimated from the data, but rather is supplied by the user to maximize performance. For this course it's not necessary to understand the mathematics behind all of these values - what's important is that you'll try out a few different choices and pick the best one.\n",
    "  \n",
    "---\n",
    "  \n",
    "Why do you supply hyperparameters?\n",
    "  \n",
    "1. Possible Answers\n",
    "  \n",
    "- [ ] They explain information about the data.\n",
    "- [x] They improve model performance.\n",
    "- [ ] They improve model fitting speed.\n",
    "  \n",
    "Great job! You supply hyperparameters to optimize your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the modeler\n",
    "  \n",
    "The `Estimator` you'll be using is a `LogisticRegression` from the `pyspark.ml.classification` submodule.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the `LogisticRegression` class from `pyspark.ml.classification`.\n",
    "2. Create a `LogisticRegression` called `lr` by calling `LogisticRegression()` with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/27 15:00:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName('flights').getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! That's the first step to any modeling in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "  \n",
    "In the next few exercises you'll be tuning your logistic regression model using a procedure called k-fold cross validation. This is a method of estimating the model's performance on unseen data (like your `test` DataFrame).\n",
    "  \n",
    "It works by splitting the training data into a few different partitions. The exact number is up to you, but in this course you'll be using PySpark's default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data.\n",
    "  \n",
    "You'll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, `elasticNetParam=` and `regParam=`, and using the cross validation error to compare all the different models so you can choose the best one!\n",
    "  \n",
    "---\n",
    "  \n",
    "What does cross validation allow you to estimate?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [x] The model's error on held out data.\n",
    "- [ ] The model's error on data used for fitting.\n",
    "- [ ] The time it will take to fit the model.\n",
    "  \n",
    "Exactly! The cross validation error is an estimate of the model's error on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the evaluator\n",
    "  \n",
    "The first thing you need when doing cross validation for model selection is a way to compare different models. Luckily, the `pyspark.ml.evaluation` submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you'll be using the `BinaryClassificationEvaluator` from the `pyspark.ml.evaluation` module.\n",
    "  \n",
    "This evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. You'll learn more about this towards the end of the chapter!\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the submodule `pyspark.ml.evaluation` as `evals`.\n",
    "2. Create evaluator by calling `evals.BinaryClassificationEvaluator()` with the argument `metricName=\"areaUnderROC\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now you can compare models using the metric output by your `evaluator`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a grid\n",
    "Next, you need to create a grid of values to search over when looking for the optimal hyperparameters. The submodule `pyspark.ml.tuning` includes a class called `ParamGridBuilder` that does just that (maybe you're starting to notice a pattern here; PySpark has a submodule for just about everything!).\n",
    "\n",
    "You'll need to use the `.addGrid()` and `.build()` methods to create a grid that you can use for cross validation. The `.addGrid()` method takes a model parameter (an attribute of the model `Estimator`, `lr`, that you created a few exercises ago) and a list of values that you want to try. The `.build()` method takes no arguments, it just returns the grid that you'll use later.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the submodule `pyspark.ml.tuning` under the alias tune.\n",
    "2. Call the class constructor `ParamGridBuilder()` with no arguments. Save this as grid.\n",
    "3. Call the `.addGrid()` method on grid with `lr.regParam` as the first argument and `numpy.arange(0, .1, .01)` as the second argument. This second call is a function from the `numpy` module (imported as `np`) that creates a list of numbers from 0 to 0.1, incrementing by 0.01. Overwrite grid with the result.\n",
    "4. Update grid again by calling the `.addGrid()` method a second time create a grid for `lr.elasticNetParam` that includes only the values [0, 1].\n",
    "5. Call the `.build()` method on grid and overwrite it with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import tuning as tune\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! That's the last ingredient in your cross validation recipe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the validator\n",
    "  \n",
    "The submodule `pyspark.ml.tuning` also has a class called `CrossValidator` for performing cross validation. This `Estimator` takes the modeler you want to fit, the grid of hyperparameters you created, and the evaluator you want to use to compare your models.\n",
    "  \n",
    "The submodule `pyspark.ml.tune` has already been imported as tune. You'll create the CrossValidator by passing it the logistic regression `Estimator` `lr`, the parameter grid, and the evaluator you created in the previous exercises.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Create a `CrossValidator` by calling `tune.CrossValidator()` with the arguments:\n",
    "- `estimator=lr`\n",
    "- `estimatorParamMaps=grid`\n",
    "- `evaluator=evaluator`\n",
    "2. Name this object `cv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You're almost a machine learning pro!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model(s)\n",
    "  \n",
    "You're finally ready to fit the models and select the best one!\n",
    "  \n",
    "Unfortunately, cross validation is a very computationally intensive procedure. Fitting all the models would take too long.\n",
    "  \n",
    "To do this locally you would use the code:\n",
    "  \n",
    "```python\n",
    "# Fit cross validation models\n",
    "models = cv.fit(training)\n",
    "\n",
    "# Extract the best model\n",
    "best_lr = models.bestModel\n",
    "```\n",
    "  \n",
    "Remember, the training data is called `training` and you're using `lr` to fit a logistic regression model. Cross validation selected the parameter values `regParam=0` and `elasticNetParam=0` as being the best. These are the default values, so you don't need to do anything else with `lr` before fitting the model.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Create `best_lr` by calling `lr.fit()` on the training data.\n",
    "2. Print `best_lr` to verify that it's an object of the `LogisticRegressionModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Dataframe flights table\n",
    "flights = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"../_datasets/flights_small.csv\"))\n",
    "flights.createOrReplaceTempView(\"flights\")  # Created table 1\n",
    "\n",
    "# Dataframe planes table\n",
    "planes = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load('../_datasets/planes.csv'))\n",
    "planes.createOrReplaceTempView(\"planes\")    # Created table 2\n",
    "\n",
    "# Rename year column to plane_year\n",
    "planes = planes.withColumnRenamed('year', 'plane_year')\n",
    "\n",
    "# Join the DataFrames\n",
    "model_data = flights.join(planes, on='tailnum', how='leftouter')\n",
    "\n",
    "# Cast the columns to integers\n",
    "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast('integer'))\n",
    "model_data = model_data.withColumn('air_time', model_data.air_time.cast('integer'))\n",
    "model_data = model_data.withColumn('month', model_data.month.cast('integer'))\n",
    "model_data = model_data.withColumn('plane_year', model_data.plane_year.cast('integer'))\n",
    "\n",
    "# Create the column plane_age\n",
    "model_data = model_data.withColumn('plane_age', model_data.year - model_data.plane_year)\n",
    "\n",
    "# Create is_late\n",
    "model_data = model_data.withColumn('is_late', model_data.arr_delay > 0)\n",
    "\n",
    "# Convert to an integer\n",
    "model_data = model_data.withColumn('label', model_data.is_late.cast('integer'))\n",
    "\n",
    "# Remove missing values\n",
    "model_data = model_data.filter(\n",
    "    'arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL'\n",
    ")\n",
    "\n",
    "# Create StringIndexer\n",
    "carr_indexer = StringIndexer(inputCol='carrier', outputCol='carrier_index')\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "carr_encoder = OneHotEncoder(inputCol='carrier_index', outputCol='carrier_fact')\n",
    "\n",
    "# Create a StringIndexer\n",
    "dest_indexer = StringIndexer(inputCol='dest', outputCol='dest_index')\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "dest_encoder = OneHotEncoder(inputCol='dest_index', outputCol='dest_fact')\n",
    "\n",
    "# Make a VectorAssembler\n",
    "vec_assembler = VectorAssembler(\n",
    "    inputCols=['month', 'air_time', 'carrier_fact', 'dest_fact', 'plane_age'],\n",
    "    outputCol='features')\n",
    "\n",
    "# Make the pipeline\n",
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler])\n",
    "\n",
    "# Fit and transform the data\n",
    "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "training, test = piped_data.randomSplit([.6, .4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 15:01:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_6117bb848e78, numClasses=2, numFeatures=81\n"
     ]
    }
   ],
   "source": [
    "# Call lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# Print best_lr\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! You fit your first Spark model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating binary classifiers\n",
    "  \n",
    "For this course we'll be using a common metric for binary classification algorithms call the AUC, or area under the curve. In this case, the curve is the ROC, or receiver operating curve. The details of what these things actually measure isn't important for this course. All you need to know is that for our purposes, the closer the AUC is to one (1), the better the model is!\n",
    "  \n",
    "---\n",
    "  \n",
    "If you've created a perfect binary classification model, what would the AUC be?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] -1\n",
    "- [x] 1\n",
    "- [ ] 0\n",
    "- [ ] .5\n",
    "  \n",
    "Great job! An AUC of one represents a model that always perfectly classifies observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "  \n",
    "Remember the test data that you set aside waaaaaay back in chapter 3? It's finally time to test your model on it! You can use the same evaluator you made to fit the model.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Use your model to generate predictions by applying `best_lr.transform()` to the test data. Save this as `test_results`.\n",
    "2. Call `evaluator.evaluate()` on `test_results` to compute the AUC. Print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7063483719812562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! What do you think of the AUC? Your model isn't half bad! You went from knowing nothing about Spark to doing advanced machine learning. Great job on making it to the end of the course! The next steps are learning how to create large scale Spark clusters and manage and submit jobs so that you can use models in the real world. Remember, Spark is still being actively developed, so there's new features coming all the time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
