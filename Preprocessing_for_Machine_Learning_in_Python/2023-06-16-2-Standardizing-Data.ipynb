{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing Data\n",
    "  \n",
    "This chapter is all about standardizing data. Often a model will make some assumptions about the distribution or scale of your features. Standardization is a way to make your data fit these assumptions and improve the algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "  \n",
    "It's possible that you'll come across datasets with lots of numerical noise, perhaps due to feature variance or differently-scaled data. The preprocessing solution for that is standardization.\n",
    "  \n",
    "**What is standardization?**\n",
    "  \n",
    "Standardization is a preprocessing method used to transform continuous data to make it look normally distributed. In scikit-learn, this is often a necessary step, because many models make underlying assumptions that the training data is normally distributed, and if it isn't, we could risk risk biasing your model. Data can be standardized in many different ways, but in this course, we're going to talk about two methods: log normalization and scaling. \n",
    "  \n",
    "It's also important to note that standardization is a preprocessing method applied to continuous, numerical data. We'll cover methods for dealing with categorical data later in the course.\n",
    "  \n",
    "**When to standardize: linear distances**\n",
    "  \n",
    "There are a few different scenarios in which we'd want to standardize your data. First, if we're working with any kind of model that uses a linear distance metric or operates in a linear space like k-nearest neighbors, linear regression, or k-means clustering, the model is assuming that the data and features we're giving it are related in a linear fashion, or can be measured with a linear distance metric, which may not always be the case.\n",
    "  \n",
    "**When to standardize: high variance**\n",
    "  \n",
    "Standardization should also be used when dataset features have a high variance, which is also related to distance metrics. This could bias a model that assumes the data is normally distributed. If a feature in our dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset.\n",
    "  \n",
    "**When to standardize: different scales**\n",
    "  \n",
    "Modeling a dataset that contains continuous features that are on different scales is another standardization scenario. For example, consider predicting house prices using two features: the number of bedrooms and the last sale price. These two features are on vastly different scales, which will confuse most models. To compare these features, we must standardize them to put them in the same linear space. All of these scenarios assume we're working with a model that makes some kind of linearity assumptions; however, there are a number of models that are perfectly fine operating in a nonlinear space, or do a certain amount of standardization upon input, but they're outside the scope of this course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to standardize\n",
    "  \n",
    "**Now that you've learned when it is appropriate to standardize your data, which of these scenarios is NOT a reason to standardize?**\n",
    "  \n",
    "Possible Answers  \n",
    "  \n",
    "- [ ] A column you want to use for modeling has extremely high variance.\n",
    "\n",
    "- [ ] You have a dataset with several continuous columns on different scales, and you'd like to use a linear model to train the data.\n",
    "\n",
    "- [ ] The models you're working with use some sort of distance metric in a linear space.\n",
    "\n",
    "- [x] Your dataset is comprised of categorical data.\n",
    "  \n",
    "Correct! Standardization is a preprocessing task performed on numerical, continuous data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling without normalizing\n",
    "  \n",
    "Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first.\n",
    "  \n",
    "Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n",
    "  \n",
    "The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on.\n",
    "  \n",
    "1. Split up the X and y sets into training and test sets, ensuring that class labels are equally distributed in both sets.\n",
    "  \n",
    "2. Fit the knn model to the training features and labels.\n",
    "  \n",
    "3. Print the test set accuracy of the knn model using the `.score()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0     1    14.23        1.71  2.43               15.6        127   \n",
       "1     1    13.20        1.78  2.14               11.2        100   \n",
       "2     1    13.16        2.36  2.67               18.6        101   \n",
       "3     1    14.37        1.95  2.50               16.8        113   \n",
       "4     1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('../_datasets/wine_types.csv')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X/y split\n",
    "X, y = wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']], wine['Type'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6888888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Instanciate the KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Seeding\n",
    "SEED = 42\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=SEED)\n",
    "\n",
    "# Fit the knn model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the accuracy score is pretty low at (69%).  \n",
    "Let's explore methods to improve this score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log normalization\n",
    "  \n",
    "The first method we'll cover for standardization is log normalization.\n",
    "  \n",
    "**What is log normalization?**\n",
    "  \n",
    "Log normalization is a method for standardizing data that can be useful when we have features with high variance. Log normalization applies a logarithmic transformation to our values, which transforms them onto a scale that approximates normality - an assumption that many models make. The method of log normalization we're going to work with takes the natural log of each number; this is the exponent you would raise above the mathematical constant e (approximately equal to 2.718) to get that number.\n",
    "  \n",
    "**What is log normalization?**\n",
    "  \n",
    "Looking at the following table, the log of 30 is 3.4, because e to the power of 3.4 equals 30. Log normalization is a good strategy when you care about relative changes in a linear model, but still want to capture the magnitude of change, and when we want to keep everything in the positive space. It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "  \n",
    "**Log normalization in Python**\n",
    "  \n",
    "Applying log normalization to data in Python is fairly straightforward. We can use the `log()` function from NumPy to do the transformation. \n",
    "  \n",
    "Here we have a DataFrame of some values. If we check the variance of the columns, we can see that column 2 has a significantly higher variance than column 1, which makes it a clear candidate for log normalization. To apply log normalization to column 2, we need the `log()` function from numpy. We can pass the column we want to log normalize directly into the function. If we take a look at both column 2 and the log-normalized column-2, we can see that the transformation has scaled down the values. If we check the variance of both column 1 and the log-normalized column 2, we can see that the variances are now much closer together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the variance\n",
    "  \n",
    "Check the variance of the columns in the wine dataset. Out of the four columns listed, which column is the most appropriate candidate for normalization?  \n",
    "  \n",
    "in: `wine.var()`  \n",
    "out:  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Type</th>\n",
    "    <td>0.601</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Alcohol</th>\n",
    "    <td>0.659</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Malic acid</th>\n",
    "    <td>1.248</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Ash</th>\n",
    "    <td>0.075</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Alcalinity of ash</th>\n",
    "    <td>11.153</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Magnesium</th>\n",
    "    <td>203.989</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Total phenols</th>\n",
    "    <td>0.392</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Flavanoids</th>\n",
    "    <td>0.998</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Nonflavanoid phenols</th>\n",
    "    <td>0.015</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Proanthocyanins</th>\n",
    "    <td>0.328</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Color intensity</th>\n",
    "    <td>5.374</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Hue</th>\n",
    "    <td>0.052</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>OD280/OD315 of diluted wines</th>\n",
    "    <td>0.504</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Proline</th>\n",
    "    <td>99166.717</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "Correct! The Proline column has an extremely high variance.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log normalization in Python\n",
    "  \n",
    "Now that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.\n",
    "  \n",
    "1. Print out the variance of the Proline column for reference.\n",
    "  \n",
    "2. Use the `np.log()` function on the Proline column to create a new, log-normalized column named Proline_log.\n",
    "  \n",
    "3. Print out the variance of the Proline_log column to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99166.71735542436\n",
      "0.17231366191842012\n"
     ]
    }
   ],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(wine.Proline.var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine.Proline_log.var())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.log()` function is an easy way to log normalize a column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data for feature comparison\n",
    "  \n",
    "**What is feature scaling?**\n",
    "  \n",
    "Scaling is a method of standardization that's most useful when we're working with a dataset that contains continuous features that are on different scales, and we're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors). Feature scaling transforms the features in your dataset so they have a mean of zero and a variance of one. This will make it easier to linearly compare features, which is a requirement for many models in scikit-learn.\n",
    "  \n",
    "**How to scale data**\n",
    "  \n",
    "Let's take a look at another DataFrame. In each column, we have numbers that have consistent scales within columns, but not across columns. If we look at the variance, it's relatively low across columns. To better model this data, scaling would be a good choice here.\n",
    "  \n",
    "Scikit-learn has a variety of scaling methods, but we'll focus on `StandardScaler()`, which is imported from `sklearn.preprocessing`. This method works by subtracting the mean and scaling each feature to have a variance of one. Once we instantiate a `StandardScaler()`, we can apply the `.fit_transform()` method on the DataFrame. We can convert the output of `.fit_transform()`, which is a numpy array, to a DataFrame to look at it more easily. If we take a look at the newly scaled DataFrame, we can see that the values have been scaled down, and if we calculate the variance by column, it's not only close to 1, but it's now the same for all of our features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - investigating columns\n",
    "  \n",
    "You want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Ash  Alcalinity of ash   Magnesium\n",
      "count  178.000000         178.000000  178.000000\n",
      "mean     2.366517          19.494944   99.741573\n",
      "std      0.274344           3.339564   14.282484\n",
      "min      1.360000          10.600000   70.000000\n",
      "25%      2.210000          17.200000   88.000000\n",
      "50%      2.360000          19.500000   98.000000\n",
      "75%      2.557500          21.500000  107.000000\n",
      "max      3.230000          30.000000  162.000000\n"
     ]
    }
   ],
   "source": [
    "print(wine[['Ash', 'Alcalinity of ash', 'Magnesium']].describe())  # Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ash                    0.075265\n",
      "Alcalinity of ash     11.152686\n",
      "Magnesium            203.989335\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wine[['Ash', 'Alcalinity of ash', 'Magnesium']].var())  # Variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding your data is a crucial first step before deciding on the most appropriate standardization technique."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - standardizing columns\n",
    "  \n",
    "Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
    "  \n",
    "1. Import the StandardScaler class.\n",
    "\n",
    "2. Instantiate a `StandardScaler()` and store it in the variable, scaler.\n",
    "  \n",
    "3. Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset.\n",
    "  \n",
    "4. Fit and transform the standard scaler to wine_subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Ash  Alcalinity of ash  Magnesium\n",
      "0  2.43               15.6        127\n",
      "1  2.14               11.2        100\n",
      "2  2.67               18.6        101 \n",
      "\n",
      "[[ 0.23205254 -1.16959318  1.91390522]\n",
      " [-0.82799632 -2.49084714  0.01814502]\n",
      " [ 1.10933436 -0.2687382   0.08835836]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Creating the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale\n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "print(wine_subset.iloc[:3], '\\n')\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)\n",
    "\n",
    "print(wine_subset_scaled[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, running `.fit_transform()` during preprocessing will both fit the method to the data as well as transform the data in a single step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized data and modeling\n",
    "  \n",
    "Now that we've learned a couple of different methods for standardization, it's time to see how this fits into the modeling workflow. As mentioned before, many models in scikit-learn require our data to be scaled appropriately across columns, otherwise we risk biasing the results.\n",
    "  \n",
    "**K-nearest neighbors**\n",
    "  \n",
    "You should already be a little familiar with both k-nearest neighbors, as well as the scikit-learn workflow, based on previous courses, but we'll do a quick review of both. K-nearest neighbors is a model that classifies data based on its distance to training set data. A new data point is assigned a label based on the class that the majority of surrounding data points belong to. \n",
    "  \n",
    "**General workflow for ML modeling**\n",
    "\n",
    "The workflow for training a model in scikit-learn starts with splitting the data into a training and test set. This can be done with scikit-learn's `train_test_split()` function. Splitting the data will allow us to evaluate the model's performance using unseen data, rather than evaluating its performance on the data it was trained on. \n",
    "  \n",
    "Once the data has been split, we can begin preprocessing the training data. It's really important to split the data prior to preprocessing, so none of the test data is used to train the model. When non-training data is used to train the model, this is called data-leakage, and it should be avoided so that any performance metrics are reflective of the model's ability to generalize to unseen data. \n",
    "  \n",
    "We instantiate a k-neighbors classifier and a standard scaler to scale our features. Here, we preprocess and fit the training features using the `.fit_transform()` method, and preprocess the test features using the `.transform()` method. Using the `.transform()` method means that the test features won't be used to fit the model and avoids data leakage. \n",
    "  \n",
    "Now that we've finished preprocessing, we can fit the KNN model to the scaled training features, and return the test set accuracy using the `.score()` method on the scaled test features and test labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on non-scaled data\n",
    "  \n",
    "Before adding standardization to your scikit-learn workflow, you'll first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.\n",
    "  \n",
    "1. Split the dataset into training and test sets.\n",
    "  \n",
    "2. Fit the knn model to the training data.\n",
    "  \n",
    "3. Print out the test set accuracy of your trained knn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load data\n",
    "wine = pd.read_csv('../_datasets/wine_types.csv')\n",
    "\n",
    "# X/y split\n",
    "X, y = wine.drop('Type', axis=1), wine['Type'] \n",
    "\n",
    "# Seeding\n",
    "SEED = 42\n",
    "\n",
    "# Instanciate KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=SEED)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy definitely isn't poor, but let's see if we can improve it by standardizing the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on scaled data\n",
    "  \n",
    "The accuracy score on the unscaled wine dataset was decent (77.78%), but let's see what you can achieve by using standardization.\n",
    "  \n",
    "1. Create the `StandardScaler()` method, stored in a variable named scaler.\n",
    "  \n",
    "2. Scale the training and test features, being careful not to introduce *data-leakage*.\n",
    "  \n",
    "3. Fit the knn model to the scaled training data.\n",
    "  \n",
    "4. Evaluate the model's performance by computing the test set accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.fit_transform()` when scaling the training features.  \n",
    "Use `.transform()` when scaling the test features.  \n",
    "Use `.fit()` when fitting the knn model to the scaled training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Load data\n",
    "wine = pd.read_csv('../_datasets/wine_types.csv')\n",
    "\n",
    "# Seeding\n",
    "SEED = 42\n",
    "\n",
    "# X/y split\n",
    "X, y = wine.drop('Type', axis=1), wine['Type']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Instantiate a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite the improvement, and definitely made scaling the data worthwhile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
