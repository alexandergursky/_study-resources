{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "  \n",
    "In this chapter, we're going to talk about a very important part of the preprocessing workflow: feature engineering. \n",
    "  \n",
    "You'll learn about feature engineering. You'll explore different ways to create new, more useful, features from the ones already in your dataset. You'll see how to encode, aggregate, and extract information from both numerical and textual features.\n",
    "  \n",
    "**What is feature engineering?**\n",
    "  \n",
    "Real-world data is often not neat and tidy, and in addition to preprocessing steps like standardization, we'll likely have to extract and expand information from existing features. Feature engineering is the creation of new features based on existing features, and it adds information to the dataset that can improve prediction or clustering tasks, or adds insight into relationships between features. In this chapter, we'll just focus on the key components for preprocessing. \n",
    "  \n",
    "There are automated ways to create new features, but for now, we're going to cover manual methods of feature engineering. Manual methods require us to already have an in-depth knowledge of the dataset that we're working with. Feature engineering is also something that is very dependent on the particular dataset you're analyzing. The goal for this chapter is to demonstrate some scenarios where feature engineering can be useful.\n",
    "  \n",
    "**Feature engineering scenarios**\n",
    "  \n",
    "There are a variety of scenarios where we might want to engineer features from existing data. An extremely common one is with text data. For example, if we're building some kind of natural language processing model, we'll have to create a vector of the words in our dataset. Another scenario might also be related to string data: maybe we have a column of people's favorite colors. In order to feed this information into a scikit-learn model, we'll have to encode this information numerically.\n",
    "  \n",
    "Another common example is with timestamps. We might see a full timestamp that includes the time down to the second or millisecond, which might be much too granular for a prediction task, so we can create a new column that contains the day or the month component. Some columns can also contain a list of some kind, such as test scores, or running times, and maybe it's more useful to use an average. These are all examples of situations where we'd want to generate new features from existing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering knowledge test\n",
    "  \n",
    "Now that you've learned about feature engineering, which of the following examples are good candidates for creating new features?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] A column of timestamps\n",
    "  \n",
    "- [ ] A column of newspaper headlines\n",
    "  \n",
    "- [ ] A column of weight measurements\n",
    "  \n",
    "- [x] Both 1 and 2\n",
    "  \n",
    "- [ ] None of the above\n",
    "  \n",
    "Correct! Timestamps can be broken into days or months, and headlines can be used for natural language processing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying areas for feature engineering\n",
    "  \n",
    "Take an exploratory look at the volunteer dataset.\n",
    "  \n",
    "Which of the following columns would you want to perform a feature engineering task on? \n",
    "- [ ] vol_requests\n",
    "  \n",
    "- [ ] title\n",
    "  \n",
    "- [ ] created_data\n",
    "  \n",
    "- [ ] category_desc\n",
    "  \n",
    "- [x] 2, 3, and 4\n",
    "  \n",
    "Correct! All three of these columns will require some feature engineering before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opportunity_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>vol_requests</th>\n",
       "      <th>event_time</th>\n",
       "      <th>title</th>\n",
       "      <th>hits</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_priority</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_desc</th>\n",
       "      <th>amsl</th>\n",
       "      <th>amsl_unit</th>\n",
       "      <th>org_title</th>\n",
       "      <th>org_content_id</th>\n",
       "      <th>addresses_count</th>\n",
       "      <th>locality</th>\n",
       "      <th>region</th>\n",
       "      <th>postalcode</th>\n",
       "      <th>primary_loc</th>\n",
       "      <th>display_url</th>\n",
       "      <th>recurrence_type</th>\n",
       "      <th>hours</th>\n",
       "      <th>created_date</th>\n",
       "      <th>last_modified_date</th>\n",
       "      <th>start_date_date</th>\n",
       "      <th>end_date_date</th>\n",
       "      <th>status</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Community Board</th>\n",
       "      <th>Community Council</th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>BIN</th>\n",
       "      <th>BBL</th>\n",
       "      <th>NTA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4996</td>\n",
       "      <td>37004</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>Volunteers Needed For Rise Up &amp; Stay Put! Home...</td>\n",
       "      <td>737</td>\n",
       "      <td>Building on successful events last summer and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Center For NYC Neighborhoods</td>\n",
       "      <td>4426</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/opportunities/4996</td>\n",
       "      <td>onetime</td>\n",
       "      <td>0</td>\n",
       "      <td>January 13 2011</td>\n",
       "      <td>June 23 2011</td>\n",
       "      <td>July 30 2011</td>\n",
       "      <td>July 30 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5008</td>\n",
       "      <td>37036</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Web designer</td>\n",
       "      <td>22</td>\n",
       "      <td>Build a website for an Afghan business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bpeace</td>\n",
       "      <td>37026</td>\n",
       "      <td>1</td>\n",
       "      <td>5 22nd St\\nNew York, NY 10010\\n(40.74053152272...</td>\n",
       "      <td>NY</td>\n",
       "      <td>10010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/opportunities/5008</td>\n",
       "      <td>onetime</td>\n",
       "      <td>0</td>\n",
       "      <td>January 14 2011</td>\n",
       "      <td>January 25 2011</td>\n",
       "      <td>February 01 2011</td>\n",
       "      <td>February 01 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5016</td>\n",
       "      <td>37143</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Urban Adventures - Ice Skating at Lasker Rink</td>\n",
       "      <td>62</td>\n",
       "      <td>Please join us and the students from Mott Hall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Street Project</td>\n",
       "      <td>3001</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>10026.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/opportunities/5016</td>\n",
       "      <td>onetime</td>\n",
       "      <td>0</td>\n",
       "      <td>January 19 2011</td>\n",
       "      <td>January 21 2011</td>\n",
       "      <td>January 29 2011</td>\n",
       "      <td>January 29 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>37237</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>Fight global hunger and support women farmers ...</td>\n",
       "      <td>14</td>\n",
       "      <td>The Oxfam Action Corps is a group of dedicated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oxfam America</td>\n",
       "      <td>2170</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/opportunities/5022</td>\n",
       "      <td>ongoing</td>\n",
       "      <td>0</td>\n",
       "      <td>January 21 2011</td>\n",
       "      <td>January 25 2011</td>\n",
       "      <td>February 14 2011</td>\n",
       "      <td>March 31 2012</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5055</td>\n",
       "      <td>37425</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Stop 'N' Swap</td>\n",
       "      <td>31</td>\n",
       "      <td>Stop 'N' Swap reduces NYC's waste by finding n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Environment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Office of Recycling Outreach and Education</td>\n",
       "      <td>36773</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NY</td>\n",
       "      <td>10455.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/opportunities/5055</td>\n",
       "      <td>onetime</td>\n",
       "      <td>0</td>\n",
       "      <td>January 28 2011</td>\n",
       "      <td>February 01 2011</td>\n",
       "      <td>February 05 2011</td>\n",
       "      <td>February 05 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   opportunity_id  content_id  vol_requests  event_time  \\\n",
       "0            4996       37004            50           0   \n",
       "1            5008       37036             2           0   \n",
       "2            5016       37143            20           0   \n",
       "3            5022       37237           500           0   \n",
       "4            5055       37425            15           0   \n",
       "\n",
       "                                               title  hits  \\\n",
       "0  Volunteers Needed For Rise Up & Stay Put! Home...   737   \n",
       "1                                       Web designer    22   \n",
       "2      Urban Adventures - Ice Skating at Lasker Rink    62   \n",
       "3  Fight global hunger and support women farmers ...    14   \n",
       "4                                      Stop 'N' Swap    31   \n",
       "\n",
       "                                             summary is_priority  category_id  \\\n",
       "0  Building on successful events last summer and ...         NaN          NaN   \n",
       "1             Build a website for an Afghan business         NaN          1.0   \n",
       "2  Please join us and the students from Mott Hall...         NaN          1.0   \n",
       "3  The Oxfam Action Corps is a group of dedicated...         NaN          1.0   \n",
       "4  Stop 'N' Swap reduces NYC's waste by finding n...         NaN          4.0   \n",
       "\n",
       "               category_desc  amsl  amsl_unit  \\\n",
       "0                        NaN   NaN        NaN   \n",
       "1  Strengthening Communities   NaN        NaN   \n",
       "2  Strengthening Communities   NaN        NaN   \n",
       "3  Strengthening Communities   NaN        NaN   \n",
       "4                Environment   NaN        NaN   \n",
       "\n",
       "                                    org_title  org_content_id  \\\n",
       "0                Center For NYC Neighborhoods            4426   \n",
       "1                                      Bpeace           37026   \n",
       "2                              Street Project            3001   \n",
       "3                               Oxfam America            2170   \n",
       "4  Office of Recycling Outreach and Education           36773   \n",
       "\n",
       "   addresses_count                                           locality region  \\\n",
       "0                1                                                NaN     NY   \n",
       "1                1  5 22nd St\\nNew York, NY 10010\\n(40.74053152272...     NY   \n",
       "2                1                                                NaN     NY   \n",
       "3                1                                                NaN     NY   \n",
       "4                1                                                NaN     NY   \n",
       "\n",
       "   postalcode  primary_loc          display_url recurrence_type  hours  \\\n",
       "0         NaN          NaN  /opportunities/4996         onetime      0   \n",
       "1     10010.0          NaN  /opportunities/5008         onetime      0   \n",
       "2     10026.0          NaN  /opportunities/5016         onetime      0   \n",
       "3      2114.0          NaN  /opportunities/5022         ongoing      0   \n",
       "4     10455.0          NaN  /opportunities/5055         onetime      0   \n",
       "\n",
       "      created_date last_modified_date   start_date_date     end_date_date  \\\n",
       "0  January 13 2011       June 23 2011      July 30 2011      July 30 2011   \n",
       "1  January 14 2011    January 25 2011  February 01 2011  February 01 2011   \n",
       "2  January 19 2011    January 21 2011   January 29 2011   January 29 2011   \n",
       "3  January 21 2011    January 25 2011  February 14 2011     March 31 2012   \n",
       "4  January 28 2011   February 01 2011  February 05 2011  February 05 2011   \n",
       "\n",
       "     status  Latitude  Longitude  Community Board  Community Council   \\\n",
       "0  approved       NaN        NaN              NaN                 NaN   \n",
       "1  approved       NaN        NaN              NaN                 NaN   \n",
       "2  approved       NaN        NaN              NaN                 NaN   \n",
       "3  approved       NaN        NaN              NaN                 NaN   \n",
       "4  approved       NaN        NaN              NaN                 NaN   \n",
       "\n",
       "   Census Tract  BIN  BBL  NTA  \n",
       "0           NaN  NaN  NaN  NaN  \n",
       "1           NaN  NaN  NaN  NaN  \n",
       "2           NaN  NaN  NaN  NaN  \n",
       "3           NaN  NaN  NaN  NaN  \n",
       "4           NaN  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "volunteer = pd.read_csv('../_datasets/volunteer_opportunities.csv')\n",
    "\n",
    "# Changing the display to see more (up to 200) columns as it cuts off normally \n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Display\n",
    "volunteer.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables\n",
    "  \n",
    "Because models in scikit-learn require numerical input, if the dataset contains categorical variables, we'll have to encode them. Let's take a look at how to do that.\n",
    "  \n",
    "**Categorical variables**\n",
    "  \n",
    "Often, real-world data contains categorical variables to store values that can only take a finite number of discrete values. For example, here's a set of some user data with categorical values. We have a subscribed column, with binary yes or no values, as well as a column with users' favorite colors, which has multiple categorical values.\n",
    "  \n",
    "**Encoding binary variables - pandas**\n",
    "  \n",
    "The first encoding we'll cover is encoding binary values, like in the column shown. This is actually quite simple, and can be done in both pandas and scikit-learn. In pandas, we can use the `.apply()` method to encode 1s and 0s in a DataFrame column. Using `.apply()`, we can write a conditional that returns a 1 if the value in subscribed is y, and a 0 if the value is n. Looking at a side by side comparison of the columns, we can see that the column is now numerically encoded. pandas could be a good choice if we've not finished preprocessing, or if we're interested in further exploratory work once we've encoded.\n",
    "  \n",
    "**Encoding binary variables - scikit-learn**\n",
    "  \n",
    "We can also encode binary variables in scikit-learn using `LabelEncoder()`. It's useful to know both methods if, for example, we're implementing encoding as part of scikit-learn's pipeline functionality, which allows us to string together different steps of the machine learning workflow. Creating a `LabelEncoder()` object also allows us to reuse this encoding on other data, such as on new data or a test set. To encode values in scikit-learn, we'll need to instantiate the `LabelEncoder()` transformer. We can use the `.fit_transform()` method to both fit the encoder to the data as well as transform the column. Printing out both the subscribed column and the new column, we can see that the y's and n's have been encoded to 1s and 0s.\n",
    "  \n",
    "**One-hot encoding**\n",
    "  \n",
    "One-hot encoding encodes categorical variables into 1s and 0s when there are more than two values to encode. It works by looking at the entire list of unique values in a column, transforming each value into an array, and designating a 1 in the appropriate position to encode that a particular value occurs. For example, in the fav_color column, we have three values: blue, green, and orange. If we were to encode these colors with 0s and 1s based on this list, we would get something like this: blue would have a 1 in the first position followed by two zeros, green would have a one in the second position, and orange would have a one in the last position. \n",
    "  \n",
    "An encoded column would look something like this:\n",
    "  \n",
    "![Alt text](../_images/encoded-col-demo.png)  \n",
    "  \n",
    "We can use the pandas `get_dummies()` function to directly encode categorical values in this way.\n",
    "  \n",
    "![Alt text](../_images/encoded-col-pandas.pd.png)  \n",
    "\n",
    "> NOTE: `from sklearn.preprocess import LabelEncoder`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - binary\n",
    "  \n",
    "Take a look at the hiking dataset. There are several columns here that need encoding before they can be modeled, one of which is the Accessible column. Accessible is a binary feature, so it has two values, Y or N, which need to be encoded into 1's and 0's. Use scikit-learn's `LabelEncoder()` method to perform this transformation.\n",
    "  \n",
    "1. Store `LabelEncoder()` in a variable named enc.\n",
    "  \n",
    "2. Using the encoder's `.fit_transform()` method, encode the hiking dataset's \"Accessible\" column. Call the new column Accessible_enc.\n",
    "  \n",
    "3. Compare the two columns side-by-side to see the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prop_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Park_Name</th>\n",
       "      <th>Length</th>\n",
       "      <th>Difficulty</th>\n",
       "      <th>Other_Details</th>\n",
       "      <th>Accessible</th>\n",
       "      <th>Limited_Access</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B057</td>\n",
       "      <td>Salt Marsh Nature Trail</td>\n",
       "      <td>Enter behind the Salt Marsh Nature Center, loc...</td>\n",
       "      <td>Marine Park</td>\n",
       "      <td>0.8 miles</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;p&gt;The first half of this mile-long trail foll...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B073</td>\n",
       "      <td>Lullwater</td>\n",
       "      <td>Enter Park at Lincoln Road and Ocean Avenue en...</td>\n",
       "      <td>Prospect Park</td>\n",
       "      <td>1.0 mile</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Explore the Lullwater to see how nature thrive...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B073</td>\n",
       "      <td>Midwood</td>\n",
       "      <td>Enter Park at Lincoln Road and Ocean Avenue en...</td>\n",
       "      <td>Prospect Park</td>\n",
       "      <td>0.75 miles</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Step back in time with a walk through Brooklyn...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B073</td>\n",
       "      <td>Peninsula</td>\n",
       "      <td>Enter Park at Lincoln Road and Ocean Avenue en...</td>\n",
       "      <td>Prospect Park</td>\n",
       "      <td>0.5 miles</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Discover how the Peninsula has changed over th...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B073</td>\n",
       "      <td>Waterfall</td>\n",
       "      <td>Enter Park at Lincoln Road and Ocean Avenue en...</td>\n",
       "      <td>Prospect Park</td>\n",
       "      <td>0.5 miles</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Trace the source of the Lake on the Waterfall ...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prop_ID                     Name  \\\n",
       "0    B057  Salt Marsh Nature Trail   \n",
       "1    B073                Lullwater   \n",
       "2    B073                  Midwood   \n",
       "3    B073                Peninsula   \n",
       "4    B073                Waterfall   \n",
       "\n",
       "                                            Location      Park_Name  \\\n",
       "0  Enter behind the Salt Marsh Nature Center, loc...    Marine Park   \n",
       "1  Enter Park at Lincoln Road and Ocean Avenue en...  Prospect Park   \n",
       "2  Enter Park at Lincoln Road and Ocean Avenue en...  Prospect Park   \n",
       "3  Enter Park at Lincoln Road and Ocean Avenue en...  Prospect Park   \n",
       "4  Enter Park at Lincoln Road and Ocean Avenue en...  Prospect Park   \n",
       "\n",
       "       Length Difficulty                                      Other_Details  \\\n",
       "0   0.8 miles       None  <p>The first half of this mile-long trail foll...   \n",
       "1    1.0 mile       Easy  Explore the Lullwater to see how nature thrive...   \n",
       "2  0.75 miles       Easy  Step back in time with a walk through Brooklyn...   \n",
       "3   0.5 miles       Easy  Discover how the Peninsula has changed over th...   \n",
       "4   0.5 miles       Easy  Trace the source of the Lake on the Waterfall ...   \n",
       "\n",
       "  Accessible Limited_Access  lat  lon  \n",
       "0          Y              N  NaN  NaN  \n",
       "1          N              N  NaN  NaN  \n",
       "2          N              N  NaN  NaN  \n",
       "3          N              N  NaN  NaN  \n",
       "4          N              N  NaN  NaN  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiking = pd.read_json('../_datasets/hiking.json')\n",
    "hiking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accessible  Accessible_enc\n",
      "0          Y               1\n",
      "1          N               0\n",
      "2          N               0\n",
      "3          N               0\n",
      "4          N               0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.fit_transform()` is a good way to both fit an encoding and transform the data in a single step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - one-hot\n",
    "  \n",
    "One of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use the `pd.get_dummies()` function to do so.\n",
    "  \n",
    "1. Call `get_dummies()` on the volunteer.category_desc column to create the encoded columns and assign it to category_enc.\n",
    "  \n",
    "2. Print out the `.head()` of the category_enc variable to take a look at the encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Education</th>\n",
       "      <th>Emergency Preparedness</th>\n",
       "      <th>Environment</th>\n",
       "      <th>Health</th>\n",
       "      <th>Helping Neighbors in Need</th>\n",
       "      <th>Strengthening Communities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Education  Emergency Preparedness  Environment  Health  \\\n",
       "0           0                       0            0       0   \n",
       "1           0                       0            0       0   \n",
       "2           0                       0            0       0   \n",
       "3           0                       0            0       0   \n",
       "4           0                       0            1       0   \n",
       "5           0                       0            1       0   \n",
       "6           0                       0            0       0   \n",
       "7           0                       0            0       0   \n",
       "8           0                       0            0       0   \n",
       "9           0                       0            0       1   \n",
       "10          0                       0            0       0   \n",
       "11          1                       0            0       0   \n",
       "12          0                       0            0       0   \n",
       "13          1                       0            0       0   \n",
       "14          0                       0            0       0   \n",
       "\n",
       "    Helping Neighbors in Need  Strengthening Communities  \n",
       "0                           0                          0  \n",
       "1                           0                          1  \n",
       "2                           0                          1  \n",
       "3                           0                          1  \n",
       "4                           0                          0  \n",
       "5                           0                          0  \n",
       "6                           0                          1  \n",
       "7                           1                          0  \n",
       "8                           0                          0  \n",
       "9                           0                          0  \n",
       "10                          0                          1  \n",
       "11                          0                          0  \n",
       "12                          0                          1  \n",
       "13                          0                          0  \n",
       "14                          1                          0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "volunteer = pd.read_csv('../_datasets/volunteer_opportunities.csv')\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "category_enc.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dummies()` is a simple and quick way to encode categorical variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering numerical features\n",
    "  \n",
    "Though we may have a dataset filled with numerical features, they may need a little bit of feature engineering to properly prepare for modeling. In this section, we'll talk about aggregate statistics as well as dates and how engineering numerical features can add value to our model's performance.\n",
    "\n",
    "**Aggregate statistics**\n",
    "  \n",
    "If we have, a collection of features related to a single feature, like temperatures on different days, we may want to take an average or median to use as a feature for modeling instead. A common method of feature engineering is to take an aggregate of a set of numbers to use in place of those features. This can be helpful in reducing the dimensionality of our feature space, or perhaps we simply don't need multiple similar values that are close in distance to each other. \n",
    "  \n",
    "In this dataset of temperatures over the course of three days in four different cities. Rather than using all three days, let's take an average of the three. First, we can subset the columns we want to aggregate over using `.loc[]`. Then, we set the `axis=` parameter to `axis=1` in order for the calculation of the mean for each row, and save the results in the mean column.\n",
    "  \n",
    "**Dates**\n",
    "  \n",
    "Dates and timestamps are another area where we might want to reduce granularity in our dataset. If we're doing time series analysis, we will likely need to keep this granularity to capture underlying trends on different timescales, but if we're running a prediction task, we may need higher-level information like the month, year, or both. Here's a collection of purchase dates. The full date is too granular for the prediction task we want to do, so let's extract the month from each date.\n",
    "  \n",
    "The first thing to do is to convert this date column into a pandas datetime column using the `pd.datetime()` function. This makes extracting the components much easier. Once it's converted, we can use the `dt.month` attribute to extract out the month. There are a lots of other attributes for extracting different components, like day and year, and I encourage you to try these out yourself. We can see that there is now a column of month values ready for modeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating numerical features\n",
    "  \n",
    "A good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times.\n",
    "  \n",
    "1. Use the `.loc[]` method to select all rows and columns to find the `.mean()` of the each columns.\n",
    "  \n",
    "2. Print the `.head()` of the DataFrame to see the mean column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>run1</th>\n",
       "      <th>run2</th>\n",
       "      <th>run3</th>\n",
       "      <th>run4</th>\n",
       "      <th>run5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sue</td>\n",
       "      <td>20.1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>19.6</td>\n",
       "      <td>20.3</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark</td>\n",
       "      <td>16.5</td>\n",
       "      <td>17.1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>17.6</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sean</td>\n",
       "      <td>23.5</td>\n",
       "      <td>25.1</td>\n",
       "      <td>25.2</td>\n",
       "      <td>24.6</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Erin</td>\n",
       "      <td>21.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>20.9</td>\n",
       "      <td>22.1</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jenny</td>\n",
       "      <td>25.8</td>\n",
       "      <td>27.1</td>\n",
       "      <td>26.1</td>\n",
       "      <td>26.7</td>\n",
       "      <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Russell</td>\n",
       "      <td>30.9</td>\n",
       "      <td>29.6</td>\n",
       "      <td>31.4</td>\n",
       "      <td>30.4</td>\n",
       "      <td>29.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  run1  run2  run3  run4  run5\n",
       "0      Sue  20.1  18.5  19.6  20.3  18.3\n",
       "1     Mark  16.5  17.1  16.9  17.6  17.3\n",
       "2     Sean  23.5  25.1  25.2  24.6  23.9\n",
       "3     Erin  21.7  21.1  20.9  22.1  22.2\n",
       "4    Jenny  25.8  27.1  26.1  26.7  26.9\n",
       "5  Russell  30.9  29.6  31.4  30.4  29.9"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_times_5k = pd.read_csv('../_datasets/running_times_5k.csv')\n",
    "running_times_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  run1  run2  run3  run4  run5   mean\n",
      "0    Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
      "1   Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
      "2   Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
      "3   Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
      "4  Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n"
     ]
    }
   ],
   "source": [
    "# Use .loc to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, :].mean(axis=1, numeric_only=True)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.loc[]` is especially helpful for operating across columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting datetime components\n",
    "  \n",
    "There are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the start_date_date column and extract just the month to use as a feature for modeling.\n",
    "  \n",
    "1. Convert the start_date_date column into a pandas datetime column and store it in a new column called start_date_converted.\n",
    "  \n",
    "2. Retrieve the month component of start_date_converted and store it in a new column called start_date_month.\n",
    "  \n",
    "3. Print the `.head()` of just the start_date_converted and start_date_month columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        July 30 2011\n",
       "1    February 01 2011\n",
       "2     January 29 2011\n",
       "3    February 14 2011\n",
       "4    February 05 2011\n",
       "Name: start_date_date, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer.start_date_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date_converted</th>\n",
       "      <th>start_date_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start_date_converted  start_date_month\n",
       "0           2011-07-30                 7\n",
       "1           2011-02-01                 2\n",
       "2           2011-01-29                 1\n",
       "3           2011-02-14                 2\n",
       "4           2011-02-05                 2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "volunteer[['start_date_converted', 'start_date_month']].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use attributes like `.month` to get the month, `.day` to get the day, and `.year` to get the year from datetime columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering text features\n",
    "  \n",
    "Though text data is a little more complicated to work with, there's a lot of useful feature engineering we can do with it.\n",
    "  \n",
    "**Extraction**\n",
    "  \n",
    "One method is to extract the pieces of information that you need: maybe part of a string, or extracting a number, and transforming it into a feature. We can also transform the text itself into features, for use with natural language processing methods or prediction tasks. Let's learn how to extract data from text fields. \n",
    "  \n",
    "We're going to use regular expressions to extract information from strings. Regular expressions are patterns that can be used to extract information from text data. You should already be familiar with regular expression, but for the purposes of this course, we're going to only focus on extracting numbers from strings. To use Python's rich regular expressions functionality, we'll need to first import the re module. Here we have a string\n",
    "  \n",
    "`my_string = 'temperature:75.6 F'`\n",
    "  \n",
    "and we want to extract the temperature digit from it, so we can model using the numerical data. \n",
    "  \n",
    "`temp = re.search('\\d+\\.\\d+', my_string)`\n",
    "\n",
    "We'll need use a pattern to extract this float, so let's break down the pattern in `re.search()`. \"`\\d`\" means that we want to grab digits, and the \"`+`\" means we want to grab as many as possible. So if there are two next to each other, we want both (like the 75). \"`\\.`\" means we want to grab the decimal point, and then there's another \"`\\d+`\" at the end to grab the digits on the right-hand side of the decimal. `re.search` then searches for a string matching the pattern, which we can extract with the `.group()` method.\n",
    "  \n",
    "`print(float(temp.group(0)))`\n",
    "  \n",
    "out: `75.6`\n",
    "  \n",
    "**Vectorizing text**\n",
    "  \n",
    "If we're working with text, we might want to model it in some way. Maybe we want to use document text in a classification task, such as classifying emails as spam or not. In order to do that, we'll need to vectorize the text and transform it into a numerical input that scikit-learn can use. We're going to create a tf/idf vector. tf/idf is a way of vectorizing text that reflects how important a word is in a document beyond how frequently it occurs. It stands for term frequency inverse document frequency and places the weight on words that are ultimately more significant in the entire corpus of words. We can create tf/idf vectors in scikit-learn by using `TfidfVectorizer()`.\n",
    "    \n",
    "`from sklearn.feature_extraction.text import TfidfVectorizer()`  \n",
    "  \n",
    "Here we have a collection of text. In order to vectorize it, we can simply pass the column of text we want to vectorize into the `.fit_transform()` method, which is called on the `TfidfVectorizer()`.\n",
    "  \n",
    "**Text classification**\n",
    "  \n",
    "Now that we have a vectorized version of the text, we can use it for classification. We'll use a Naive Bayes classifier, which is based on Bayes' theorem of conditional probability, seen here, and performs well on text classification tasks. \n",
    "  \n",
    "Naive Bayes treats each feature as independent from the others, which can be a naive assumption, but works out quite well on text data. Because each feature is treated independently, this classifier works well on high-dimensional data and is very efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "  \n",
    "**Defined as**:\n",
    "  \n",
    "$P(A | B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "  \n",
    "where:  \n",
    "  \n",
    "$P(A | B)$ represents the conditional probability of event $A$ given event $B$  \n",
    "$P(A \\cap B)$ denotes the probability of both events $A$ and $B$ occurring simultaneously  \n",
    "$P(B)$ is the probability of event $B$  \n",
    "  \n",
    "Conditional probability is a concept that is used in Bayes' theorem. It is the probability of an event $A$ given that another event $B$ has occurred. Conditional probability is a fundamental concept in probability theory and it provides a way to quantify the likelihood of an event based on additional information or conditions. As so, it is widely used in various statistical and machine learning applications. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "  \n",
    "**The Naive Bayes algorithm is defined as**: \n",
    "  \n",
    "$P(C_k | x_1, x_2, ..., x_n) = \\frac{P(C_k) \\cdot P(x_1, x_2, ..., x_n | C_k)}{P(x_1, x_2, ..., x_n)}$\n",
    "\n",
    "where:\n",
    "\n",
    "$P(C_k)$ is the prior probability of class $C_k$  \n",
    "$P(x_1, x_2, ..., x_n | C_k)$ is the likelihood of observing features $x_1, x_2, ..., x_n$ given class $C_k$  \n",
    "$P(x_1, x_2, ..., x_n)$ is the probability of observing features $x_1, x_2, ..., x_n$  \n",
    "  \n",
    "**Alternatively it can be written as**:  \n",
    "  \n",
    "$P(C | X) = \\frac{P(X | C) \\cdot P(C)}{P(X)}$\n",
    "  \n",
    "where:\n",
    "  \n",
    "$P(C | X)$ represents the posterior probability of class $C$ given evidence $X$  \n",
    "$P(X | C)$ denotes the likelihood of evidence $X$ given class $C$  \n",
    "$P(C)$ is the prior probability of class $C$  \n",
    "$P(X)$ is the probability of evidence $X$  \n",
    "  \n",
    "**In both Naive Bayes formulas**:\n",
    "  \n",
    "$C_k$ represents a specific class or category.  \n",
    "$X$ represents a set of features or evidence.  \n",
    "$P(C_k | x_1, x_2, ..., x_n)$ and $P(C | X)$ denote the posterior probability of class $C_k$ or $C$ given the evidence $x_1, x_2, ..., x_n$ or $X$, respectively.  \n",
    "$P(C_k)$ and $P(C)$ are the prior probabilities of class $C_k$ or $C$, respectively.  \n",
    "$P(x_1, x_2, ..., x_n | C_k)$ and $P(X | C)$ represent the likelihood of observing the evidence $x_1, x_2, ..., x_n$ or $X$ given class $C_k$ or $C$, respectively.  \n",
    "$P(x_1, x_2, ..., x_n)$ and $P(X)$ are the probabilities of observing the evidence $x_1, x_2, ..., x_n$ or $X$, respectively.  \n",
    "  \n",
    "The two Naive Bayes formulas are mathematically equivalent, but they may be written in slightly different notations or symbols. The first formula uses the subscript $k$ to denote different classes, while the second formula uses a single class $C$. Additionally, the first formula explicitly represents the evidence $x_1, x_2, ..., x_n$, while the second formula represents it as a single entity $X$. However, the underlying principle and calculation of conditional probability using Bayes' theorem are the same in both formulas.\n",
    "  \n",
    "Bayes' theorem allows us to update our beliefs or knowledge about the probability of an event based on new evidence. It is widely used in various fields, including statistics, machine learning, and data analysis, particularly in Bayesian inference.\n",
    "  \n",
    "In summary, conditional probability is a concept that describes the likelihood of an event given that another event has occurred, while Bayes' theorem is a mathematical formula that provides a framework for updating probabilities based on new evidence. Bayes' theorem involves conditional probabilities but extends beyond them by incorporating prior knowledge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting string patterns\n",
    "  \n",
    "The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame.\n",
    "  \n",
    "1. Search the text in the length argument for numbers and decimals using an appropriate pattern.\n",
    "  \n",
    "2. Extract the matched pattern and convert it to a float.\n",
    "  \n",
    "3. Apply the `return_mileage()` function to each row in the hiking.Length column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length  Length_num\n",
      "0   0.8 miles        0.80\n",
      "1    1.0 mile        1.00\n",
      "2  0.75 miles        0.75\n",
      "3   0.5 miles        0.50\n",
      "4   0.5 miles        0.50\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "\n",
    "    if length == None:\n",
    "        return\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search('\\d+\\.\\d+', length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group())\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking.Length.apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are a useful way to perform text extraction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing text\n",
    "  \n",
    "You'll now transform the volunteer dataset's title column into a text vector, which you'll use in a prediction task in the next exercise.\n",
    "  \n",
    "1. Store the volunteer.title column in a variable named title_text.\n",
    "  \n",
    "2. Instantiate a `TfidfVectorizer()` as tfidf_vec.\n",
    "  \n",
    "3. Transform the text in title_text into a tf-idf vector using tfidf_vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Need to drop NaN observations in the 'category_desc' for train_test_split\n",
    "volunteer = pd.read_csv('../_datasets/volunteer_opportunities.csv')\n",
    "volunteer = volunteer.dropna(subset=['category_desc'], axis=0)\n",
    "\n",
    "# Taking the title text from the title column\n",
    "title_text = volunteer.title\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using tf/idf vectors\n",
    "  \n",
    "Now that you've encoded the volunteer dataset's title column into tf/idf vectors, you'll use those vectors to predict the category_desc column.\n",
    "  \n",
    "1. Split the text_tfidf vector and y target variable into training and test sets, setting the `stratify=` parameter equal to y, since the class distribution is uneven. Notice that we have to run the `.toarray()` method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "  \n",
    "2. Fit the X_train and y_train data to the Naive Bayes model, nb.\n",
    "  \n",
    "3. Print out the test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 278)\t0.6821380095940299\n",
      "  (0, 1048)\t0.7312234513930028\n",
      "  (1, 832)\t0.4089128467305852\n",
      "  (1, 559)\t0.4089128467305852\n",
      "  (1, 90)\t0.2211952015096988\n",
      "  (1, 890)\t0.3668183240931356\n",
      "  (1, 490)\t0.38428912950191935\n",
      "  (1, 38)\t0.4089128467305852\n",
      "  (1, 1017)\t0.4089128467305852\n",
      "  (2, 680)\t0.20380137329146378\n",
      "  (2, 498)\t0.17806074091111465\n",
      "  (2, 240)\t0.3097133180239295\n",
      "  (2, 27)\t0.3097133180239295\n",
      "  (2, 708)\t0.3097133180239295\n",
      "  (2, 969)\t0.16859216949793618\n",
      "  (2, 535)\t0.26756671781861974\n",
      "  (2, 356)\t0.3097133180239295\n",
      "  (2, 1061)\t0.27783064576035155\n",
      "  (2, 944)\t0.22729780579979794\n",
      "  (2, 68)\t0.1625840364724973\n",
      "  (2, 487)\t0.3097133180239295\n",
      "  (2, 423)\t0.3097133180239295\n",
      "  (2, 368)\t0.3097133180239295\n",
      "  (3, 947)\t0.7071067811865476\n",
      "  (3, 922)\t0.7071067811865476\n",
      "  :\t:\n",
      "  (612, 681)\t0.49600210945097983\n",
      "  (612, 378)\t0.49600210945097983\n",
      "  (612, 773)\t0.38733928023398656\n",
      "  (612, 380)\t0.20302987585336688\n",
      "  (612, 1037)\t0.19533322522098384\n",
      "  (613, 937)\t0.4699182426672902\n",
      "  (613, 522)\t0.4699182426672902\n",
      "  (613, 854)\t0.40597053635215596\n",
      "  (613, 1039)\t0.2721889755783925\n",
      "  (613, 480)\t0.3932463594471568\n",
      "  (613, 691)\t0.40597053635215596\n",
      "  (614, 997)\t0.6767669133759634\n",
      "  (614, 389)\t0.7361973546268497\n",
      "  (615, 39)\t0.7765546136247382\n",
      "  (615, 608)\t0.56068012330996\n",
      "  (615, 1037)\t0.2874034296650989\n",
      "  (616, 1082)\t0.42103566356600525\n",
      "  (616, 617)\t0.42103566356600525\n",
      "  (616, 370)\t0.42103566356600525\n",
      "  (616, 507)\t0.3776931875033005\n",
      "  (616, 684)\t0.33435071144059575\n",
      "  (616, 1060)\t0.26831576383014705\n",
      "  (616, 459)\t0.2476657593151863\n",
      "  (616, 980)\t0.23691996699847181\n",
      "  (616, 1037)\t0.1558256065923858\n"
     ]
    }
   ],
   "source": [
    "# CSR Matrix\n",
    "print(text_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(text_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5161290322580645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Instanciate seed\n",
    "SEED = 42\n",
    "\n",
    "# Instanciate the Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer['category_desc']\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=SEED)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Displaying the models accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model doesn't score very well. We'll work on selecting the best features for modeling in the next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
