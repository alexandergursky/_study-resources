{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "  \n",
    "Now that you've learned all about preprocessing you'll try these techniques out on a dataset that records information on UFO sightings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFOs and preprocessing\n",
    "  \n",
    "Now it's time for you to apply the concepts you've learned throughout this course to a brand new dataset.\n",
    "  \n",
    "**Identifying areas for preprocessing**\n",
    "  \n",
    "The final chapter in this course will walk you through an entire preprocessing workflow on a dataset related to UFO sightings. Each row in this dataset contains information like the location, the type of the sighting, the number of seconds and minutes the sighting lasted, a description of the sighting, and the date the sighting was recorded. As you might imagine, there are a number of preprocessing tasks that need to be done prior to doing any modeling on this dataset.\n",
    "  \n",
    "**Important concepts to remember**\n",
    "  \n",
    "In the very first chapter of this course, we covered things like removing missing data, altering the type of columns in a DataFrame, and creating training and test sets based on class distribution. Some useful pandas functions to remember are `.dropna()` and `.isna()` for missing data, `astype()` for type conversion, and the `stratify=` parameter in the `train_test split()` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking column types\n",
    "  \n",
    "Take a look at the UFO dataset's column types using the `.info()` method. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object, and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on.\n",
    "  \n",
    "1. Call the `.info()` method on the ufo dataset.\n",
    "  \n",
    "2. Convert the type of the seconds column to the float data type.\n",
    "  \n",
    "3. Convert the type of the date column to the datetime data type.\n",
    "  \n",
    "4. Call `.info()` on ufo again to see if the changes worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/3/2011 19:21</td>\n",
       "      <td>woodville</td>\n",
       "      <td>wi</td>\n",
       "      <td>us</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1209600.0</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>Red blinking objects similar to airplanes or s...</td>\n",
       "      <td>12/12/2011</td>\n",
       "      <td>44.9530556</td>\n",
       "      <td>-92.291111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/3/2004 19:05</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>oh</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30sec.</td>\n",
       "      <td>Many fighter jets flying towards UFO</td>\n",
       "      <td>10/27/2004</td>\n",
       "      <td>41.4994444</td>\n",
       "      <td>-81.695556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/25/2009 21:00</td>\n",
       "      <td>coon rapids</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green&amp;#44 red&amp;#44 and blue pulses of light tha...</td>\n",
       "      <td>12/12/2009</td>\n",
       "      <td>45.1200000</td>\n",
       "      <td>-93.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/21/2002 05:45</td>\n",
       "      <td>clemmons</td>\n",
       "      <td>nc</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>300.0</td>\n",
       "      <td>about 5 minutes</td>\n",
       "      <td>It was a large&amp;#44 triangular shaped flying ob...</td>\n",
       "      <td>12/23/2002</td>\n",
       "      <td>36.0213889</td>\n",
       "      <td>-80.382222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/19/2010 12:55</td>\n",
       "      <td>calgary (canada)</td>\n",
       "      <td>ab</td>\n",
       "      <td>ca</td>\n",
       "      <td>oval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>A white spinning disc in the shape of an oval.</td>\n",
       "      <td>8/24/2010</td>\n",
       "      <td>51.083333</td>\n",
       "      <td>-114.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date              city state country      type    seconds  \\\n",
       "0   11/3/2011 19:21         woodville    wi      us   unknown  1209600.0   \n",
       "1   10/3/2004 19:05         cleveland    oh      us    circle       30.0   \n",
       "2   9/25/2009 21:00       coon rapids    mn      us     cigar        0.0   \n",
       "3  11/21/2002 05:45          clemmons    nc      us  triangle      300.0   \n",
       "4   8/19/2010 12:55  calgary (canada)    ab      ca      oval        0.0   \n",
       "\n",
       "    length_of_time                                               desc  \\\n",
       "0          2 weeks  Red blinking objects similar to airplanes or s...   \n",
       "1           30sec.               Many fighter jets flying towards UFO   \n",
       "2              NaN  Green&#44 red&#44 and blue pulses of light tha...   \n",
       "3  about 5 minutes  It was a large&#44 triangular shaped flying ob...   \n",
       "4                2     A white spinning disc in the shape of an oval.   \n",
       "\n",
       "     recorded         lat        long  \n",
       "0  12/12/2011  44.9530556  -92.291111  \n",
       "1  10/27/2004  41.4994444  -81.695556  \n",
       "2  12/12/2009  45.1200000  -93.287500  \n",
       "3  12/23/2002  36.0213889  -80.382222  \n",
       "4   8/24/2010   51.083333 -114.083333  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "ufo = pd.read_csv('../_datasets/ufo_sightings_large.csv')\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            4935 non-null   object \n",
      " 1   city            4926 non-null   object \n",
      " 2   state           4516 non-null   object \n",
      " 3   country         4255 non-null   object \n",
      " 4   type            4776 non-null   object \n",
      " 5   seconds         4935 non-null   float64\n",
      " 6   length_of_time  4792 non-null   object \n",
      " 7   desc            4932 non-null   object \n",
      " 8   recorded        4935 non-null   object \n",
      " 9   lat             4935 non-null   object \n",
      " 10  long            4935 non-null   float64\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 424.2+ KB\n",
      "None \n",
      "\n",
      "seconds           float64\n",
      "date       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the DataFrame info\n",
    "print(ufo.info(), '\\n')\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo['seconds'].astype('float')\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo['date'])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo[['seconds', 'date']].dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice job on transforming the column types! This will make feature engineering and standardization much easier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping missing data\n",
    "  \n",
    "In this exercise, you'll remove some of the rows where certain columns have missing values. You're going to look at the length_of_time column, the state column, and the type column. You'll drop any row that contains a missing value in at least one of these three columns.\n",
    "  \n",
    "1. Print out the number of missing values in the length_of_time, state, and type columns, in that order, using `.isna()` and `.sum()`.\n",
    "  \n",
    "2. Drop rows that have missing values in at least one of these columns.\n",
    "  \n",
    "3. Print out the shape of the new ufo_no_missing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4935, 11)\n",
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n",
      "(4283, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dataframe shape\n",
    "print(ufo.shape)\n",
    "\n",
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[['length_of_time', 'state', 'type']].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing\n",
    "ufo_no_missing = ufo.dropna(subset=['length_of_time', 'state', 'type'], axis=0)\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with this set going forward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables and standardization\n",
    "  \n",
    "The next tasks we're going to tackle are dealing with some of the categorical variables and standardization in the UFO dataset.\n",
    "  \n",
    "**Categorical variables**\n",
    "  \n",
    "Recall that there are a number of categorical variables in the UFO dataset, including location data and the type of the encounter. The following exercises will be about dealing with these categorical variables. There are a number of categorical variables that need to be one hot encoded. Remember that we can one hot encode variables with pandas' `get_dummies()` function.\n",
    "  \n",
    "**Standardization**\n",
    "  \n",
    "In addition, we need to standardize the seconds column. Recall that we can check the variance of a column with the `.var()` method. After we've done that, we can log normalize the column using NumPy's `log()` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting numbers from strings\n",
    "  \n",
    "The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from that text field using regular expressions.\n",
    "  \n",
    "1. Search time_string for numbers using an appropriate RegEx pattern.\n",
    "  \n",
    "2. Use the `.apply()` method to call the `return_minutes()` on every row of the length_of_time column.\n",
    "  \n",
    "3. Print out the `.head()` of both the length_of_time and minutes columns to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "ufo = pd.read_csv('../_datasets/ufo_sample.csv')\n",
    "\n",
    "# Changing the type of seconds to float\n",
    "ufo['seconds'] = ufo['seconds'].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo['date'] = pd.to_datetime(ufo['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   minutes   length_of_time\n",
      "0      5.0  about 5 minutes\n",
      "1     10.0       10 minutes\n",
      "2      2.0        2 minutes\n",
      "3      2.0        2 minutes\n",
      "4      5.0        5 minutes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Creating a function to extract the time as an int from a string\n",
    "def return_minutes(time_string):\n",
    "    \"\"\"\n",
    "    Extracts the time as an integer from a string.\n",
    "\n",
    "    Parameters:\n",
    "    - time_string (str): A string containing time information.\n",
    "\n",
    "    Returns:\n",
    "    - int: The extracted time as an integer, or None if no numbers are found.\n",
    "\n",
    "    Example:\n",
    "    >>> return_minutes(\"10 minutes\")\n",
    "    10\n",
    "    >>> return_minutes(\"1 hour 30 minutes\")\n",
    "    1\n",
    "    \"\"\"\n",
    "\n",
    "    # Search for numbers in time_string using regular expression '\\d+' to extract digits\n",
    "    num = re.search('\\d+', time_string)\n",
    "\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Apply the extraction function to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[['minutes', 'length_of_time']].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minutes information is now in a form where it can be inputted into a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying features for standardization\n",
    "  \n",
    "In this exercise, you'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the seconds and minutes column, you'll see that the variance of the seconds column is extremely high. Because seconds and minutes are related to each other (an issue we'll deal with when we select features for modeling), let's `np.log()` normalize the seconds column.\n",
    "  \n",
    "1. Calculate the variance in the seconds and minutes columns and take a close look at the results.\n",
    "  \n",
    "2. Perform `np.log()` normalization on the seconds column, transforming it into a new column named seconds_log.\n",
    "  \n",
    "3. Print out the variance of the seconds_log column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization of a value $x$ within a range using logarithm is given by:\n",
    "  \n",
    "$normalized(x) = \\frac{{\\log(x - \\text{{min\\_value}} + 1)}}{{\\log(\\text{{max\\_value}} - \\text{{min\\_value}} + 1)}}$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of a dataset is calculated using the formula:\n",
    "  \n",
    "$variance = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2$\n",
    "  \n",
    "Variance is a statistical measure that quantifies the spread or dispersion of a dataset. It provides information about how individual data points deviate from the mean. A high variance indicates that the data points are widely spread out from the mean, while a low variance indicates that the data points are clustered closely around the mean.\n",
    "  \n",
    "Components of the variance equation:\n",
    "  \n",
    "$x_i$: Represents each individual value in the dataset. The subscript $i$ denotes a specific data point in the dataset.  \n",
    "$\\mu$: Represents the mean (average) of the dataset. To calculate the mean, you sum up all the values and divide by the total number of data points.  \n",
    "$N$: Represents the number of data points in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    424087.417474\n",
      "minutes       117.907176\n",
      "dtype: float64\n",
      "1.1223923881183004\n"
     ]
    }
   ],
   "source": [
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[['seconds', 'minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = ufo.seconds.apply(np.log)\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo.seconds_log.var())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to engineer new features in the ufo dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering new features\n",
    "  \n",
    "Now that we've taken care of some of the more straightforward preprocessing tasks, it's time to engineer new features.\n",
    "  \n",
    "**UFO feature engineering**\n",
    "  \n",
    "There are several fields in the UFO dataset that are great candidates for feature engineering. From the date field, we may want to know the month of the sighting. The number of minutes needs to be extracted from the length of time field. And finally, the description field contains a text description of the sighting. \n",
    "  \n",
    "It would be interesting to vectorize that text and see what we can learn from it. Some important code to remember for date extraction is to use attributes like `.dt.month` and `.dt.hour` to get the pieces of the date you need. Regular expressions will help you extract numbers from text, and you can use the `.group()` method to return the results. And finally, scikit-learn and `TfidfVectorizer()` will vectorize text fields."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables\n",
    "  \n",
    "There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You'll do that transformation here, using both binary and one-hot encoding methods.\n",
    "  \n",
    "1. Using `.apply()`, write a conditional `lambda` function that returns a 1 if the value is \"us\", `else` `return` 0.\n",
    "  \n",
    "2. Print out the number of `.unique()` values in the type column.\n",
    "  \n",
    "3. Using `pd.get_dummies()`, create a one-hot encoded set of the type column.\n",
    "  \n",
    "4. Finally, use `pd.concat()` to concatenate the type_set encoded variables to the ufo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>minutes</th>\n",
       "      <th>seconds_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-11-21 05:45:00</td>\n",
       "      <td>clemmons</td>\n",
       "      <td>nc</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>300.0</td>\n",
       "      <td>about 5 minutes</td>\n",
       "      <td>It was a large&amp;#44 triangular shaped flying ob...</td>\n",
       "      <td>12/23/2002</td>\n",
       "      <td>36.021389</td>\n",
       "      <td>-80.382222</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date      city state country      type  seconds  \\\n",
       "0 2002-11-21 05:45:00  clemmons    nc      us  triangle    300.0   \n",
       "\n",
       "    length_of_time                                               desc  \\\n",
       "0  about 5 minutes  It was a large&#44 triangular shaped flying ob...   \n",
       "\n",
       "     recorded        lat       long  minutes  seconds_log  \n",
       "0  12/23/2002  36.021389 -80.382222      5.0     5.703782  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Index(['changing', 'chevron', 'cigar', 'circle', 'cone', 'cross', 'cylinder',\n",
      "       'diamond', 'disk', 'egg', 'fireball', 'flash', 'formation', 'light',\n",
      "       'other', 'oval', 'rectangle', 'sphere', 'teardrop', 'triangle',\n",
      "       'unknown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to encode country (us) values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo.country.apply(lambda x: 1 if x == 'us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo.type.unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo.type)\n",
    "print(type_set.columns)\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue on by extracting date components."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from dates\n",
    "  \n",
    "Another feature engineering task to perform is month and year extraction. Perform this task on the date column of the ufo dataset.\n",
    "  \n",
    "1. Print out the `.head()` of the date column.\n",
    "  \n",
    "2. Retrieve the month attribute of the date column.\n",
    "  \n",
    "3. Retrieve the year attribute of the date column.\n",
    "  \n",
    "4. Take a look at the `.head()` of the date, month, and year columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2002-11-21 05:45:00\n",
      "1   2012-06-16 23:00:00\n",
      "2   2013-06-09 00:00:00\n",
      "3   2013-04-26 23:27:00\n",
      "4   2013-09-13 20:30:00\n",
      "Name: date, dtype: datetime64[ns]\n",
      "                 date  month  year\n",
      "0 2002-11-21 05:45:00     11  2002\n",
      "1 2012-06-16 23:00:00      6  2012\n",
      "2 2013-06-09 00:00:00      6  2013\n",
      "3 2013-04-26 23:27:00      4  2013\n",
      "4 2013-09-13 20:30:00      9  2013\n"
     ]
    }
   ],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo.date.head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].dt.month\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].dt.year\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date', 'month', 'year']].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas series attributes `.dt.month` and `.dt.year` are extremely useful for extraction tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "  \n",
    "Formula:  \n",
    "  \n",
    "$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$\n",
    "  \n",
    "Where:  \n",
    "\n",
    "- $TF(t, d)$: Represents the Term Frequency of term $t$ in document $d$. It measures the frequency of a term within a document, often computed as the count of term $t$ divided by the total number of terms in document $d$.  \n",
    "- $IDF(t)$: Represents the Inverse Document Frequency of term $t$. It measures the rarity of term $t$ across a collection of documents, often computed as the logarithm of the total number of documents divided by the count of documents that contain term $t$.  \n",
    "  \n",
    "TF-IDF combines both the local importance (TF) and the global importance (IDF) of a term to quantify its significance in a specific document within a collection of documents.\n",
    "    \n",
    "---\n",
    "  \n",
    "### Scikit-learn's Implementation of TF-IDF\n",
    "  \n",
    "Imported as:  \n",
    "  \n",
    "`from sklearn.feature_extraction.text import TfidfVectorizer`\n",
    "  \n",
    "Formula:\n",
    "  \n",
    "$\\text{TF-IDF}(t, d) = (\\text{TF}(t, d) + 1) \\times \\log\\left(\\frac{N+1}{\\text{DF}(t) + 1}\\right) + 1$\n",
    "  \n",
    "Where:\n",
    "  \n",
    "- $TF(t, d)$: Represents the Term Frequency of term $t$ in document $d$.  \n",
    "- $DF(t)$: Represents the Document Frequency of term $t$, which is the number of documents in the collection that contain term $t$.  \n",
    "- $N$: Represents the total number of documents in the collection.  \n",
    "- The $+1$ terms in the equation are for smoothing, preventing potential division by zero errors, and avoiding extreme IDF values for terms that appear in all documents.\n",
    "  \n",
    "The equation in scikit-learn's `TfidfVectorizer` incorporates both the logarithmic IDF transformation and the sublinear TF scaling to provide a more balanced and effective representation of TF-IDF for text analysis tasks. You can use this equation to understand the underlying calculation performed by scikit-learn's `TfidfVectorizer` when transforming text data into TF-IDF features.\n",
    "  \n",
    "The `TfidfVectorizer()` class in scikit-learn allows you to preprocess text data and convert it into a matrix where each row represents a document and each column represents a term. The cell values represent the TF-IDF scores for each term in each document. This matrix can then be used as input for machine learning algorithms or other text analysis tasks.\n",
    "  \n",
    "Overall, `TfidfVectorizer()` is a useful tool for transforming text data into a numerical representation that can be used for various NLP tasks, such as document classification, clustering, information retrieval, and more.\n",
    "  \n",
    "**Suppose we have a collection of three documents**:\n",
    "  \n",
    "Document 1: \"I love cats.\"  \n",
    "Document 2: \"I hate dogs.\"  \n",
    "Document 3: \"I have a cat and a dog.\"  \n",
    "  \n",
    "Using `TfidfVectorizer()`, we can transform this collection of documents into a TF-IDF matrix. Here's a sample output:  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>cat</th>\n",
    "    <th>dog</th>\n",
    "    <th>hate</th>\n",
    "    <th>have</th>\n",
    "    <th>love</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Document 1</td>\n",
    "    <td>0.594534</td>\n",
    "    <td>0.000000</td>\n",
    "    <td>0.000000</td>\n",
    "    <td>0.000000</td>\n",
    "    <td>0.80473</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Document 2</td>\n",
    "    <td>0.000000</td>\n",
    "    <td>0.594534</td>\n",
    "    <td>0.80473</td>\n",
    "    <td>0.000000</td>\n",
    "    <td>0.00000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Document 3</td>\n",
    "    <td>0.425441</td>\n",
    "    <td>0.425441</td>\n",
    "    <td>0.000000</td>\n",
    "    <td>0.594534</td>\n",
    "    <td>0.00000</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "In this matrix, each row represents a document, and each column represents a term. The cell values represent the TF-IDF scores. Higher values indicate that a term is more important within a particular document.\n",
    "  \n",
    "For example, in Document 1, the term \"cat\" has a TF-IDF score of 0.594534, while the term \"love\" has a score of 0.80473. The term \"dog\" has a score of 0.0 in Document 1 since it doesn't appear in that document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization\n",
    "  \n",
    "You'll now transform the desc column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field.\n",
    "  \n",
    "1. Print out the `.head()` of the desc column.\n",
    "  \n",
    "2. Instantiate a `TfidfVectorizer()` object.\n",
    "  \n",
    "3. Fit and transform the desc column using vec.\n",
    "  \n",
    "4. Print out the `.shape` of the desc_tfidf vector, to take a look at the number of columns this created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    It was a large&#44 triangular shaped flying ob...\n",
      "1    Dancing lights that would fly around and then ...\n",
      "2    Brilliant orange light or chinese lantern at o...\n",
      "3    Bright red light moving north to north west fr...\n",
      "4    North-east moving south-west. First 7 or so li...\n",
      "Name: desc, dtype: object\n",
      "(1866, 3422)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Take a look at the head of the desc field\n",
    "print(ufo.desc.head())\n",
    "\n",
    "# Instantiate the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform desc using vec\n",
    "desc_tfidf = vec.fit_transform(ufo.desc)\n",
    "\n",
    "# Look at the number of columns and rows\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and modeling\n",
    "  \n",
    "In this final section, you'll select which features to use for modeling and you'll model the processed UFO data in different ways.\n",
    "  \n",
    "**Feature selection and modeling**\n",
    "  \n",
    "We need to do a little bit of feature selection before we model this data. Keep in mind that you want to eliminate redundant features, and there are a couple of candidates for that in this dataset, both in its original form and due to feature engineering. We also have a text vector that we can inspect and eliminate words from. As far as modeling goes, you've had plenty of practice with it, and now you get to see the results of your preprocessing work.\n",
    "  \n",
    "**Final thoughts**\n",
    "  \n",
    "And finally, remember that preprocessing and modeling are often iterative practices, and it might take a few tries to find the ideal feature configuration that improves your model's performance. It also helps to be extremely knowledgeable about the dataset that you're working with, as well as having a good understanding of the model you're trying to build."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the ideal dataset\n",
    "  \n",
    "Now to get rid of some of the unnecessary features in the ufo dataset. Because the country column has been encoded as country_enc, you can select it and drop the other columns related to location: city, country, lat, long, and state.\n",
    "  \n",
    "You've engineered the month and year columns, so you no longer need the date or recorded columns. You also standardized the seconds column as seconds_log, so you can drop seconds and minutes.\n",
    "  \n",
    "You vectorized desc, so it can be removed. For now you'll keep type.\n",
    "  \n",
    "You can also get rid of the length_of_time column, which is unnecessary after extracting minutes.\n",
    "  \n",
    "1. Make a list of all the columns to drop, to_drop.\n",
    "  \n",
    "2. Drop these columns from ufo.\n",
    "  \n",
    "3. Use the `words_to_filter()` function you created previously; pass in vocab, vec`.vocabulary_`, desc_tfidf, and keep the top 4 words as the last parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquired the 2 functions that I made in last exercise to use here\n",
    "\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    \"\"\"\n",
    "    Returns the top weighted words from a vectorized representation of text.\n",
    "\n",
    "    Parameters:\n",
    "    - vocab (list): List of words representing the vocabulary.\n",
    "    - original_vocab (dict): Mapping of index to original word from the vectorizer.\n",
    "    - vector (scipy.sparse.csr_matrix): Vectorized representation of text.\n",
    "    - vector_index (int): Index of the vector in the vector matrix.\n",
    "    - top_n (int): Number of top weighted words to return.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of top weighted words.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary of word indices and their corresponding weights\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform the zipped dictionary into a pandas Series with words as indices\n",
    "    zipped_series = pd.Series({vocab[i]: zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to retrieve the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    \n",
    "    # Retrieve the original words corresponding to the indices\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    \"\"\"\n",
    "    Returns a set of word indices to filter based on the top weighted words in a vectorized representation of text.\n",
    "\n",
    "    Parameters:\n",
    "    - vocab (list): List of words representing the vocabulary.\n",
    "    - original_vocab (dict): Mapping of index to original word from the vectorizer.\n",
    "    - vector (scipy.sparse.csr_matrix): Vectorized representation of text.\n",
    "    - top_n (int): Number of top weighted words to consider for filtering.\n",
    "\n",
    "    Returns:\n",
    "    - set: Set of word indices to filter.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set to remove duplicate word indices\n",
    "    return set(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required vocab\n",
    "vocab_csv = pd.read_csv('../_datasets/vocab_ufo.csv', index_col=0).to_dict()\n",
    "vocab = vocab_csv['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              seconds  seconds_log   minutes\n",
      "seconds      1.000000     0.853371  0.980944\n",
      "seconds_log  0.853371     1.000000  0.825924\n",
      "minutes      0.980944     0.825924  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "print(ufo[['seconds', 'seconds_log', 'minutes']].corr())\n",
    "\n",
    "# Make a list of features to drop\n",
    "to_drop = ['city', 'country', 'date', 'desc', 'lat', \n",
    "           'length_of_time', 'seconds', 'minutes', 'long', 'state', 'recorded']\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, top_n=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're almost done. In the next exercises, you'll model the UFO data in a couple of different ways."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 1\n",
    "  \n",
    "In this exercise, you're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. The X dataset contains the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The y labels are the encoded country column, where 1 is \"us\" and 0 is \"ca\".\n",
    "  \n",
    "1. Print out the `.columns` of the X set.\n",
    "  \n",
    "2. Split the X and y sets, ensuring that the class distribution of the labels is the same in the training and tests sets, and using a `random_state=` of 42.\n",
    "  \n",
    "3. Fit knn to the training data.\n",
    "  \n",
    "4. Print the test set accuracy of the knn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>seconds_log</th>\n",
       "      <th>country_enc</th>\n",
       "      <th>changing</th>\n",
       "      <th>chevron</th>\n",
       "      <th>cigar</th>\n",
       "      <th>circle</th>\n",
       "      <th>cone</th>\n",
       "      <th>cross</th>\n",
       "      <th>cylinder</th>\n",
       "      <th>diamond</th>\n",
       "      <th>disk</th>\n",
       "      <th>egg</th>\n",
       "      <th>fireball</th>\n",
       "      <th>flash</th>\n",
       "      <th>formation</th>\n",
       "      <th>light</th>\n",
       "      <th>other</th>\n",
       "      <th>oval</th>\n",
       "      <th>rectangle</th>\n",
       "      <th>sphere</th>\n",
       "      <th>teardrop</th>\n",
       "      <th>triangle</th>\n",
       "      <th>unknown</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>triangle</td>\n",
       "      <td>5.703782</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>light</td>\n",
       "      <td>6.396930</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>light</td>\n",
       "      <td>4.787492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>light</td>\n",
       "      <td>4.787492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sphere</td>\n",
       "      <td>5.703782</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       type  seconds_log  country_enc  changing  chevron  cigar  circle  cone  \\\n",
       "0  triangle     5.703782            1         0        0      0       0     0   \n",
       "1     light     6.396930            1         0        0      0       0     0   \n",
       "2     light     4.787492            0         0        0      0       0     0   \n",
       "3     light     4.787492            1         0        0      0       0     0   \n",
       "4    sphere     5.703782            1         0        0      0       0     0   \n",
       "\n",
       "   cross  cylinder  diamond  disk  egg  fireball  flash  formation  light  \\\n",
       "0      0         0        0     0    0         0      0          0      0   \n",
       "1      0         0        0     0    0         0      0          0      1   \n",
       "2      0         0        0     0    0         0      0          0      1   \n",
       "3      0         0        0     0    0         0      0          0      1   \n",
       "4      0         0        0     0    0         0      0          0      0   \n",
       "\n",
       "   other  oval  rectangle  sphere  teardrop  triangle  unknown  month  year  \n",
       "0      0     0          0       0         0         1        0     11  2002  \n",
       "1      0     0          0       0         0         0        0      6  2012  \n",
       "2      0     0          0       0         0         0        0      6  2013  \n",
       "3      0     0          0       0         0         0        0      4  2013  \n",
       "4      0     0          0       1         0         0        0      9  2013  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the display to see more (up to 50) columns as it cuts off normally \n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# Display\n",
    "ufo_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X/y split, dropping 'type' because we OHEncoded it already, 'country_enc' = target\n",
    "X = ufo_dropped.drop(['type', 'country_enc'], axis=1)\n",
    "y = ufo_dropped['country_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone',\n",
      "       'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash',\n",
      "       'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
      "       'teardrop', 'triangle', 'unknown', 'month', 'year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.fit_transform()` when scaling the training features.  \n",
    "Use `.transform()` when scaling the test features.  \n",
    "Use `.fit()` when fitting the model to the scaled training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867237687366167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Seeding\n",
    "SEED = 42\n",
    "\n",
    "# Instanciating the model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=SEED)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy score of knn on the test sets\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs pretty well (at 86.72% accuracy). It seems like you've made pretty good feature selection choices here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 2\n",
    "  \n",
    "Finally, you'll build a model using the text vector we created, desc_tfidf, using the filtered_words list to create a filtered text vector. Let's see if you can predict the type of the sighting based on the text. You'll use a Naive Bayes model for this.\n",
    "  \n",
    "1. Filter the desc_tfidf vector by passing a list of filtered_words into the index.\n",
    "  \n",
    "2. Split the filtered_text features and y, ensuring an equal class distribution in the training and test sets; use a `random_state=` of 42.\n",
    "  \n",
    "3. Use the nb model's `.fit()` to fit X_train and y_train.\n",
    "  \n",
    "4. Print out the `.score()` of the nb model on the X_test and y_test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise ask to predict type of sighting based on the vocab (description, ufo.desc)\n",
    "y = ufo_dropped['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17987152034261242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Seeding\n",
    "SEED = 42\n",
    "\n",
    "# Instanciate model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    filtered_text.toarray(),\n",
    "    y,\n",
    "    stratify=y,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model performs very poorly on this text data (at 17.99% accuracy). This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've learned valuable skills for preparing your data for modeling. You now know how to deal with missing data and incorrect types, how to standardize numerical values and process categorical ones, how to engineer new features that will improve your dataset, and finally, how to select features for modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
