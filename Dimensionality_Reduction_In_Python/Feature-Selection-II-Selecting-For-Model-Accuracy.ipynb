{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection II - Selecting for Model Accuracy  \n",
    "In this second chapter on feature selection, you'll learn how to let models help you find the most important features in a dataset for predicting a particular target feature. In the final lesson of this chapter, you'll combine the advice of multiple, different, models to decide on which features are worth keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features for model performance  \n",
    "  \n",
    "**Selecting features for model performance**  \n",
    "  \n",
    "In previous chapters, we've always looked at individual or pairwise properties of features to decide on whether we keep them in the dataset or not. Another, more pragmatic, approach is to select features based on how they affect model performance.  \n",
    "  \n",
    "**Ansur dataset sample**\n",
    "  \n",
    "Consider this sample of the ANSUR dataset with one target variable, \"Gender\" which we'll try to predict, and five body measurement features to do so.  \n",
    "  \n",
    "**Pre-processing the data**\n",
    "  \n",
    "To train a model on this data we should first perform a train - test split, and in this case also standardize the training feature dataset X_train to have a mean of zero and a variance of one. Notice that we've used the `.fit_transform()` method to fit the scaler and transform the data in one command.\n",
    "  \n",
    "**Creating a logistic regression model**\n",
    "  \n",
    "We can then create and fit a logistic regression model on this standardized training data. To see how the model performs on the test set we first scale these features with the `.transform()` method of the scaler that we just fit on the training set and then make our prediction. we get a test-set accuracy of 99%.  \n",
    "  \n",
    "**Inspecting the feature coefficients**  \n",
    "  \n",
    "However, when we look at the feature coefficients that the logistic regression model uses in its decision function, we'll see that some values are pretty close to zero. Since these coefficients will be multiplied with the feature values when the model makes a prediction, features with coefficients close to zero will contribute little to the end result. We can use the `zip()` function to transform the output into a dictionary that shows which feature has which coefficient. If we want to remove a feature from the initial dataset with as little effect on the predictions as possible, we should pick the one with the lowest coefficient, \"handlength\" in this case. The fact that we standardized the data first makes sure that we can compare the coefficients to one another.  \n",
    "  \n",
    "**Features that contribute little to a model**  \n",
    "  \n",
    "When we remove the \"handlength\" feature at the start of the process, our model accuracy remains unchanged at 99% while we did reduce our dataset's complexity. We could repeat this process until we have the desired number of features remaining, but it turns out, there's a Scikit-learn function that does just that.  \n",
    "  \n",
    "**Recursive Feature Elimination**  \n",
    "  \n",
    "`from sklearn.feature_selection import RFE`  \n",
    "`ref = RFE(estimator= LogisticRegression(), n_features_to_select= 2, verbose= 1)`  \n",
    "RFE for \"Recursive Feature Elimination\" is a feature selection algorithm that can be wrapped around any model that produces feature coefficients or feature importance values. We can pass it the model we want to use and the number of features we want to select. While fitting to our data it will repeat a process where it first fits the internal model and then drops the feature with the weakest coefficient. It will keep doing this until the desired number of features is reached. If we set RFE's `verbose=` parameter higher than zero we'll be able to see that features are dropped one by one while fitting. We could also decide to just keep the 2 features with the highest coefficients after fitting the model once, but this recursive method is safer, since dropping one feature will cause the other coefficients to change.  \n",
    "  \n",
    "**Inspecting the RFE results**  \n",
    "  \n",
    "`X.columns[rfe.support_]`  \n",
    "`print(dict(zip(X.columns, rfe.ranking_)))`  \n",
    "`print(accuracy_score(y_test, rfe.predict(X_test_std)))`  \n",
    "Once RFE is done we can check the `.support_` attribute that contains True/False values to see which features were kept in the dataset. Using the `zip()` function once more, we can also check out rfe's `.ranking_` attribute to see in which iteration a feature was dropped. Values of 1 mean that the feature was kept in the dataset until the end while high values mean the feature was dropped early on. Finally, we can test the accuracy of the model with just two remaining features, 'chestdepth' and 'neckcircumference', turns out the accuracy is still untouched at 99%. This means the other 3 features had little to no impact on the model an its predictions.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: `pprint` is a Python module that provides a `pprint()` function (pretty-print) for printing complex data structures like dictionaries, lists, and tuples in a more readable and organized way. The `pprint()` function prints the data in a formatted and indented way that makes it easier to read and understand, especially for nested data structures with multiple levels of indentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a diabetes classifier  \n",
    "  \n",
    "You'll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>triceps</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>family</th>\n",
       "      <th>age</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  diastolic  triceps  insulin   bmi  family  age      test\n",
       "0         1       89         66       23       94  28.1   0.167   21  negative\n",
       "1         0      137         40       35      168  43.1   2.288   33  positive\n",
       "2         3       78         50       32       88  31.0   0.248   26  positive\n",
       "3         2      197         70       45      543  30.5   0.158   53  positive\n",
       "4         1      189         60       23      846  30.1   0.398   59  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "diabetes_df = pd.read_csv('../_datasets/PimaIndians.csv')\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# X/y split\n",
    "X, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Creating the scaler and the ML instance\n",
    "scaler = StandardScaler()\n",
    "lr = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.966102% accuracy on test set.\n",
      "{'age': 0.44,\n",
      " 'bmi': 0.4,\n",
      " 'diastolic': 0.04,\n",
      " 'family': 0.25,\n",
      " 'glucose': 1.01,\n",
      " 'insulin': 0.17,\n",
      " 'pregnant': 0.26,\n",
      " 'triceps': 0.11}\n"
     ]
    }
   ],
   "source": [
    "# Fit the scaler on the training features and transform them in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scaler the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Print accuracy metrics and feature coefficients\n",
    "print('{0:1%} accuracy on test set.'.format(accuracy_score(y_test, y_pred)))\n",
    "pprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get almost 80% accuracy on the test set. Take a look at the differences in model coefficients for the different features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Recursive Feature Elimination  \n",
    "  \n",
    "Now that we've created a diabetes classifier, let's see if we can reduce the number of features without hurting the model accuracy too much.  \n",
    "  \n",
    "On the second line of code the features are selected from the original dataframe. Adjust this selection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79.6% accuracy on test set.\n",
      "{'age': 0.34,\n",
      " 'bmi': 0.38,\n",
      " 'diastolic': 0.03,\n",
      " 'family': 0.35,\n",
      " 'glucose': 1.23,\n",
      " 'insulin': 0.19,\n",
      " 'pregnant': 0.05,\n",
      " 'triceps': 0.24}\n"
     ]
    }
   ],
   "source": [
    "# Run to see what is the lowest coefficient\n",
    "X = diabetes_df[[\n",
    "    'pregnant', 'glucose', 'triceps', 'diastolic', \n",
    "    'insulin', 'bmi', 'family', 'age'\n",
    "]]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculate the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print('{0: .1%} accuracy on test set.'.format(acc))\n",
    "pprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80.6% accuracy on test set.\n",
      "{'age': 0.35,\n",
      " 'bmi': 0.39,\n",
      " 'family': 0.34,\n",
      " 'glucose': 1.24,\n",
      " 'insulin': 0.2,\n",
      " 'pregnant': 0.05,\n",
      " 'triceps': 0.24}\n"
     ]
    }
   ],
   "source": [
    "# Remove the feature with the lowest model coefficient ('diastolic')\n",
    "X = diabetes_df[[\n",
    "    'pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age'\n",
    "]]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculate the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print('{0: .1%} accuracy on test set.'.format(acc))\n",
    "pprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6% accuracy on test set.\n",
      "{'age': 0.37, 'bmi': 0.34, 'family': 0.34, 'glucose': 1.13, 'triceps': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# Remove the 2 features with the lowest model coefficients ('insulin', 'pregnant')\n",
    "X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "pprint(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.5% accuracy on test set.\n",
      "{'glucose': 1.28}\n"
     ]
    }
   ],
   "source": [
    "# Only keep the feature with the highest coefficient\n",
    "X = diabetes_df[['glucose']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model to the data\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all but one feature only reduced the accuracy by a few percent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Recursive Feature Elimination  \n",
    "  \n",
    "Now let's automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X/y split\n",
    "X, y = diabetes_df.iloc[:, :-1], diabetes_df.iloc[:, -1]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Creating LogisticRegression instance\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fitting the scaler on the training features and transforming them in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scalar the test features\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "{'pregnant': 3, 'glucose': 1, 'diastolic': 6, 'triceps': 4, 'insulin': 5, 'bmi': 1, 'family': 2, 'age': 1}\n",
      "Index(['glucose', 'bmi', 'age'], dtype='object')\n",
      "78.0% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fit the eliminator to the data\n",
    "rfe.fit(X_train_std, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not elimiated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# CAlculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test_std))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we eliminate all but the 3 most relevant features we get a 83.1% accuracy on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based feature selection  \n",
    "  \n",
    "**Tree-based feature selection**  \n",
    "  \n",
    "Some models will perform feature selection by design to avoid overfitting.  \n",
    "  \n",
    "**Random forest classifier**  \n",
    "  \n",
    "`from sklearn.ensemble import RandomForestClassifier`  \n",
    "`from sklearn.metrics import accuracy_score`  \n",
    "`rf = RandomForestClassifier()`  \n",
    "`rf.fit(X_train, y_train)`  \n",
    "`print(accuracy_score(y_test, rf.predict(X_test)))`  \n",
    "One of those, is the random forest classifier. It's an ensemble model that will pass different, random, subsets of features to a number of decision trees. To make a prediction it will aggregate over the predictions of the individual trees. The example forest shown here contains four decision trees. While simple in design, random forests often manage to be highly accurate and avoid overfitting even with the default Scikit-learn settings. If we train a random forest classifier on the 93 numeric features of the ANSUR dataset to predict gender, its test set accuracy is 99%. This means it managed to escape the curse of dimensionality and didn't overfit on the many features in the training set.  \n",
    "  \n",
    "![Alt text](../_images/rfc.png)  \n",
    "  \n",
    "In this illustration of what the trained model could look like, the first decision tree in the forest used the neck circumference feature on its first decision node and hand length later on to determine if a person was male or female. By averaging how often features are used to make decisions inside the different decision trees, and taking into account whether these are important decisions near the root of the tree or less important decisions in the smaller branches of the tree, the random forest algorithm manages to calculate feature importance values.  \n",
    "  \n",
    "**Feature importance values**  \n",
    "  \n",
    "`rf = RandomForestClassifier()`  \n",
    "`rf.fit(X_train, y_train)`  \n",
    "`print(rf.feature_importances_)`  \n",
    "`print(sum(rf.feature_importances_))`  \n",
    "These values can be extracted from a trained model using the `feature_importances_` attribute. Just like the coefficients produced by the logistic regressor, these feature importance values can be used to perform feature selection, since for unimportant features they will be close to zero. An advantage of these feature importance values over coefficients is that they are comparable between features by default, since they always sum up to one. Which means we don't have to scale our input data first. We can use the feature importance values to create a True/False mask for features that meet a certain importance threshold. Then, we can apply that mask to our feature DataFrame to implement the actual feature selection.  \n",
    "  \n",
    "**RFE with random forests**  \n",
    "  \n",
    "`mask = rf.feature_importances_ > 0.1`  \n",
    "`print(mask)`  \n",
    "`X_reduced = X.loc[:, mask]`  \n",
    "`print(X_reduced.columns)`  \n",
    "Remember dropping one weak feature can make other features relatively more or less important. If you want to play safe and minimize the risk of dropping the wrong features, you should not drop all least important features at once but rather one by one. To do so we can once again wrap a Recursive Feature Eliminator, or `RFE()`, around our model. Here, we've reduced the number of features to six with no reduction in test set accuracy. However, training the model once for each feature we want to drop can result in too much computational overhead.  \n",
    "  \n",
    "`from sklearn.feature_selection import RFE`  \n",
    "`rfe = RFE(estimator= RandomForestClassifier(), n_features_to_select= 6, step= 10, verbose= 1)`  \n",
    "`rfe.fit(X_train, y_train)`  \n",
    "`print(accuracy_score(y_test, rfe.predict(X_test)))`  \n",
    "To speed up the process we can pass the `step=` parameter to `RFE()`. Here, we've set it to 10 so that on each iteration the 10 least important features are dropped. Once the final model is trained, we can use the feature eliminator's `.support_` attribute as a mask to print the remaining column names.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a random forest model  \n",
    "  \n",
    "You'll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You'll fit the model on the training data after performing the train-test split and consult the feature importance values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0.13,\n",
      " 'bmi': 0.12,\n",
      " 'diastolic': 0.09,\n",
      " 'family': 0.12,\n",
      " 'glucose': 0.25,\n",
      " 'insulin': 0.14,\n",
      " 'pregnant': 0.07,\n",
      " 'triceps': 0.09}\n",
      "79.6% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "pprint(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model gets 79.6% accuracy on the test set and 'glucose' is the most important feature (0.25)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest for feature selection\n",
    "\n",
    "Now lets use the fitted random model to select the most important features from our input dataset X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False False False]\n",
      "Index(['glucose'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Creating a mask for feature importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Displaying the mask\n",
    "print(mask)\n",
    "\n",
    "# Applying the mask to the feature dataset, X\n",
    "reduced_X = X.loc[:, mask]\n",
    "\n",
    "# Display selected column names\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the feature 'glucose' was sufficiently important."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination with random forests\n",
    "  \n",
    "You'll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Index(['glucose', 'insulin'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Wrap the feature eliminator around the random forest model\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 4 features.\n",
      "Index(['glucose', 'bmi'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Wrap the feature eliminator around the random forest model\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Compared to the quick and dirty single threshold method from the previous exercise one of the selected features is different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "  \n",
    "So far, we focused on how to reduce dimensionality using classification algorithms. Let's see what we can do with regressions.\n",
    "  \n",
    "**Linear model concept**  \n",
    "  \n",
    "To refresh how linear regressions work, we'll build a model that derives the linear function between three input values and a target. However, we'll be creating the feature dataset and linear function ourselves, so that we can control the ground truth that our model tries to derive.\n",
    "  \n",
    "**Creating our own dataset**  \n",
    "  \n",
    "We create three features x1, x2, and x3 that all follow a simple normal distribution. We can then create our own target y with a function of our choice. Let's say that y=20+5x1+2x2+0x3+error (and an error term). The 20 at the start is called the intercept; 5, 2 and 0 are the coefficients of our features, they determine how big an effect each has on the target. The third feature has a coefficient of zero and will therefore have no effect on the target whatsoever. It would be best to remove it from the dataset, since it could confuse a model and make it overfit. Now that we've set the ground truth for this dataset, let's see if a model can derive it.\n",
    "  \n",
    "**Linear regression in Python**  \n",
    "  \n",
    "When you fit a `LinearRegression()` model with Scikit-Learn, the model object will have `.coef_` for coefficient attribute that contains a NumPy array with a number of elements equal to the number of input features. These are the three values we just set to 5, 2, and 0 and the model was able to estimate them pretty accurately. Same goes for the intercept. To check how accurate the model's predictions are we can calculate the R-squared value on the test set. This tells us how much of the variance in the target feature our model can predict. Our model scores an impressive 97.6%. However, the third feature, which had no effect whatsoever, was estimated to have a small effect of -0.05. If there would be more of these irrelevant features, the model could overfit. To solve this, we'll have to look at what the model actually does while training.  \n",
    "```\n",
    "from sklearn.linear_model import LinearRegression()\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#Actual coefficients = [5 2 0]\n",
    "#Actual intercept = 20\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)\n",
    "\n",
    "#Terminal\n",
    "[4.95 1.83 -0.05]\n",
    "19.8\n",
    "\n",
    "#R-squared\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "#Terminal\n",
    "0.976\n",
    "```\n",
    "\n",
    "  \n",
    "**Loss function: Mean Squared Error**\n",
    "  \n",
    "The model will try to find optimal values for the intercept and coefficients by minimizing a loss function. This function contains the mean sum of the squared differences between actual and predicted values, the gray squares in the plot. Minimizing this MSE makes the model as accurate a possible. However, we don't want our model to be super accurate on the training set if that means it no longer generalizes to new data.  \n",
    "  \n",
    "![Alt text](../_images/mse.png)  \n",
    "  \n",
    "  \n",
    "**Adding regularization**  \n",
    "  \n",
    "To avoid this we can introduce regularization. The model will then not only try to be as accurate as possible by minimizing the MSE, it will also try to keep the model simple by keeping the coefficients low. The strength of regularization can be tweaked with alpha, when it's too low the model might overfit, when it's too high the model might become too simple and inaccurate. One linear model that includes this type of regularization is called Lasso, for least absolute shrinkage and selection operator.\n",
    "  \n",
    "![Alt text](../_images/mse-reg.png)  \n",
    "  \n",
    "**Lasso regressor**  \n",
    "  \n",
    "Lasso: least absolute shrinkage and selection operator.  \n",
    "  \n",
    "When we fit it on our dataset we see that it indeed reduced the coefficient of the third feature to zero, ignoring it, but also that it reduced the other coefficients resulting in a lower R squared.\n",
    "To avoid this we can change the alpha parameter. When we set it to 0.05 the third feature is still ignored but the other coefficients are reduced less and our R squared is up again.\n",
    "  \n",
    "```\n",
    "from sklearn.linear_model import Lasso\n",
    "la = Lasso()\n",
    "la.fit(X_train, y_train)\n",
    "\n",
    "#Actual coefficients = [5 2 0]\n",
    "print(la.coef_)\n",
    "\n",
    "#Terminal\n",
    "[4.07 0.59 0. ]\n",
    "\n",
    "#R-squared\n",
    "print(la.score(X_test, y_test))\n",
    "\n",
    "#Terminal\n",
    "0.861\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "la = Lasso(alpha=0.05)\n",
    "la.fit(X_train, y_train)\n",
    "\n",
    "#Actual coefficients = [5 2 0]\n",
    "print(la.coef_)\n",
    "\n",
    "#Terminal\n",
    "[4.91 1.76 0. ]\n",
    "\n",
    "#R-squared\n",
    "print(la.score(X_test, y_test))\n",
    "\n",
    "#Terminal\n",
    "0.974\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a LASSO regressor\n",
    "\n",
    "You'll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the `Lasso()` regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge.\n",
    "  \n",
    "You'll standardize the data first using the `StandardScaler()` that has been instantiated for you as scaler to make sure all coefficients face a comparable regularizing force trying to bring them down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "ansur_df = pd.read_csv('../_datasets/ANSUR_II_MALE.csv')\n",
    "\n",
    "# Unused columns in the dataset/illustration\n",
    "unused = ['Gender', 'Branch', 'Component', 'BMI_class', 'Height_class', 'weight_kg', 'stature_m']\n",
    "\n",
    "# Drop the non-numeric columns from df\n",
    "ansur_df = ansur_df.drop(unused, axis=1)\n",
    "\n",
    "# X/y split\n",
    "X, y = ansur_df.drop('BMI', axis=1), ansur_df['BMI']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Lasso()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Lasso()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Creating the Lasso model\n",
    "la = Lasso()\n",
    "\n",
    "# Fit the model to the standardized training data\n",
    "la.fit(X_train_std, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've fitted the Lasso model to the standardized training data. Now let's look at the results!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso model results\n",
    "Now that you've trained the Lasso model, you'll score its predictive capacity (R^2) on the test set and count how many features are ignored because their coefficient is reduced to zero.\n",
    "  \n",
    "1. Transform the test set with the pre-fitted scaler.\n",
    "2. Calculate the R<sup>2</sup> value on the scaled test data.\n",
    "3. Create a list that has True values when coefficients equal 0.\n",
    "4. Calculate the total number of features with a coefficient of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 84.7% of the variance in the test set (R^2).\n",
      "The model has ignored 82 out of 91 features.\n"
     ]
    }
   ],
   "source": [
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set (R^2).\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict almost 85% of the variance in the BMI value using just 9 out of 91 of the features. The R<sup>2</sup> could be higher though."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the regularization strength\n",
    "The current Lasso model has an R<sup>2</sup> score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power.\n",
    "  \n",
    "Let's improve the balance between predictive power and model simplicity by tweaking the alpha parameter.\n",
    "  \n",
    "Find the highest value for alpha that gives an R<sup>2</sup> value above 98% from the options: 1, 0.5, 0.1, and 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can predict 84.7% of the variance in the test set\n",
      "82 out of 91 features were ignored\n",
      "The model can predict 93.8% of the variance in the test set\n",
      "79 out of 91 features were ignored\n",
      "The model can predict 98.3% of the variance in the test set\n",
      "64 out of 91 features were ignored\n",
      "Max R-squared: 0.9828, alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "# List of alphas\n",
    "alpha_list = [1, 0.5, 0.1, 0.01]\n",
    "\n",
    "# Placeholder values\n",
    "max_r = 0\n",
    "max_alpha = 0\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    # Find the highest alpha value with R-squared above 98%\n",
    "    la = Lasso(alpha=alpha, random_state=0)\n",
    "\n",
    "    # Fitting the model and calculating performance stats recursively\n",
    "    la.fit(X_train_std, y_train)\n",
    "    r_squared = la.score(X_test_std, y_test)\n",
    "    n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "    # Print performance stats\n",
    "    print('The model can predict {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "    print('{} out of {} features were ignored'.format(n_ignored_features, len(la.coef_)))\n",
    "    if r_squared > 0.98:\n",
    "        max_r = r_squared\n",
    "        max_alpha = alpha\n",
    "        break\n",
    "\n",
    "# Display\n",
    "print('Max R-squared: {:.4}, alpha: {}'.format(max_r, max_alpha))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this more appropriate regularization strength we can predict 98% of the variance in the BMI value while ignoring 2/3 of the features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining feature selectors\n",
    "  \n",
    "In the previous lesson we saw how Lasso models allow you to tweak the strength of regularization with the alpha parameter.\n",
    "\n",
    "**Lasso regressor**\n",
    "  \n",
    "We manually set this alpha parameter to find a balance between removing as much features as possible and model accuracy. However, manually finding a good alpha value can be tedious. Good news is, there is a way to automate this.\n",
    "  \n",
    "**LassoCV regressor**\n",
    "  \n",
    "The `LassoCV()` class will use cross validation to try out different alpha settings and select the best one. When we fit this model to our training data it will get an `.alpha_` attribute with the optimal value.\n",
    "LassoCV regressor\n",
    "  \n",
    "To actually remove the features to which the Lasso regressor assigned a zero coefficient, we once again create a mask with True values for all non-zero coefficients. We can then apply it to our feature dataset X with the `.loc` method.\n",
    "\n",
    "**Taking a step back**\n",
    "  \n",
    "Two lessons ago we talked about random forests, they're an example of an ensemble model. It's designed on the idea that a lot of weak predictors can combine to form a strong one. When we use models to perform feature selection we could apply the same idea. Instead of trusting a single model to tell us which features are important we could have multiple models each cast their vote on whether we should keep a feature or not. We could then combine the votes to make a decision.\n",
    "  \n",
    "**Feature selection with LassoCV**\n",
    "  \n",
    "To do so lets first train the models one by one. We'll be predicting BMI in the ANSUR dataset just like you did in the last exercises. If we use `LassoCV()` we'll get an R squared of 99% and when we create a mask that tells us whether a feature has a coefficient different from 0 we can see that this is the case for 66 out of 91 features. We'll put this lcv_mask to the side for a moment and move on to the next model.\n",
    "\n",
    "**Feature selection with random forest**\n",
    "  \n",
    "The second model we train is a random forest regressor model. We've wrapped a Recursive Feature Selector or `RFE()`, around the model to have it select the same number of features as the `LassoCV()` regressor did. We can then use the `.support_` attribute of the fitted model to create rf_mask.\n",
    "  \n",
    "**Feature selection with gradient boosting**\n",
    "  \n",
    "Then, we do the same thing with a gradient boosting regressor. Like random forests gradient boosting is an ensemble method that will calculate feature importance values. The trained model too has a `.support_` attribute that we can use to create gb_mask.\n",
    "\n",
    "**Combining the feature selectors**\n",
    "  \n",
    "Finally, we can start counting the votes on whether to select a feature. We use NumPy's `sum()` function, pass it the three masks in a list, and set the axis argument to 0. We'll then get an array with the number of votes that each feature got. What we do with this vote then depends on how conservative we want to be. If we want to make sure we don't lose any information, we could select all features with at least one vote. In this example we chose to have at least two models voting for a feature in order to keep it. All that is left now is to actually implement the dimensionality reduction. We do that with the `.loc[]` method of our feature DataFrame X."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a LassoCV regressor\n",
    "\n",
    "You'll be predicting biceps circumference on a subsample of the male ANSUR dataset using the `LassoCV()` regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the X/y split\n",
    "X = ansur_df[['acromialheight', 'axillaheight', 'bideltoidbreadth', 'buttockcircumference', 'buttockkneelength', 'buttockpopliteallength', 'cervicaleheight', 'chestcircumference', 'chestheight',\n",
    "       'earprotrusion', 'footbreadthhorizontal', 'forearmcircumferenceflexed', 'handlength', 'headbreadth', 'heelbreadth', 'hipbreadth', 'iliocristaleheight', 'interscyeii',\n",
    "       'lateralfemoralepicondyleheight', 'lateralmalleolusheight', 'neckcircumferencebase', 'radialestylionlength', 'shouldercircumference', 'shoulderelbowlength', 'sleeveoutseam',\n",
    "       'thighcircumference', 'thighclearance', 'verticaltrunkcircumferenceusa', 'waistcircumference', 'waistdepth', 'wristheight', 'BMI']]\n",
    "y = ansur_df['bicepscircumferenceflexed']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fitting and transforming the X_train, transforming the X_test\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha = 0.035\n",
      "The model explains 85.62% of the test set variance\n",
      "24 features out of 32 selected\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "\n",
    "# Create and fit the LassoCV\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R-squared on the test-set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('The model explains {0:.2%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask),len(lcv_mask)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We got a decent R squared and removed 10 features. We'll save the lcv_mask for later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble models for extra votes\n",
    "\n",
    "The `LassoCV()` model selected 22 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let's use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE).\n",
    "  \n",
    "1. Select 10 features with RFE on a GradientBoostingRegressor and drop 3 features on each step.\n",
    "  \n",
    "2. Calculate the R-squared on the test-set\n",
    "  \n",
    "3. Assign the support array of the fitted model to gb_mask.\n",
    "  \n",
    "4. Modify the first step to select 10 features with RFE on a `RandomForestRegressor()` and drop 3 features on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 11 features.\n",
      "The model can explain 83.26% of the variance in the test-set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator= GradientBoostingRegressor(), n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R-squared on the test-set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.2%} of the variance in the test-set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to the mask\n",
    "gb_mask = rfe_gb.support_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 32 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 26 features.\n",
      "Fitting estimator with 23 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 11 features.\n",
      "The model can explain 82.30% of the variance in the test-set\n"
     ]
    }
   ],
   "source": [
    "# Creating the RandomForestRegressor() model with the Recursive Feature Eliminator\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Creating the model and fitting it\n",
    "rfe_rf = RFE(RandomForestRegressor(), n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R-squared on the test-set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.2%} of the variance in the test-set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to the mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the Lasso linear model from the previous exercise, we now have the votes from 3 models on which features are important."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining 3 feature selectors\n",
    "\n",
    "We'll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 3 0 1 1 3 1 1 1 3 1 1 1 0 0 1 0 1 1 1 3 3 0 3 2 2 1 2 0 3]\n",
      "[False False  True  True False False False  True False False False  True\n",
      " False False False False False False False False False False  True  True\n",
      " False  True False False False False False  True]\n",
      "Index(['bideltoidbreadth', 'buttockcircumference', 'chestcircumference',\n",
      "       'forearmcircumferenceflexed', 'shouldercircumference',\n",
      "       'shoulderelbowlength', 'thighcircumference', 'BMI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, gb_mask, rf_mask], axis=0)\n",
    "print(votes)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes == 3\n",
    "print(meta_mask)\n",
    "\n",
    "# Apply the dimensionality reduction on x\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "print(X_reduced.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 84.0% of the variance in the test set using 8 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Creating the LR\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Plug the reduced data into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit/transform the training features, then fitting the whole model on the scaled features\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Acquiring the r^2\n",
    "r_squared = lr.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lr.coef_)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the votes from 3 models you were able to select just 8 features that allowed a simple linear model to get a high accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
