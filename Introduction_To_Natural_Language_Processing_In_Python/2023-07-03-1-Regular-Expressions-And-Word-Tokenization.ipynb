{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions & word tokenization\n",
    "  \n",
    "In this course, you'll learn natural language processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning.\n",
    "  \n",
    "This chapter will introduce some basic NLP concepts, such as word tokenization and regular expressions to help parse text. You'll also learn how to handle non-English text and more difficult tokenization you might find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to regular expressions\n",
    "  \n",
    "In this course you'll be learning about regular expressions.\n",
    "  \n",
    "**What is Natural Language Processing?**\n",
    "  \n",
    "Natural language processing is a massive field of study and actively used practice which aims to make sense of language using statistics and computers. In this course, you will learn some of the basics of NLP which will help you move from simple to more difficult and advanced topics. Even though this is the first course, you will still get some exposure to the challenges of the field such as topic identification and text classification. \n",
    "  \n",
    "Some interesting NLP areas you might have heard about are: topic identification, chatbots, text classification, translation, sentiment analysis. There are also many more! You will learn the fundamentals of some of these topics as we move through the course.\n",
    "  \n",
    "<img src='../_images/introduction-to-regular-expressions-and-nlp.png' alt='img' width='530'>\n",
    "  \n",
    "**What exactly are regular expressions?**\n",
    "  \n",
    "*Regular expressions are strings you can use that have a special syntax, which allows you to match patterns and find other strings.*  \n",
    "  \n",
    "A pattern is a series of letters or symbols which can map to an actual text or words or punctuation. You can use regular expressions to do things like find links in a webpage, parse email addresses and remove unwanted strings or characters. Regular expressions are often referred to as regex and can be used easily with python via the `re` library. Here we have a simple import of the library. We can match a substring by using the `re.match()` method which matches a pattern with a string. It takes the pattern as the first argument, the string as the second and returns a match object, here we see it matched exactly what we expected: abc. We can also use special patterns that regex understands, like the `\\w+` which will match a word. We can see here via the match object representation that it has matched the first word it found -- hi.\n",
    "  \n",
    "<img src='../_images/introduction-to-regular-expressions-and-nlp1.png' alt='img' width='560'>\n",
    "  \n",
    "**Common regex patterns**\n",
    "  \n",
    "There are hundreds of characters and patterns you can learn and memorize with regular expressions, but to get started, I want to share a few common patterns. The first pattern `\\w` we already saw, it is used to match words. The `\\d` pattern allows us to match digits, which can be useful when you need to find them and separate them in a string. The `\\s` pattern matches spaces, the `.` is a wildcard character. The wildcard will match ANY letter or symbol. The `+` and `*` characters allow things to become greedy, grabbing repeats of single letters or whole patterns. For example to match a full word rather than one character, we need to add the `+` symbol after the `\\w`. Using these character classes as capital letters negates them so the `\\S` matches anything that is not a space. You can also create a group of characters you want by putting them inside square brackets `[ ]`, like our lowercase group.\n",
    "  \n",
    "**Common regex patterns (2)**\n",
    "  \n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "  <th>pattern</th>\n",
    "  <th>matches</th>\n",
    "  <th>examples</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "  <td>\\w+</td>\n",
    "  <td>word</td>\n",
    "  <td>'Magic'</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\\d</td>\n",
    "  <td>digit</td>\n",
    "  <td>9</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\\s</td>\n",
    "  <td>space</td>\n",
    "  <td>' '</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>.*</td>\n",
    "  <td>wildcard</td>\n",
    "  <td>'username74'</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>+ or *</td>\n",
    "  <td>greedy match</td>\n",
    "  <td>'aaaaaaa'</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>\\S</td>\n",
    "  <td>not space</td>\n",
    "  <td>'no_spaces'</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>[a-z]</td>\n",
    "  <td>lowercase group</td>\n",
    "  <td>'abcdfg'</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "  \n",
    "**Python's re module**\n",
    "  \n",
    "In the following exercises, you'll use the `re` module to perform some simple activities, like splitting on a pattern or finding all patterns in a string. In addition to `.split()` and `.findall()`, `.search()` and `.match()` are also quite popular. You saw a simple match at the beginning of this video, and search is similar but doesn't require you to match the pattern from the beginning of the string. The syntax for the regex library is always to pass the pattern first, and the string second. Depending on the method, it may return an iterator, a new string or a match object. Here we see the `re.split()` method will take a pattern for spaces and a string with some spaces and return a list object with the results of splitting on spaces. This can be used for tokenization, so you can preprocess text using regex while doing natural language processing.\n",
    "  \n",
    "**Python's re Module**  \n",
    "- `.split()`: split a string on regex\n",
    "- `.findall()`: find all patterns in a string\n",
    "- `.search()`: search for a pattern\n",
    "- `.match()`: match an entire string or substring based on a pattern\n",
    "  \n",
    "**Note on schema**  \n",
    "- Pattern first, and the string second\n",
    "- May return an iterator, string, or match object\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Now it's your turn! Get started writing your first Regex and I'll see you back here soon!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which pattern?\n",
    "  \n",
    "Which of the following Regex patterns results in the following text?\n",
    "  \n",
    "```python\n",
    ">>> my_string = \"Let's write RegEx!\"\n",
    ">>> re.findall(PATTERN, my_string)\n",
    "['Let', 's', 'write', 'RegEx']\n",
    "```\n",
    "  \n",
    "In the IPython Shell, try replacing PATTERN with one of the below options and observe the resulting output. The `re` module has been pre-imported for you and `my_string` is available in your namespace.\n",
    "  \n",
    "Possible answers  \n",
    "- [ ] PATTERN = r\"\\s+\"\n",
    "- [x] PATTERN = r\"\\w+\"\n",
    "- [ ] PATTERN = r\"[a-z]\"\n",
    "- [ ] PATTERN = r\"\\w\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicing regular expressions: `re.split()` and `re.findall()`\n",
    "  \n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at `my_string` first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "  \n",
    "Note: It's important to prefix your regex patterns with `r` to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"`\\n`\" in Python is used to indicate a new line, but if you use the `r` prefix, it will be interpreted as the raw string \"`\\n`\" - that is, the character \"`\\`\" followed by the character \"`n`\" - and not as a new line.\n",
    "  \n",
    "The regular expression module re has already been imported for you.\n",
    "  \n",
    "*Remember from the video that the syntax for the regex library is to always to pass the pattern first, and then the string second.*\n",
    "  \n",
    "1. Split `my_string` on each sentence ending. To do this:\n",
    "- Write a pattern called `sentence_endings` to match sentence endings (`.?!`).\n",
    "- Use `re.split()` to split `my_string` on the pattern and print the result.\n",
    "2. Find and print all capitalized words in `my_string` by writing a pattern called `capitalized_words` and using `re.findall()`.\n",
    "- Remember the `[a-z]` pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.\n",
    "3. Write a pattern called spaces to match one or more spaces (\"`\\s+`\") and then use `re.split()` to split `my_string` on this pattern, keeping all punctuation intact. Print the result.\n",
    "4. Find all digits in `my_string` by writing a pattern called digits (\"`\\d+`\") and using `re.findall()`. Print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String that we will be working with\n",
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capicalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Practice is the key to mastering RegEx."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to tokenization\n",
    "  \n",
    "In this video, we'll learn more about string tokenization!\n",
    "  \n",
    "**What is tokenization?**\n",
    "  \n",
    "Tokenization is the process of transforming a string or document into smaller chunks, which we call tokens. This is usually one step in the process of preparing a text for natural language processing. There are many different theories and rules regarding tokenization, and you can create your own tokenization rules using regular expresssions, but normally tokenization will do things like break out words or sentences, often separate punctuation or you can even just tokenize parts of a string like separating all hashtags in a Tweet.\n",
    "  \n",
    "<img src='../_images/intro-to-tokenization.png' alt='img' width='480'>\n",
    "  \n",
    "**`nltk` library**\n",
    "  \n",
    "One library that is commonly used for simple tokenization is `nltk`, the natural language toolkit library. Here is a short example of using the `word_tokenize` method to break down a string into tokens. We can see from the result that words are separated and punctuation are individual tokens as well.\n",
    "  \n",
    "<img src='../_images/intro-to-tokenization1.png' alt='img' width='500'>\n",
    "  \n",
    "**Why tokenize?**\n",
    "  \n",
    "Why bother with tokenization? Because it can help us with some simple text processing tasks like mapping part of speech, matching common words and perhaps removing unwanted tokens like common words or repeated words. Here, we have a good example. The sentence is: \"I don't like Sam's shoes.\" When we tokenize it we can clearly see the negation in the not and we can see possession with the 's. These indicators can help us determine meaning from simple text.\n",
    "  \n",
    "<img src='../_images/intro-to-tokenization2.png' alt='img' width='500'>\n",
    "  \n",
    "**Other nltk tokenizers**\n",
    "  \n",
    "Beyond just tokenizing words, NLTK has plenty of other tokenizers you can use, including these ones you'll be working with in this chapter. The `sent_tokenize()` function will split a document into individual sentences. The `regexp_tokenize()` uses regular expressions to tokenize the string, giving you more granular control over the process. And the `tweettokenizer()` does neat things like recognize hashtags, mentions and when you have too many punctuation symbols following a sentence. How convenient!!!\n",
    "  \n",
    "<img src='../_images/intro-to-tokenization3.png' alt='img' width='500'>\n",
    "  \n",
    "**More regex practice**\n",
    "  \n",
    "You'll be using more regex in this section as well, not only when you are tokenizing, but also figuring out how to parse tokens and text. Using the regex module's `re.match()` and `re.search()` are pretty essential tools for Python string processing. Learning when to use search versus match can be challenging, so let's take a look at how they are different. When we use search and match with the same pattern and string with the pattern is at the beginning of the string, we see we find identical matches. That is the case with matching and searching abcde with the pattern abc. When we use search for a pattern that appears later in the string we get a result, but we don't get the same result using match. This is because match will try and match a string from the beginning until it cannot match any longer. \n",
    "  \n",
    "Search will go through the ENTIRE string to look for match options. If you need to find a pattern that might not be at the beginning of the string, you should use search. If you want to be specific about the composition of the entire string, or at least the initial pattern, then you should use match.\n",
    "  \n",
    "<img src='../_images/intro-to-tokenization4.png' alt='img' width='520'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Now it's your turn to try some tokenization!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization with NLTK\n",
    "  \n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as `scene_one`. Feel free to check it out in the IPython Shell!\n",
    "  \n",
    "Your job in this exercise is to utilize `word_tokenize` and sent_tokenize from `nltk.tokenize` to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.\n",
    "  \n",
    "1. Import the `sent_tokenize` and `word_tokenize` functions from `nltk.tokenize`.\n",
    "2. Tokenize all the sentences in `scene_one` using the `sent_tokenize()` function.\n",
    "3. Tokenize the fourth sentence in sentences, which you can access as `sentences[3]`, using the `word_tokenize()` function.\n",
    "4. Find the unique tokens in the entire scene by using `word_tokenize()` on `scene_one` and then converting it into a set using `set()`.\n",
    "5. Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../_datasets/grail.txt', 'r') as file:\n",
    "    holy_grail = file.read()\n",
    "    scene_one = re.split('SCENE 2:', holy_grail)[0]  # splits at 'SCENE 2:' and assigns the first part to the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'found', 'empty', 'not', 'bird', 'Who', 'maintain', ',', 'African', 'Yes', 'European', 'horse', 'or', 'why', 'house', 'they', 'yeah', 'tell', 'could', 'servant', 'I', 'right', 'swallows', 'where', 'lord', 'Please', 'Britons', 'ridden', 'simple', 'SOLDIER', \"'em\", '#', 'on', 'yet', '2', 'an', 'second', 'does', 'it', 'line', 'from', 'speak', 'your', 'air-speed', 'Well', 'snows', 'beat', \"'m\", 'But', 'have', 'son', 'go', 'that', 'these', '?', 'sun', 'get', 'together', 'just', \"'d\", 'Where', 'KING', 'our', 'wants', 'through', 'bring', 'if', '--', 'Saxons', 'Am', 'must', 'ounce', 'be', 'south', 'two', 'am', 'do', 'interested', 'martin', 'all', 'That', 'Ridden', 'SCENE', 'will', ']', 'carry', 'Listen', 'needs', 'temperate', 'goes', 'agree', \"'\", 'non-migratory', 'under', 'feathers', 'Mercea', 'forty-three', 'climes', 'England', 'this', 'kingdom', 'maybe', 'guiding', 'winter', 'point', 'in', 'at', 'course', 'velocity', 'coconut', 'order', 'is', 'breadth', 'them', 'me', '.', 'defeator', 'land', 'mean', 'carried', 'he', 'times', 'wind', 'Arthur', 'No', 'join', 'Will', 'warmer', 'strand', 'grip', 'halves', 'a', 'matter', 'Uther', 'using', 'Pull', 'grips', 'It', '!', 'one', '1', 'Camelot', 'sovereign', 'bangin', 'Not', 'suggesting', 'use', 'swallow', 'Court', 'ratios', 'strangers', 'to', 'wings', '...', 'Whoa', 'with', 'What', \"n't\", 'back', 'anyway', 'question', 'then', 'Supposing', 'carrying', 'pound', 'Pendragon', 'Patsy', 'Halt', 'In', 'its', 'clop', 'court', 'Wait', 'creeper', 'ARTHUR', 'you', 'fly', 'castle', 'other', 'We', 'knights', \"'ve\", 'minute', 'migrate', 'the', 'tropical', 'trusty', 'Found', 'here', 'who', 'King', 'search', 'five', 'got', 'The', 'seek', 'may', 'by', 'but', 'A', '[', 'husk', 'held', \"'s\", 'covered', 'dorsal', 'since', 'every', 'length', 'plover', ':', 'zone', 'Oh', 'You', 'Are', 'master', 'are', 'coconuts', 'They', 'weight', 'and', 'my', 'ask', 'So', \"'re\", 'of', 'there'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Tokenization is fundamental to NLP, and you'll end up using it a lot in text mining and information retrieval projects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with `re.search()`\n",
    "  \n",
    "In this exercise, you'll utilize `re.search()` and `re.match()` to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "  \n",
    "You have both `scene_one` and sentences available from the last exercise; now you can use them with `re.search()` and `re.match()` to extract and match more text.\n",
    "  \n",
    "1. Use `re.search()` to search for the first occurrence of the word \"coconuts\" in `scene_one`. Store the result in match.\n",
    "2. Print the start and end indexes of match using its `.start()` and `.end()` methods, respectively.\n",
    "3. Write a regular expression called `pattern1` to find anything in square brackets.\n",
    "4. Use `re.search()` with the pattern to find the first text in `scene_one` in square brackets in the scene. Print the result.\n",
    "5. Create a pattern to match the script notation (e.g. Character:), assigning the result to `pattern2`.\n",
    "- Remember that you will want to match any words or spaces that precede the : (such as the space within SOLDIER #1:).\n",
    "6. Use `re.match()` with your new pattern to find and print the script notation in the fourth line. The tokenized sentences are available in your namespace as sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.+]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're familiar with the basics of tokenization and regular expressions, it's time to learn about more advanced tokenization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced tokenization with regex\n",
    "  \n",
    "In this video, we'll take a look at doing more advanced tokenization with regex.\n",
    "  \n",
    "**Regex groups using or \"|\"**\n",
    "  \n",
    "One new regex pattern you will find useful for advanced tokenization is the ability to use the or method. In regex, OR is represented by the pipe character. To use the or, you can define a group using parenthesis. Groups can be either a pattern or a set of characters you want to match. You can also define explicit character classes using square brackets. We'll go a bit more into depth on groups and ranges soon. Let's take an example that we want to tokenize using regular expressions and we want to find all digits and words. We define our pattern using a group with the OR symbol and make them greedy so they catch the full word or digits. Then, we can call `.findall()` using Python's `re` library and return our tokens. Notice that our pattern does not match punctuation but properly matches the words and digits.\n",
    "  \n",
    "<img src='../_images/regex-patterns-advanced-token.png' alt='img' width='520'>\n",
    "  \n",
    "**Regex ranges and groups**\n",
    "  \n",
    "Let's take a look at another more advanced topic, defining groups and character ranges. Here we have another chart of patterns, and this time we are using ranges or character classes marked by the square brackets and groups marked by the parentheses. We can see in this chart that we can use square brackets to define a new character class. \n",
    "  \n",
    "For example, we can match all upper and lowercase english letters using Uppercase A hyphen Uppercase Z which will match all uppercase and then lowercase a hyphen lowercase z which will match all lowercase letters. \n",
    "  \n",
    "We can also make ranges to match all digits `0-9`. \n",
    "\n",
    "Or perhaps a more complex range like uppercase and lowercase English with the hyphen and period. Because the hyphen and period are special characters in regex, we must tell regex we mean an ACTUAL period or hyphen. To do so, we use what is called an escape character and in regex that means to place a backwards slash in front of our character so it knows then to look for a hyphen or period. \n",
    "  \n",
    "On the other hand, with groups which are designated by the parentheses, we can only match what we explicitly define in the group. So a-z matched only a, `a-z`. \n",
    "  \n",
    "Groups are useful when you want to define an explicit group, such as the final example; where we are taking spaces or commas.\n",
    "  \n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "  <th>pattern</th>\n",
    "  <th>matches</th>\n",
    "  <th>example</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "  <td>[A-Za-z]+</td>\n",
    "  <td>upper and lowercase English alphabet</td>\n",
    "  <td>'ABCDEFghijk'</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>[0-9]</td>\n",
    "  <td>numbers from 0 to 9</td>\n",
    "  <td>9</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>[A-Za-z\\-\\.]+</td>\n",
    "  <td>upper and lowercase English alphabet, - and .</td>\n",
    "  <td>'My-Website.com'</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>(a-z)</td>\n",
    "  <td>a, - and z</td>\n",
    "  <td>'a-z'</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>(\\s+|,)</td>\n",
    "  <td>spaces or a comma</td>\n",
    "  <td>', '</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "  \n",
    "**Character range with `re.match()`**\n",
    "  \n",
    "In this code example, we can use match with a character range to match all lowercase ascii, any digits and spaces. It is greedy marked by the + after the range definition, but once it hits the comma, it can't match anymore. This short example demonstrates that thinking about what regex method you use (such as search versus match) and whether you define a group or a range can have a large impact on the usefulness and readability of your patterns.\n",
    "  \n",
    "<img src='../_images/regex-patterns-advanced-token1.png' alt='img' width='520'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Now it's your turn to practice advanced regex techniques to help with tokenization!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a tokenizer\n",
    "  \n",
    "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "  \n",
    "```python\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "```\n",
    "  \n",
    "The string is available in your workspace as `my_string`, and the patterns have been pre-loaded as `pattern1`, `pattern2`, `pattern3`, and `pattern4`, respectively.\n",
    "  \n",
    "Additionally, `regexp_tokenize` has been imported from `nltk.tokenize`. You can use `regexp_tokenize(string, pattern)` with `my_string` and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer.\n",
    "  \n",
    "Possible answers\n",
    "  \n",
    "- [ ] `r\"(\\w+|\\?|!)\"`\n",
    "- [x] `r\"(\\w+|#\\d|\\?|!)\"`\n",
    "- [ ] `r\"(#\\d\\w\\?!)\"`\n",
    "- [ ] `r\"\\s+\"`\n",
    "  \n",
    "```python\n",
    "In [4]:\n",
    "regexp_tokenize(my_string, pattern2)\n",
    "Out[4]:\n",
    "\n",
    "['SOLDIER',\n",
    " '#1',\n",
    " 'Found',\n",
    " 'them',\n",
    " '?',\n",
    " 'In',\n",
    " 'Mercea',\n",
    " '?',\n",
    " 'The',\n",
    " 'coconut',\n",
    " 's',\n",
    " 'tropical',\n",
    " '!']\n",
    "```\n",
    "  \n",
    "Well done! Option two preformed the best."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex with NLTK tokenization\n",
    "  \n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The `nltk.tokenize`.`TweetTokenizer` class gives you some extra methods and attributes for parsing tweets.\n",
    "  \n",
    "Here, you're given some example tweets to parse using both `TweetTokenizer` and `regexp_tokenize` from the `nltk.tokenize` module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n",
    "  \n",
    "Unlike the syntax for the regex library, with `nltk_tokenize()` you pass the pattern as the second argument.\n",
    "  \n",
    "1. From `nltk.tokenize`, import `regexp_tokenize` and `TweetTokenizer`.\n",
    "2. A regex pattern to define hashtags called pattern1 has been defined for you. Call `regexp_tokenize()` with this hashtag pattern on the first tweet in tweets and assign the result to hashtags.\n",
    "3. Print hashtags (this has already been done for you).\n",
    "4. Write a new pattern called pattern2 to match mentions and hashtags. A mention is something like @DataCamp.\n",
    "5. Then, call `regexp_tokenize()` with your new hashtag pattern on the last tweet in tweets and assign the result to `mentions_hashtags`.\n",
    "6. You can access the last element of a list using -1 as the index, for example, `tweets[-1]`.\n",
    "Print `mentions_hashtags` (this has been done for you).\n",
    "7. Create an instance of `TweetTokenizer` called `tknzr` and use it inside a list comprehension to tokenize each tweet into a new list called `all_tokens`.\n",
    "8. To do this, use the `.tokenize()` method of `tknzr`, with `t` as your iterator variable.\n",
    "Print `all_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of the tweet we will use for the exercise\n",
    "tweets = [\n",
    "    'This is the best #nlp exercise ive found online! #python',\n",
    "    '#NLP is super fun! <3 #learning',\n",
    "    'Thanks @datacamp :) #nlp #python'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"[@|#]\\w+\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization\n",
    "  \n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "  \n",
    "Here, you have access to a string called `german_text`, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "  \n",
    "The following modules have been pre-imported from `nltk.tokenize`: `regexp_tokenize` and `word_tokenize`.\n",
    "  \n",
    "Unicode ranges for emoji are:\n",
    "  \n",
    "('\\U0001F300'-'\\U0001F5FF'),  \n",
    "('\\U0001F600-\\U0001F64F'),  \n",
    "('\\U0001F680-\\U0001F6FF'),  \n",
    "('\\u2600'-\\u26FF-\\u2700-\\u27BF').  \n",
    "  \n",
    "1. Tokenize all the words in german_text using `word_tokenize()`, and print the result.\n",
    "2. Tokenize only the capital words in `german_text`.\n",
    "- First, write a pattern called `capital_words` to match only capital words. Make sure to check for the German Ãœ! To use this character in the exercise, copy and paste it from these instructions.\n",
    "Then, tokenize it using `regexp_tokenize()`.\n",
    "3. Tokenize only the emoji in `german_text`. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use `regexp_tokenize()` to tokenize the emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'When are we going for pizza? ðŸ• And do you want to ride with Ãœber? ðŸš•'\n",
    "german_text = 'Wann gehen wir Pizza essen? ðŸ• Und fÃ¤hrst du mit Ãœber? ðŸš•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'ðŸ•', 'Und', 'fÃ¤hrst', 'du', 'mit', 'Ãœber', '?', 'ðŸš•']\n",
      "['Wann', 'Pizza', 'Und', 'Ãœber']\n",
      "['ðŸ•', 'ðŸš•']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting word length with nltk\n",
    "  \n",
    "In this video, we are going to learn about using charts with our NLP tools.\n",
    "  \n",
    "**Getting started with `matplotlib`**\n",
    "  \n",
    "`Matplotlib` is a charting library used by many different open-source Python projects to create data visualizations, charts and graphs. It has fairly straightforward functionality with lots of options for graphs like histograms, bar charts, line charts and scatter plots. It even has advanced functionality like generating 3D graphs and animations.\n",
    "  \n",
    "**Plotting a histogram with matplotlib**\n",
    "  \n",
    "Matplotlib is usually imported by simply aliasing the `pyplot` module as `plt`. If we want to plot a basic histogram, which is a type of plot used to show distribution of data, we can pass in a small array to the `plt.hist()` function. The array `[1, 5, 5, 7, 7, 7, 9]` has 5 appearing twice and 7 appearing three times, so it's a good candidate to show distribution. Finally, we call the `plt.show()` function and `matplotlib` will show us the generated chart in our system's standard graphics viewing tool.\n",
    "  \n",
    "<img src='../_images/matplotlib-and-nltk.png' alt='img' width='450'>\n",
    "  \n",
    "**Generated histogram**\n",
    "  \n",
    "This is the chart that we generated using the previous code. We notice that indeed it has determined proper bins for each entry and we can see that the 7 and 5 bins reflect the distribution we expected to see. It's not the prettiest chart by default, but making it look nicer is fairly easy with more arguments and several available helper libraries.\n",
    "  \n",
    "**Combining NLP data extraction with plotting**\n",
    "  \n",
    "We can then use skills we have learned throughout this first chapter to tokenize text and chart word length for a simple sentence. First, we perform the necessary imports to use NLTK for word tokenization and `matplotlib` charting. Then, we tokenize the words and punctuation in a short sentence. Finally, we can use Python list comprehension with our tokenized words array to transform it to a list of lengths. \n",
    "  \n",
    "As a brief refresher on list comprehensions, it is a succint way to write a for loop. If we look at the syntax, we have opening and closing square brackets. Then we can iterate over any list and make a new list using this simple syntax. \n",
    "  \n",
    "Here, we create a list that holds the lengths of each word in the words array simply by saying `len(w) for w in words`. This will iterate over each word, calculate the length and return it as a new list. We then pass this array of token lengths to the `plt.hist()` function and generate our chart using the `plt.show()` method.\n",
    "  \n",
    "<img src='../_images/matplotlib-and-nltk1.png' alt='img' width='600'>\n",
    "  \n",
    "**Word length histogram**\n",
    "  \n",
    "Here is the generated histogram from our previous code. We can see from the chart that we have a majority of four-letter words in our example sentence. Of course, with a simple sentence, this is easy enough to simply count by hand -- but for an entire play or book, this would be tedious and prone to error -- so writing it in code makes it a lot easier.\n",
    "  \n",
    "<img src='../_images/matplotlib-and-nltk2.png' alt='img' width='450'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Now it's your turn to start plotting NLP charts with `matplotlib`!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting practice\n",
    "  \n",
    "Try using your new skills to find and chart the number of words per line in the script using `matplotlib`. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
    "  \n",
    "Using list comprehensions here will speed up your computations. For example: `my_lines = [tokenize(l) for l in lines]` will call a function tokenize on each line in the list lines. The new transformed list will be saved in the `my_lines` variable.\n",
    "  \n",
    "You have access to the entire script in the variable `holy_grail`. Go for it!\n",
    "  \n",
    "1. Split the script `holy_grail` into lines using the newline ('`\\n`') character.\n",
    "2. Use `re.sub()` inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1. The pattern has been written for you.\n",
    "3. Use a list comprehension to tokenize lines with `regexp_tokenize()`, keeping only words. Recall that the pattern for words is \"`\\w+`\".\n",
    "4. Use a list comprehension to create a list of line lengths called `line_num_words`.\n",
    "5. Use `t_line` as your iterator variable to iterate over `tokenized_lines`, and then `len()` function to compute line lengths.\n",
    "6. Plot a histogram of `line_num_words` using `plt.hist()`. Don't forgot to use `plt.show()` as well to display the plot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about the below regular expression\n",
    "  \n",
    "`pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"`\n",
    "  \n",
    "The given pattern is a regular expression pattern that can be used to match specific patterns in a text. Let's break it down:\n",
    "  \n",
    "1. `[A-Z]{2,}`: This part matches two or more consecutive uppercase letters (A to Z). `{2,}` specifies that the preceding pattern (uppercase letters) must occur at least 2 times in a row.\n",
    "  \n",
    "2. `(\\s)?`: This part matches an optional whitespace character. `\\s` represents any whitespace character, and `?` makes it optional. The parentheses `()` are used to group this part.\n",
    "  \n",
    "3. `(#\\d)?`: This part matches an optional pattern consisting of a hash symbol (`#`) followed by a digit (`\\d`). `#` matches the literal character '#', and `\\d` matches any digit (0-9). The `?` makes this pattern optional.\n",
    "  \n",
    "4. `([A-Z]{2,})?`: This part matches an optional sequence of two or more consecutive uppercase letters (A to Z). Similar to the first part, `{2,}` specifies that the pattern (uppercase letters) must occur at least 2 times. The `?` makes this pattern optional.\n",
    "  \n",
    "5. `:`: This part matches the colon character (':').\n",
    "  \n",
    "Putting it all together, the pattern `\"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"` can be used to match various patterns in a text. Here are a few examples of strings that would match this pattern:\n",
    "  \n",
    "- \"AB:\"\n",
    "- \"ABC:\"\n",
    "- \"AB #1:\"\n",
    "- \"AB C:\"\n",
    "- \"ABC #1:\"\n",
    "- \"ABC #1 D:\"\n",
    "- INSPECTOR:\n",
    "- OFFICER #1:\n",
    "- OFFICER #2:\n",
    "- CAMERAMAN:\n",
    "- BEDEVERE:\n",
    "- ARTHUR:\n",
    "- FRENCH GUARD:\n",
    "- ROBIN:\n",
    "- BRIDGEKEEPER:\n",
    "- HEAD KNIGHT:\n",
    "  \n",
    "Each part of the pattern serves a specific purpose, allowing for flexibility in matching different variations of uppercase letters, whitespace, a hash symbol followed by a digit, and a colon at the end.\n",
    "  \n",
    "---\n",
    "  \n",
    "In regular expressions (regex), brackets (`[]`), curly brackets (`{}`), and parentheses (`()`) have specific meanings and uses:\n",
    "\n",
    "1. Brackets (`[]`):\n",
    "- Character Class: Inside brackets, you can define a character class to match a single character. For example, `[abc]` matches any character 'a', 'b', or 'c'. It can also define ranges like `[a-z]` to match any lowercase letter.\n",
    "- Negation: Placing a caret (`^`) at the beginning of the brackets negates the character class. For example, `[^0-9]` matches any character that is not a digit.\n",
    "  \n",
    "2. Curly Brackets (`{}`):\n",
    "- Quantifiers: Curly brackets are used to specify quantifiers that control the repetition of the preceding element. For example, `a{3}` matches exactly three consecutive 'a' characters, and `a{2,4}` matches 2 to 4 consecutive 'a' characters.\n",
    "  \n",
    "3. Parentheses (`()`):\n",
    "- Grouping: Parentheses are used to group parts of a regex pattern together. It allows applying quantifiers, alternations, or other operations to a group as a whole. For example, `(ab)+` matches one or more occurrences of 'ab' in sequence.\n",
    "- Capturing: Parentheses also capture the matched substring for further use. The captured group can be referenced later in the regex or in replacement operations. For example, `(abc)` captures the substring 'abc' for later use.\n",
    "  \n",
    "It's important to note that the usage and behavior of brackets, curly brackets, and parentheses can vary slightly depending on the regex engine or programming language you are using. Therefore, it's recommended to consult the documentation or specific regex resources for the particular regex flavor you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '# of words per line in holy_grail')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')  # Remember .split() is a generator\n",
    "\n",
    "# Replace all script lines for speaker (for each speaker, remove the notation for them and place their 'lines' on their own observation)\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"  # Refer to the above note for context\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, '\\w+') for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(line_num_words)\n",
    "plt.title('# of words per line in holy_grail')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, and congratulations on finishing Chapter 1! See you in Chapter 2, where you'll begin learning about topic identification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
