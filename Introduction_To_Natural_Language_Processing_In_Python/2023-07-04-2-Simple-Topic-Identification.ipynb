{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple topic identification\n",
    "  \n",
    "This chapter will introduce you to topic identification, which you can apply to any text you encounter in the wild. Using basic NLP models, you will identify topics from texts based on term frequencies. You'll experiment and compare two simple methods: bag-of-words and Tf-idf using NLTK, and a new library `Gensim`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful links**  \n",
    "  \n",
    "[Regex Testing](https://regex101.com)  \n",
    "[NLTK Documentation](https://www.nltk.org)  \n",
    "[Gensim Documentation](https://radimrehurek.com/gensim/auto_examples/index.html)  \n",
    "[Python Documentation for Text Processing Services (re module and strings)](https://docs.python.org/3/library/text.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import re                           # Regular Expressions:      Text manipulation\n",
    "from pprint import pprint           # Pretty Print:             Advanced printing operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word counts with bag-of-words**\n",
    "  \n",
    "Welcome to chapter two! We'll begin with using word counts with a bag of words approach.\n",
    "  \n",
    "**Bag-of-words**\n",
    "  \n",
    "Bag of words is a very simple and basic method to finding topics in a text. For bag of words, you need to first create tokens using tokenization, and then count up all the tokens you have. The theory is that the more frequent a word or token is, the more central or important it might be to the text. Bag of words can be a great way to determine the significant words in a text based on the number of times they are used.\n",
    "  \n",
    "**Bag-of-words**  \n",
    "- Basic method for finding topics in a text\n",
    "- Need to first create tokens using tokenization\n",
    "- ... and then count up all the tokens\n",
    "- The more frequent a word, the more important it might be\n",
    "- Can be a great way to determine the significant words in a text\n",
    "  \n",
    "**Bag-of-words example**\n",
    "  \n",
    "Here we see an example series of sentences, mainly about a cat and a box. If we just us a simple bag of words model with tokenization like we learned in chapter one and remove the punctuation, we can see the example result. Box, cat, The and the are some of the most important words because they are the most frequent. Notice that the word THE appears twice in the bag of words, once with uppercase and once lowercase. If we added a preprocessing step to handle this issue, we could lowercase all of the words in the text so each word is counted only once.\n",
    "  \n",
    "<img src='../_images/nlp-bag-of-words-example.png' alt='img' width='500'>\n",
    "  \n",
    "**Bag-of-words in Python**\n",
    "  \n",
    "We can use the NLP fundamentals we already know, such as tokenization with NLTK to create a list of tokens. We will use a new class called `Counter` which we import from the standard library module `collections`. The list of tokens generated using `word_tokenize` can be passed as the initialization argument for the `Counter` class. The result is a counter object which has similar structure to a dictionary and allows us to see each token and the frequency of the token. `Counter` objects also have a method called `.most_common()`, which takes an integer argument, such as 2 and would then return the top 2 tokens in terms of frequency. The return object is a series of tuples inside a list. For each tuple, the first element holds the token and the second element represents the frequency. Note: other than ordering by token frequency, the `.most_common()` method does not sort the tokens it returns or tell us there are more tokens with that same frequency.\n",
    "  \n",
    "<img src='../_images/nlp-bag-of-words-example1.png' alt='img' width='500'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Now you know a bit about bag of words and can get started building your own using Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words picker\n",
    "  \n",
    "It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text?\n",
    "\n",
    "\"The cat is in the box. The cat box.\"\n",
    "  \n",
    "**Possible answers**\n",
    "  \n",
    "- [ ] ('the', 3), ('box.', 2), ('cat', 2), ('is', 1)\n",
    "- [ ] ('The', 3), ('box', 2), ('cat', 2), ('is', 1), ('in', 1), ('.', 1)\n",
    "- [ ] ('the', 3), ('cat box', 1), ('cat', 1), ('box', 1), ('is', 1), ('in', 1)\n",
    "- [x] ('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)\n",
    "  \n",
    "**Solution**\n",
    "  \n",
    "```python\n",
    "In [1]: from nltk.tokenize import word_tokenize\n",
    "\n",
    "In [2]: word_tokenize(\"The cat is in the box. The cat box.\", language='english')\n",
    "Out[2]: ['The', 'cat', 'is', 'in', 'the', 'box', '.', 'The', 'cat', 'box', '.']\n",
    "```\n",
    "**Alternative Solution**  \n",
    "  \n",
    "```python\n",
    "In [3]:\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "Counter(word_tokenize(my_string)).most_common(len(word_tokenize(my_string)))\n",
    "\n",
    "Out [3]: \n",
    "[('The', 2),\n",
    " ('cat', 2),\n",
    " ('box', 2),\n",
    " ('.', 2),\n",
    " ('is', 1),\n",
    " ('in', 1),\n",
    " ('the', 1)]\n",
    "```\n",
    "  \n",
    "Well done!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Counter with bag-of-words\n",
    "  \n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as `article_title`. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "  \n",
    "`word_tokenize` has been imported for you.\n",
    "  \n",
    "1. Import `Counter` from `collections`.\n",
    "2. Use `word_tokenize()` to split the article into tokens.\n",
    "3. Use a list comprehension with `t` as the iterator variable to convert all the tokens into lowercase. The `.lower()` method converts text into lowercase.\n",
    "4. Create a bag-of-words counter called `bow_simple` by using `Counter()` with `lower_tokens` as the argument.\n",
    "5. Use the `.most_common()` method of `bow_simple` to print the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "with open('../_datasets/wikipedia_articles/wiki_text_debugging.txt', 'r') as file:\n",
    "    article = file.read()\n",
    "    article_title = word_tokenize(article)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151),\n",
      " ('the', 150),\n",
      " ('.', 89),\n",
      " ('of', 81),\n",
      " (\"''\", 69),\n",
      " ('to', 63),\n",
      " ('a', 60),\n",
      " ('``', 47),\n",
      " ('in', 44),\n",
      " ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the aricle: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple, Bag-of-Words\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "pprint(bow_simple.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text preprocessing\n",
    "  \n",
    "In this video, we will cover some simple text preprocessing.\n",
    "  \n",
    "**Why preprocess?**\n",
    "  \n",
    "Text processing helps make for better input data when performing machine learning or other statistical methods. For example, in the last few exercises you have applied small bits of preprocessing (like tokenization) to create a bag of words. You also noticed that applying simple techniques like lowercasing all of the tokens, can lead to slightly better results for a bag-of-words model. Preprocessing steps like tokenization or lowercasing words are commonly used in NLP. Other common techniques are things like *lemmatization* or *stemming*, where you shorten the words to their root stems, or techniques like removing stop words, which are common words in a language that don't carry a lot of meaning -- such as and or the, or removing punctuation or unwanted tokens. Of course, each model and process will have different results -- so it's good to try a few different approaches to preprocessing and see which works best for your task and goal.\n",
    "  \n",
    "**Preprocessing example**\n",
    "  \n",
    "We have here some example input and output text we might expect from preprocessing. First we have a simple two sentence string about pets. Then we have some example output tokens we want. You can see that the text has been tokenized and that everything is lowercase. We also notice that stopwords have been removed and the plural nouns have been made singular.\n",
    "  \n",
    "<img src='../_images/nlp-preprocessing-examples.png' alt='img' width='530'>\n",
    "  \n",
    "**Text preprocessing with Python**\n",
    "  \n",
    "We can perform text preprocessing using many of the tools we already know and have learned. In this code, we are using the same text as from our previous video, a few sentences about a cat with a box. We can use list comprehensions to tokenize the sentences which we first make lowercase using the string `.lower()` method. The string `.is_alpha()` method will return `True` if the string has only alphabetical characters. We use the `.is_alpha()` method along with an if statement iterating over our tokenized result to only return only alphabetic strings (this will effectively strip tokens with numbers or punctuation). To read out the process in both code and English we say we take each token from the `word_tokenize` output of the lowercase text if it contains only alphabetical characters. In the next line, we use another list comprehension to remove words that are in the stopwords list. This stopwords list for english comes built in with the NLTK library. Finally, we can create a counter and check the two most common words, which are now cat and box (unlike the and box which were the two tokens returned in our first result). Preprocessing has already improved our bag of words and made it more useful by removing the stopwords and non-alphabetic words.\n",
    "  \n",
    "<img src='../_images/nlp-preprocessing-examples1.png' alt='img' width='530'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "You can now get started by preprocessing your own text!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing steps\n",
    "  \n",
    "Which of the following are useful text preprocessing steps?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] Stems, spelling corrections, lowercase.\n",
    "- [x] Lemmatization, lowercasing, removing unwanted tokens.\n",
    "- [ ] Removing stop words, leaving in capital words.\n",
    "- [ ] Strip stop words, word endings and digits.\n",
    "  \n",
    "Well done!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing practice\n",
    "  \n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "  \n",
    "You start with the same tokens you created in the last exercise: `lower_tokens`. You also have the `Counter` class imported.\n",
    "  \n",
    "1. Import the `WordNetLemmatizer` class from `nltk.stem`.\n",
    "2. Create a list `alpha_only` that contains only alphabetical characters. You can use the `.isalpha()` method to check for this.\n",
    "3. Create another list called `no_stops` consisting of words from `alpha_only` that are not contained in `english_stops`.\n",
    "4. Initialize a `WordNetLemmatizer` object called `wordnet_lemmatizer` and use its `.lemmatize()` method on the tokens in `no_stops` to create a new list called lemmatized.\n",
    "5. Create a new `Counter` called bow with the lemmatized words.\n",
    "6. Lastly, print the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexandergursky/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "# nltk.download('wordnet')\n",
    "# Out [i]: [nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
    "# Out [i]: [nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
    "# Out [i]: [nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
    "# Out [i]: False\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "\n",
    "# Disable SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please note that disabling SSL certificate verification introduces potential security risks. It is recommended to enable certificate verification in a production environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading english stopwords\n",
    "with open('../_datasets/english_stopwords.txt', 'r') as file:\n",
    "    english_stops = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 39),\n",
      " ('system', 25),\n",
      " ('bug', 17),\n",
      " ('software', 16),\n",
      " ('problem', 15),\n",
      " ('tool', 15),\n",
      " ('computer', 14),\n",
      " ('process', 13),\n",
      " ('term', 13),\n",
      " ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()  # Lemmatize using WordNet's built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "pprint(bow.most_common(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to `gensim`\n",
    "  \n",
    "In this video, we will get started using a new tool called `Gensim`.\n",
    "  \n",
    "**What is `gensim`?**\n",
    "  \n",
    "**`Gensim`** is a popular open-source natural language processing library. It uses top academic models to perform complex tasks like building document or word vectors, corpora and performing topic identification and document comparisons.\n",
    "  \n",
    "**gensim**  \n",
    "  \n",
    "- Popular open-source NLP library\n",
    "- Uses top academic models to perform complex tasks\n",
    "- Building document or word vectors\n",
    "- Performing topic identification and document comparison\n",
    "  \n",
    "**What is a word vector?**\n",
    "  \n",
    "You might be wondering what a word or document vector is? Here are some examples <span style=\"color:red;\">**(Image Error: IMAGE NOT SHOWN IN VIDEO)**</span> here in visual form. A word embedding or vector is trained from a larger corpus and is a multi-dimensional representation of a word or document. You can think of it as a multi-dimensional array normally with sparse features (lots of zeros and some ones). With these vectors, we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find. For example, in this graphic we can see that the vector operation king minus queen is approximately equal to man minus woman. Or that Spain is to Madrid as Italy is to Rome. The deep learning algorithm used to create word vectors has been able to distill this meaning based on how those words are used throughout the text.\n",
    "  \n",
    "**`Gensim` example**\n",
    "  \n",
    "The graphic we have here is an example of LDA visualization. LDA stands for *latent dirichlet allocation*, and it is a statistical model we can apply to text using `Gensim` for topic analysis and modelling. This graph is just a portion of a blog post written in 2015 using `Gensim` to analyze US presidential addresses. The article is really neat and you can find the link here.\n",
    "  \n",
    "<img src='../_images/gensim-example-usa-presidential-addresses.png' alt='img' width='530'>\n",
    "  \n",
    "**Creating a `gensim` dictionary**\n",
    "  \n",
    "`Gensim` allows you to build corpora and dictionaries using simple classes and functions. A corpus (or if plural, corpora) is a set of texts used to help perform natural language processing tasks. Here, our documents are a list of strings that look like movie reviews about space or sci-fi films. First we need to do some basic preprocessing. For brevity, we will only tokenize and lowercase. For better results, we would want to apply more of the preprocessing we have learned in this chapter, such as removing punctuation and stop words. Then we can pass the tokenized documents to the `Gensim` Dictionary class. This will create a mapping with an id for each token. This is the beginning of our corpus. We now can represent whole documents using just a list of their token ids and how often those tokens appear in each document. We can take a look at the tokens and their ids by looking at the `.token2id` attribute, which is a dictionary of all of our tokens and their respective ids in our new dictionary.\n",
    "  \n",
    "<img src='../_images/gensim-example-usa-presidential-addresses1.png' alt='img' width='530'>\n",
    "  \n",
    "**Creating a `gensim` corpus**\n",
    "  \n",
    "Using the dictionary we built in the last slide, we can then create a `Gensim` corpus. This is a bit different than a normal corpus -- which is just a collection of documents. `Gensim` uses a simple bag-of-words model which transforms each document into a bag of words using the token ids and the frequency of each token in the document. Here, we can see that the `Gensim` corpus is a list of lists, each list item representing one document. Each document a series of tuples, the first item representing the `tokenid` from the dictionary and the second item representing the token frequency in the document. In only a few lines, we have a new bag-of-words model and corpus thanks to `Gensim`. And unlike our previous Counter-based bag of words, this `Gensim` model can be easily saved, updated and reused thanks to the extra tools we have available in `Gensim`. Our dictionary can also be updated with new texts and extract only words that meet particular thresholds. We are building a more advanced and feature-rich bag-of-words model which can then be used for future exercises.\n",
    "  \n",
    "<img src='../_images/gensim-example-usa-presidential-addresses2.png' alt='img' width='530'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Now you can get started building your own dictionary with `Gensim`!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are word vectors?\n",
    "  \n",
    "What are word vectors and how do they help with NLP?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] They are similar to bags of words, just with numbers. You use them to count how many tokens there are.\n",
    "- [ ] Word vectors are sparse arrays representing bigrams in the corpora. You can use them to compare two sets of words to one another.\n",
    "- [x] Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
    "- [ ] Word vectors don't actually help NLP and are just hype.\n",
    "  \n",
    "Well done! Keep working to use some word vectors yourself!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and querying a corpus with `gensim`\n",
    "  \n",
    "It's time to apply the methods you learned in the previous video to create your first `gensim` dictionary and corpus!\n",
    "  \n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called `articles`. You'll need to do some light preprocessing and then generate the `gensim` dictionary and corpus.\n",
    "  \n",
    "1. Import `Dictionary` from `gensim.corpora.dictionary`.\n",
    "2. Initialize a `gensim` `Dictionary` with the tokens in `articles`.\n",
    "3. Obtain the id for \"`computer`\" from `dictionary`. To do this, use its `.token2id` method which returns ids from text, and then chain `.get()` which returns tokens from ids. Pass in \"`computer`\" as an argument to `.get()`.\n",
    "4. Use a list comprehension in which you iterate over articles to create a `gensim` `MmCorpus` from `dictionary`.\n",
    "5. In the output expression, use the `.doc2bow()` method on dictionary with article as the argument.\n",
    "6. Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "# Extracting all txt files in directory, and preprocessing in-order to do exercise, originally not given\n",
    "path_list = glob.glob('../_datasets/wikipedia_articles/*.txt')\n",
    "articles = []                                   # Storing articles, global iterable variable to append to\n",
    "for article_path in path_list:\n",
    "    article = []                                # 'Holding-cell' for all extracted files, local iterable\n",
    "    with open(article_path, 'r') as file:\n",
    "        a = file.read()                         # Cycled variable that cycles the articles\n",
    "    tokens = word_tokenize(a)                   # Tokenization of words in article[i]\n",
    "    lower_tokens = [t.lower() for t in tokens]  # Convert all tokenized-words to lowercase\n",
    "    \n",
    "    # Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "    # Remove all stop words: no_stops\n",
    "    no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "    articles.append(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "[(4, 1), (6, 6), (7, 2), (9, 5), (18, 1), (19, 1), (20, 1), (22, 1), (24, 2), (28, 3)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary  # pip3 install gensim\n",
    "\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)  # Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")  # computer_id = int(223)\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]  # Corpus serialized using the sparse coordinate Matrix Market format\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim bag-of-words\n",
    "  \n",
    "Now, you'll use your new `gensim` corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
    "  \n",
    "You have access to the `dictionary` and `corpus` objects you created in the previous exercise, as well as the Python `defaultdict` and `itertools` to help with the creation of intermediate data structures for analysis.\n",
    "  \n",
    "- `defaultdict` allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument `int`, we are able to ensure that any non-existent keys are automatically assigned a default value of `0`. This makes it ideal for storing the counts of words in this exercise.\n",
    "  \n",
    "- `itertools.chain.from_iterable()` allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists).\n",
    "  \n",
    "The fifth document from `corpus` is stored in the variable `doc`, which has been sorted in descending order.\n",
    "  \n",
    "<br></br>\n",
    "\n",
    "1. Using the first for loop, print the top five words of `bow_doc` using each `word_id` with the `dictionary` alongside `word_count`.\n",
    "- The `word_id` can be accessed using the `.get()` method of `dictionary`.\n",
    "2. Create a `defaultdict` called `total_word_count` in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (`word_count`).\n",
    "3. Remember to specify int when creating the `defaultdict`, and inside the second for loop, increment each `word_id` of `total_word_count` by `word_count`.\n",
    "4. Create a sorted list from the `defaultdict`, using words across the entire corpus. To achieve this, use the `.items()` method on `total_word_count` inside `sorted()`.\n",
    "5. Similar to how you printed the top five words of `bow_doc` earlier, print the top five words of `sorted_word_count` as well as the number of occurrences of each word across all the documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer 252\n",
      "computers 100\n",
      "first 61\n",
      "cite 59\n",
      "computing 59\n",
      "computer 597\n",
      "software 451\n",
      "cite 322\n",
      "ref 259\n",
      "code 235\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict  # The default factory is called without arguments to produce a new value when a key is not present\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "\n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good work!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf with gensim\n",
    "  \n",
    "In this video, we will learn how to use a TFIDF model with Gensim.\n",
    "  \n",
    "**What is tf-idf?**\n",
    "  \n",
    "Tf-idf stands for term-frequncy - inverse document frequency. It is a commonly used natural language processing model that helps you determine the most important words in each document in the corpus. The idea behind tf-idf is that each corpus might have more shared words than just stopwords. These common words are like stopwords and should be removed or at least down-weighted in importance. For example, if I am an astronomer, sky might be used often but is not important, so I want to downweight that word. TF-Idf does precisely that. It will take texts that share common language and ensure the most common words across the entire corpus don't show up as keywords. Tf-idf helps keep the document-specific frequent words weighted high and the common words across the entire corpus weighted low.\n",
    "  \n",
    "<img src='../_images/what-is-tf-idf.png' alt='img' width='530'>\n",
    "  \n",
    "**Tf-idf formula**\n",
    "  \n",
    "The equation to calculate the weights can be outlined like so: The weight of token $i$ in document $j$ is calculated by taking the term frequency (or how many times the token appears in the document) multiplied by the log of the total number of documents divided by the number of documents that contain the same term. Let's unpack this a bit. First, the weight will be low if the term doesnt appear often in the document because the $tf$ variable will then be low. However, the weight will also be a low if the logarithm ($\\log()$) is close to zero, meaning the internal equation is low. Here we can see if the total number of documents divded by the number of documents that have the term is close to one, then our logarithm will be close to zero. So words that occur across many or all documents will have a very low tf-idf weight. On the contrary, if the word only occurs in a few documents, that logarithm will return a higher number.\n",
    "  \n",
    "$formula:$\n",
    "  \n",
    "$\\Large w_{i, j} = \\text{tf}_{i, j} â€¢ \\log (\\frac{N}{\\text{df}_i})$  \n",
    "  \n",
    "$where:$\n",
    "  \n",
    "$w_{i,j}$ = tf-idf for token $i$ in document $j$  \n",
    "$tf_{i,j}$ = Number of occurences for token $i$ in document $j$  \n",
    "$df_{i}$ = Number of documents that contain token $i$  \n",
    "$N$ = Total number of documents  \n",
    "  \n",
    "**Tf-idf with gensim**\n",
    "  \n",
    "You can build a Tfidf model using Gensim and the corpus you developed previously. Taking a look at the corpus we used in the last video, around movie reviews, we can use the Bag of Words corpus to translate it into a TF-idf model by simply passing it in initialization. We can then reference each document by using it like a dictionary key with our new tfidf model. For the second document in our corpora, we see the token weights along with the token ids. Notice there are some large differences! Token id 10 has a weight of 0.77 whereas tokens 0 and 1 have weights below 0.18. These weights can help you determine good topics and keywords for a corpus with shared vocabulary.\n",
    "  \n",
    "<img src='../_images/what-is-tf-idf1.png' alt='img' width='530'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Now you can build a tfidf model using Gensim to explore topics in the Wikipedia article list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is tf-idf?\n",
    "  \n",
    "You want to calculate the tf-idf weight for the word \"computer\", which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word \"computer\", tf-idf can be calculated by multiplying term frequency with inverse document frequency.\n",
    "  \n",
    "Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
    "  \n",
    "Which of the below options is correct?\n",
    "  \n",
    "Possible answers  \n",
    "  \n",
    "- [x] (5 / 100) * log(200 / 20)\n",
    "- [ ] (5 * 100) / log(200 * 20)\n",
    "- [ ] (20 / 5) * log(200 / 20)\n",
    "- [ ] (200 * 5) * log(400 / 5)\n",
    "  \n",
    "Correct!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf with Wikipedia\n",
    "Now it's your turn to determine new significant terms for your corpus by applying gensim's tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - `dictionary`, `corpus`, and `doc`. Will tf-idf make for more interesting results on the document level?\n",
    "  \n",
    "`TfidfModel` has been imported for you from `gensim.models.tfidfmodel`.\n",
    "  \n",
    "1. Initialize a new `TfidfModel` called `tfidf` using `corpus`.\n",
    "2. Use `doc` to calculate the weights. You can do this by passing `[doc]` to `tfidf`.\n",
    "3. Print the first five term ids with weights.\n",
    "4. Sort the term ids and weights in a new list from highest to lowest weight. *This has been done for you.*\n",
    "5. Using your pre-existing `dictionary`, print the top five weighted words (`term_id`) from `sorted_tfidf_weights`, along with their weighted score (`weight`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 0.005149712197382678), (6, 0.005127761019345027), (7, 0.008207466968432171), (9, 0.02574856098691339), (18, 0.0032491066476585816)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mechanical 0.1847740145909077\n",
      "circuit 0.15142303140509794\n",
      "manchester 0.1427799203657014\n",
      "alu 0.1397751059123981\n",
      "thomson 0.12812718041969826\n"
     ]
    }
   ],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
