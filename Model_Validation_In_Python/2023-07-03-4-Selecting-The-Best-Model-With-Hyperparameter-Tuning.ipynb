{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting the best model with Hyperparameter tuning\n",
    "  \n",
    "The first three chapters focused on model validation techniques. In chapter 4 we apply these techniques, specifically cross-validation, while learning about hyperparameter tuning. After all, model validation makes tuning possible and helps us select the overall best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to hyperparameter tuning\n",
    "  \n",
    "In this lesson, we are going to start applying the model validation techniques we have been practicing while introducing hyperparameter tuning.\n",
    "  \n",
    "**Model parameters**\n",
    "  \n",
    "To start, let's first review what model parameters are, as model parameters and model hyperparameters are quite different. Model parameters are created as the result of fitting a model and are estimated by the input data. They are used to make predictions on new data and are not manually set by the modeler.\n",
    "  \n",
    "**Linear regression parameters**\n",
    "  \n",
    "For example, in a linear model, the coefficients and intercept are considered model parameters. We can print a linear model's parameters using `lr.coef_` and `lr.intercept_`. Notice that these parameters are only created after the model has been fit.\n",
    "  \n",
    "<img src='../_images/model-parameters-example.png' alt='img' width='740'>\n",
    "  \n",
    "**Linear regression parameters**\n",
    "  \n",
    "If we did not call `.fit()`, the coefficients and intercept would not exist for the `lr` object.\n",
    "  \n",
    "<img src='../_images/model-parameters-example1.png' alt='img' width='740'>\n",
    "  \n",
    "**Model hyperparameters**\n",
    "  \n",
    "So, if model parameters are the result of training a model, then what are hyperparameters? Hyperparameters are the values that are set before training occurs. So anytime we refer to a parameter as being manually set, we are referring to hyperparameters. We have already been working with some hyperparameters for scikit-learn's random forest models, such as `n_estimators=`, and `max_depth=`. Let's cover a few more of the basic hyperparameters for these models.\n",
    "  \n",
    "**Random forest hyperparameters**\n",
    "  \n",
    "The table above only has four of the 15 or so possible hyperparameters, and we have already discussed the first three: `n_estimators=`, `max_depth=`, and `max_features=` during this course. I am adding `min_samples_split=` to our list, which is the minimum number of samples required to make a split at the end of a leaf. If a leaf in a decision tree has four observations in it, `min_samples_split=` must be 4 or greater in order for this leaf to be split into more leaves. So what now? If these are hyperparameters, how do we tune them?\n",
    "  \n",
    "<img src='../_images/model-parameters-example2.png' alt='img' width='740'>\n",
    "  \n",
    "**What is hyperparameter tuning?**\n",
    "  \n",
    "Throughout this course, we have been hinting at various aspects of hyperparameter tuning. We have used various hyperparameters and altered the values of these hyperparameters to suit our specific model or data. Hyperparameter tuning consists of selecting hyperparameters to test and then running a specific type of model with various values for these hyperparameters. For each run of the model, we keep track of how well the model did for a specified accuracy metric, as well as keep track of the hyperparameters that were used.\n",
    "  \n",
    "**Specifying ranges**\n",
    "  \n",
    "One of the hardest parts of this process is selecting the right hyperparameters to tune, and specifying the appropriate value ranges for each hyperparameter. For example, consider the three ranges of values specified in the example above. When we run hyperparameter tuning, we run our random forest model at different values from the ranges specified. We might select a random max depth, a minimum sample for creating splits of 8, and a maximum feature count of 4. We used the `random.choice()` function to select randomly from the depth list. To review which parameters were used at any time, you can use the `.get_params()` method on your model.\n",
    "  \n",
    "<img src='../_images/model-parameters-example3.png' alt='img' width='740'>\n",
    "  \n",
    "**Too many hyperparameters!**\n",
    "  \n",
    "If you do check out the contents of `.get_params()`, you might feel overwhelmed by the amount of options available. For this model, there are 16 different hyperparameters. In practice, however, only a handful of these hyperparameters will be tuned at the same time. Tuning too many can take forever to train and might make reading the output difficult.\n",
    "  \n",
    "<img src='../_images/model-parameters-example4.png' alt='img' width='740'>\n",
    "  \n",
    "**General guidelines**\n",
    "  \n",
    "It's best to start with the basics and tune the hyperparameters you understand. Read through the documentation for the ones that you don't, and test values you have seen in other models. As you practice this technique more, you will become more comfortable with the process.\n",
    "  \n",
    "**General guidelines**  \n",
    "- Start with the basics\n",
    "- Read documentation\n",
    "- Test practical ranges\n",
    "  \n",
    "**Model Parameters**  \n",
    "- Learned or estimated from the data\n",
    "- The result of fitting a model\n",
    "- Used when making future predictions\n",
    "- Not manually set\n",
    "  \n",
    "**Model Hyperparameters**  \n",
    "- Manually set *before* the training occurs\n",
    "- Specify how the training is supposed to happen\n",
    "  \n",
    "**Hyperparameter tuning**  \n",
    "- Select hyperparameters\n",
    "- Run a single model type at different value sets\n",
    "- Create ranges of possible values to select from\n",
    "- Specify a single accuracy metric\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's work on the basic steps for hyperparameter tuning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Hyperparameters\n",
    "  \n",
    "For a school assignment, your professor has asked your class to create a random forest model to predict the average test score for the final exam.\n",
    "  \n",
    "After developing an initial random forest model, you are unsatisfied with the overall accuracy. You realize that there are too many hyperparameters to choose from, and each one has a lot of possible values. You have decided to make a list of possible ranges for the hyperparameters you might use in your next model.\n",
    "  \n",
    "Your professor has provided de-identified data for the last ten quizzes to act as the training data. There are 30 students in your class.\n",
    "  \n",
    "1. Print `.get_params()` in the console to review the possible parameters of the model that you can tune.\n",
    "2. Create a maximum depth list, `[4, 8, 12]` and a minimum samples list `[2, 5, 10]` that specify possible values for each hyperparameter.\n",
    "3. Create one final list to use for the maximum features.\n",
    "4. Use values 4 through the maximum number of features possible (10), by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Model instantiation\n",
    "rfr = RandomForestRegressor(n_estimators=100, max_features='auto', random_state=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'squared_error',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1111,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display parameters\n",
    "rfr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Depth\n",
    "max_depth = [4, 8, 12]\n",
    "\n",
    "# Minimum samples for a split\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Max features \n",
    "max_features = [4, 6, 8, 10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Hyperparameter tuning requires selecting parameters to tune, as well the possible values these parameters can be set to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a model using ranges\n",
    "  \n",
    "You have just finished creating a list of hyperparameters and ranges to use when tuning a predictive model for an assignment. You have used `max_depth=`, `min_samples_split=`, and `max_features=` as your range variable names.\n",
    "  \n",
    "1. Randomly select a `max_depth=`, `min_samples_split=`, and `max_features=` using your range variables.\n",
    "2. Print out all of the parameters for `rfr` to see which values were randomly selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'squared_error',\n",
       " 'max_depth': 4,\n",
       " 'max_features': 10,\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 10,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Fill in rfr using your variables\n",
    "rfr = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=random.sample(max_depth, 1)[0],                   # Randomly takes 1 sample and takes the first one as sample() returns a list.\n",
    "    min_samples_split=random.sample(min_samples_split, 1)[0],   # Randomly takes 1 sample and takes the first one as sample() returns a list.\n",
    "    max_features = random.sample(max_features, 1)[0]            # Randomly takes 1 sample and takes the first one as sample() returns a list.\n",
    ")\n",
    "\n",
    "# Print out the parameters\n",
    "rfr.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `min_samples_split` was randomly set to 2. Since you specified a random state, `min_samples_split` will always be set to 2 if you only run this model one time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV\n",
    "  \n",
    "Now that we have discussed the basics of hyperparameter tuning let's combine model validation with tuning to start creating the most accurate, validated models possible.\n",
    "  \n",
    "**Grid searching hyperparameters**\n",
    "  \n",
    "Consider if we only had two parameters to choose from, the number of trees and the maximum depth. If we had five options for the number of trees and four options for the maximum depth, this would create 20 possible combinations of these parameters. Notice that they form a grid of possible parameters. We could conduct a complete grid search, and run our random forest model using each unique combination of the two hyperparameters.\n",
    "  \n",
    "<img src='../_images/grid-searching-hyper-tuning.png' alt='img' width='740'>\n",
    "  \n",
    "**Grid searching continued**\n",
    "  \n",
    "There is one main benefit for grid searching, which is that each possible combination of values will be tested. However, there is one major drawback: any additional parameter added for testing will grow the training time exponentially. Therefore grid searching is only possible with a limited number of parameters, and a limited number of ranges.\n",
    "  \n",
    "**Better methods**\n",
    "  \n",
    "There are two amazing alternatives to using grid search, which both have their advantages over grid searching. Random searching, which consists of randomly selecting from all hyperparameter values from the list of possible ranges, and Bayesian optimization, which uses the past results of each test to update the hyperparameters for the next run. Bayesian approaches are out of the scope for this course, so we will focus on random searching methods.\n",
    "  \n",
    "**Random search**\n",
    "  \n",
    "To implement a random search, we can use scikit-learn's method, `RandomizedSearchCV()`. This method will randomly select hyperparameters for each model run based on the user-defined hyperparameter space. `RandomizedSearchCV()` requires a dictionary of hyperparameters and their possible values. Here we have specified four max depths, nine max features, and nine `min_samples_split=` options. Using a grid search with these possible parameters would take 324 total model runs,as 4 * 9 * 9 = 324. However, using a random search, we can get similar results only using 30 or 40 runs.\n",
    "  \n",
    "<img src='../_images/grid-searching-hyper-tuning1.png' alt='img' width='740'>\n",
    "  \n",
    "**Random search parameters**\n",
    "  \n",
    "To use this method we need to specify a few other parameters. The parameter `n_iter=` specifies the number of models to run. estimator allows us to set the base model, such as a random forest regression model, and scoring allows us to specify a scoring function.\n",
    "  \n",
    "<img src='../_images/grid-searching-hyper-tuning2.png' alt='img' width='740'>\n",
    "  \n",
    "**Setting RandomizedSearchCV parameters**\n",
    "  \n",
    "Aside from setting up the parameter distributions, we need to create a model and the scoring function to use. The estimator specified here uses a `RandomForestRegression()` model, with 20 trees. We have also specified the mean absolute error to be the scoring function.\n",
    "  \n",
    "<img src='../_images/grid-searching-hyper-tuning3.png' alt='img' width='740'>\n",
    "  \n",
    "**RandomizedSearchCV implemented**\n",
    "  \n",
    "Let's finally implement the `RandomizedSearchCV` method. We use our model, `rfr`, the parameter distribution, specify to use 40 parameter sets and set the `cv=` value to 5. Ah! So hopefully, the \"CV\" on the end of this method helps us see why we are even discussing hyperparameter tuning in a course about model validation! The cross-validation techniques we have been discussing will be used with random searching to help us select the best model for our data. After all, if we test 40 different parameter sets, how do we determine which one is the best? And how do we appropriately compare their results? We have to use the techniques we have learned so far in this course.\n",
    "  \n",
    "<img src='../_images/grid-searching-hyper-tuning4.png' alt='img' width='740'>\n",
    "  \n",
    "**RandomizedSearchCV implemented**\n",
    "  \n",
    "After using `RandomizedSearchCV`, we should have a validated model that has better accuracy than using the base implementation of that model. To actually complete the random search, we use the `.fit()` method, just like any other scikit-learn model.\n",
    "  \n",
    "<img src='../_images/grid-searching-hyper-tuning5.png' alt='img' width='740'>\n",
    "  \n",
    "**Grid Search**  \n",
    "  \n",
    "Benefits  \n",
    "- Tests every possible combination\n",
    "  \n",
    "Drawbacks  \n",
    "- Additional hyperparameters increase training time exponentially  \n",
    "  \n",
    "Alternatives  \n",
    "- Random searching\n",
    "- Bayesian optimization\n",
    "  \n",
    "**Let's explore some examples!**\n",
    "  \n",
    "We will save exploring the output of the `RandomizedSearchCV()` method for the next lesson. For now, let's run through some examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for RandomizedSearch\n",
    "  \n",
    "Last semester your professor challenged your class to build a predictive model to predict final exam test scores. You tried running a few different models by randomly selecting hyperparameters. However, running each model required you to code it individually.\n",
    "  \n",
    "After learning about `RandomizedSearchCV()`, you're revisiting your professors challenge to build the best model. In this exercise, you will prepare the three necessary inputs for completing a random search.\n",
    "  \n",
    "1. Finalize the parameter dictionary by adding a list for the `max_depth=` parameter with options 2, 4, 6, and 8.\n",
    "2. Create a random forest regression model with ten trees and a `random_state=` of 1111.\n",
    "3. Create a mean squared error scorer to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "\n",
    "# Finish the dictionary by adding the max_depth parameter\n",
    "param_dist = {\n",
    "    \"max_depth\": [2, 4, 6, 8],\n",
    "    \"max_features\": [2, 4, 6, 8, 10],\n",
    "    \"min_samples_split\": [2, 4, 8, 16]\n",
    "}\n",
    "\n",
    "# Create a random forest regression model\n",
    "rfr = RandomForestRegressor(n_estimators=10, random_state=1111)\n",
    "\n",
    "# Create a scorer to use (use the mean squared error)\n",
    "scorer = make_scorer(mean_squared_error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! To use `RandomizedSearchCV()`, you need a distribution dictionary, an estimator, and a scorerâ€”once you've got these, you can run a random search to find the best parameters for your model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing RandomizedSearchCV\n",
    "  \n",
    "You are hoping that using a random search algorithm will help you improve predictions for a class assignment. You professor has challenged your class to predict the overall final exam average score.\n",
    "  \n",
    "In preparation for completing a random search, you have created:\n",
    "  \n",
    "- `param_dist`: the hyperparameter distributions\n",
    "- `rfr`: a random forest regression model\n",
    "- `scorer`: a scoring method to use\n",
    "  \n",
    "1. Load the method for conducting a random search in `sklearn`.\n",
    "2. Complete a random search by filling in the parameters: `estimator`, `param_distributions`, and `scoring`.\n",
    "3. Use 5-fold cross validation for this random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Build a random search using param_dist, rfr, and scorer\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rfr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring=scorer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Although it takes a lot of steps, hyperparameter tuning with random search is well worth it and can improve the accuracy of your models. Plus, you are already using cross-validation to validate your best model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting your final model\n",
    "  \n",
    "In this lesson, we will explore the output of a random search implementation and then select, and reuse our final model.\n",
    "  \n",
    "**Random search output**\n",
    "  \n",
    "To start, we will assume the variable `rs` is an implementation of `RandomizedSearchCV()` that has already been fit on data. Let's explore some of the key attributes of `rs`. Of course, the first attributes us eager-beavers have to check focuses on the best model. And so, the attributes have been so named `.best_score_`, ``best_params_`, and `.best_estimator_`. These attributes provide the results from the best model found in the random search, and you will use these attributes often.\n",
    "  \n",
    "<img src='../_images/random-search-attributes.png' alt='img' width='740'>\n",
    "  \n",
    "**Other attributes**\n",
    "  \n",
    "Of course, there are other attributes to explore. Perhaps the most useful will be `rs.cv_results_`, which contains a dictionary full of the cross-validation results. This dictionary includes keys such as \"`mean_test_scores`,\" which gives the average cross-validation test score for each run. The dictionary also contains the key \"`params`,\" which contains all of the selected parameters for each model run.\n",
    "  \n",
    "<img src='../_images/random-search-attributes1.png' alt='img' width='740'>\n",
    "  \n",
    "**Using `.cv_results_`**\n",
    "  \n",
    "We can use these attributes to create visuals of the output or make inferences on which hyperparameters are having the biggest impact on the model. For example, let's look at the mean test scores grouped by the maximum depth of the model. Here we grabbed the max depth from each of the 10 models, as well as the mean test score. We then created a `pandas` DataFrame and grouped the scores by the maximum depth. If we look at the output, a max depth of 2, 4, and even 6 all produced really low scores. However, a max depth of 8 and 10 almost achieved 90% accuracy.\n",
    "  \n",
    "<img src='../_images/random-search-attributes2.png' alt='img' width='740'>\n",
    "  \n",
    "**Other attributes continued**\n",
    "  \n",
    "There are a ton of ways to use the `.cv_results_` output to visualize the effect of each parameter. In the case, we just explored, it's probably best to use a larger max depth when running your models. These results might inspire you to rerun the random search with a slightly different hyperparameter space. Right now, we just want to select the best model from our random search.\n",
    "  \n",
    "<img src='../_images/random-search-attributes3.png' alt='img' width='740'>\n",
    "  \n",
    "**Selecting the best model**\n",
    "  \n",
    "However, you perform hyperparameter tuning; in the end, you'll need to select one final model. This may be the model with the best accuracy or the model with the highest precision or even recall. For now, let's assume we are going for the best mean squared error. The model with the lowest error from the cross-validation is our guess for the model that will perform the best on future data. The `.best_estimator_` attribute contains the model that performed the best during cross-validation.\n",
    "  \n",
    "<img src='../_images/random-search-attributes4.png' alt='img' width='740'>\n",
    "  \n",
    "**Comparing types of models**\n",
    "  \n",
    "As an aside, if you built different types of models, say a random forest and a gradient boosting model, you can test the accuracies of your final models on the test set that you held out. This gives an unbiased estimate and can help you make your final overall decision. In the case above, you would select the gradient boosting model as the final model because it had a lower mean squared error on the test data.\n",
    "  \n",
    "<img src='../_images/random-search-attributes5.png' alt='img' width='740'>\n",
    "  \n",
    "**Using `.best_estimator_`**\n",
    "  \n",
    "Let's use `.best_estimator_` from a random forest model. You can use the method `.predict()` on this estimator with new data just like any other scikit-learn model. You can check all of the parameters that were used by calling the `.get_params()` method or you can save the estimator as a pickle file using the joblib module for reuse later. This will allow you to load your model on a later date or to share your model with a colleague.\n",
    "  \n",
    "<img src='../_images/random-search-attributes6.png' alt='img' width='740'>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's work through a couple examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best classification accuracy\n",
    "  \n",
    "You are in a competition at work to build the best model for predicting the winner of a Tic-Tac-Toe game. You already ran a random search and saved the results of the most accurate model to rs.\n",
    "  \n",
    "Which parameter set produces the best classification accuracy?\n",
    "  \n",
    "Possible answers  \n",
    "  \n",
    "- [ ] {'max_depth': 8, 'min_samples_split': 4, 'n_estimators': 10}\n",
    "- [ ] {'max_depth': 2, 'min_samples_split': 4, 'n_estimators': 10}\n",
    "- [x] {'max_depth': 12, 'min_samples_split': 4, 'n_estimators': 20}\n",
    "- [ ] {'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
    "  \n",
    "Solution:  \n",
    "```python\n",
    "In[1]: rs.best_params_\n",
    "Out[1]: {'n_estimators': 20, 'min_samples_split': 4, 'max_depth': 12}\n",
    "```\n",
    "  \n",
    "Perfect! These parameters do produce the best testing accuracy. Good job! Remember, to reuse this model you can use `rs.best_estimator_`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the best precision model\n",
    "  \n",
    "Your boss has offered to pay for you to see three sports games this year. Of the 41 home games your favorite team plays, you want to ensure you go to three home games that they will definitely win. You build a model to decide which games your team will win.\n",
    "  \n",
    "To do this, you will build a random search algorithm and focus on model precision (to ensure your team wins). You also want to keep track of your best model and best parameters, so that you can use them again next year (if the model does well, of course). You have already decided on using the random forest classification model rfc and generated a parameter distribution param_dist.\n",
    "  \n",
    "1. Create a precision scorer, `precision` using `make_scorer(<scoring_function>)`.\n",
    "2. Complete the random search method by using `rfc` and `param_dist`.\n",
    "3. Use `rs.cv_results_` to print the mean test scores.\n",
    "4. Print the best overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = pd.read_csv('../_datasets/sports.csv')\n",
    "\n",
    "# X/y split\n",
    "X = sports.drop('win', axis=1)\n",
    "y = sports['win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Model instantiation\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Parameter dictionary\n",
    "param_dist = {\n",
    "    'max_depth': range(2, 12, 2),\n",
    "    'min_samples_split': range(2, 12, 2),\n",
    "    'n_estimators': [10, 25, 50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for each run was: [0.89382387 0.76399271 0.66760857 0.8698715  0.87361949 0.83619829\n",
      " 0.68623613 0.81670331 0.91696302 0.8998569 ].\n",
      "The best accuracy for a single model was: 0.9169630160924696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "# Create a precision scorer\n",
    "precision = make_scorer(precision_score)\n",
    "\n",
    "# Finalize the random search\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=rfc,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=precision,\n",
    "    cv=5, \n",
    "    n_iter=10,\n",
    "    random_state=1111\n",
    ")\n",
    "\n",
    "# Fitting model\n",
    "rs.fit(X, y)\n",
    "\n",
    "# Print the mean test scores:\n",
    "print('The accuracy for each run was: {}.'.format(rs.cv_results_['mean_test_score']))\n",
    "# Print the best model scores:\n",
    "print('The best accuracy for a single model was: {}'.format(rs.best_score_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - Your model's precision was 92%! The best model accurately predicts a winning game 92% of the time. If you look at the mean test scores, you can tell some of the other parameter sets did really poorly. Also, since you used cross-validation, you can be confident in your predictions. Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
