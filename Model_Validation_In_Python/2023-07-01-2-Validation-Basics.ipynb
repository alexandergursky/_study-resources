{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Basics\n",
    "  \n",
    "This chapter focuses on the basics of model validation. From splitting data into training, validation, and testing datasets, to creating an understanding of the bias-variance tradeoff, we build the foundation for the techniques of K-Fold and Leave-One-Out validation practiced in chapter three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train, test, and validation datasets\n",
    "  \n",
    "Let's get started with creating training, testing, and validation datasets.\n",
    "  \n",
    "**Traditional train/test split**\n",
    "  \n",
    "In the first few lessons, we called data \"seen\" data if it was used for model fitting, while \"unseen\" data described the data we did not train our model on. In model validation, we use holdout samples to replicate this idea. We define a holdout dataset as any data that is not used for training and is only used to assess model performance. The available data is split into two datasets. One used for training, and one that is simply off limits while we are training our models, called a test (or holdout) dataset. This step is vital to model validation and is the number one step you can take to ensure your model's performance.\n",
    "  \n",
    "<img src='../_images/train-test-split-holdout-validation-sets.png' text='alt text' width='700'>\n",
    "  \n",
    "**Dataset definitions and ratios**\n",
    "  \n",
    "We use the holdout sample as a testing dataset so that we can have an unbiased estimate for our model's performance after we are completely done training. Generally, a good rule of thumb is using an 80/20 split. This equates to setting aside twenty percent of the data for the test set and using the rest for training. You might choose to use more training data when the overall data is limited, or less training data if the modeling method is computationally expensive.\n",
    "  \n",
    "<img src='../_images/train-test-split-holdout-validation-sets1.png' text='alt text' width='700'>\n",
    "  \n",
    "**The X and y datasets**\n",
    "  \n",
    "Before we use scikit-learn's holdout creation function `train_test_split()`, we will use the `tic_tac_toe` dataset and create an `X` dataset with the predictive data, and a `y` dataset with just the responses. The first nine columns of `tic_tac_toe` can be used for training, while the 10th column contains the response values. As a quick aside, classification models for categorical values, such as those found in the `tic_tac_toe` dataset, require dummy variables. \n",
    "  \n",
    "<img src='../_images/train-test-split-holdout-validation-sets2.png' text='alt text' width='740'>\n",
    "  \n",
    "**Creating holdout samples**\n",
    "  \n",
    "The `train_test_split()` function is straightforward. We split both the `X` and the `y` datasets, into both a train and a test dataset. This function has a few parameters that we will use. `test_size=` takes either a float or an integer and specifies how big the test set should be. If `test_size=` is blank, you can instead use `train_size=` to set the size of the training set. And finally, `random_state=` allows for setting the model seed and helps maintain reproducibility.\n",
    "  \n",
    "**Dataset for preliminary testing?**\n",
    "  \n",
    "We know that the test set is off limits until we are completely done training, but what do we do when testing model parameters? For example, if we run a random forest model with 100 trees and one with 1000 trees, which dataset do we use to test these results?\n",
    "  \n",
    "**Holdout samples for parameter tuning**\n",
    "  \n",
    "When testing parameters, tuning hyper-parameters, or anytime we are frequently evaluating model performance we need to create a second holdout sample, called the validation dataset. For this dataset, the available data is the original training dataset, which is then split in the same manner used to split the original complete dataset. We use the validation sample to assess our model's performance when using different parameter values.\n",
    "  \n",
    "<img src='../_images/train-test-split-holdout-validation-sets3.png' text='alt text' width='740'>\n",
    "  \n",
    "**Train, validation, test continued**\n",
    "  \n",
    "To create both holdout samples, the testing, and the validation datasets, we use scikit-learn's `train_test_split()` function twice. The first call will create training and testing datasets like normal. The second call we split this so-called temporary training dataset into the final training and validation datasets. In this example, we first used an 80/20 split to create the test set. With the 80% training dataset, we used a 75/25 split to create a validation dataset. Leaving us with 60% of the data for training, 20% for validation, and 20% for testing.\n",
    "  \n",
    "<img src='../_images/train-test-split-holdout-validation-sets4.png' text='alt text' width='740'>\n",
    "\n",
    "**It's holdout time**\n",
    "  \n",
    "Let's practice making holdout sets to use in our models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one holdout set\n",
    "  \n",
    "Your boss has asked you to create a simple random forest model on the `tic_tac_toe` dataset. She doesn't want you to spend much time selecting parameters; rather she wants to know how well the model will perform on future data. For future Tic-Tac-Toe games, it would be nice to know if your model can predict which player will win.\n",
    "  \n",
    "The dataset `tic_tac_toe` has been loaded for your use.\n",
    "  \n",
    "Note that in Python, `=\\` indicates the code was too long for one line and has been split across two lines.\n",
    "  \n",
    "1. Create the `X` dataset by creating dummy variables for all of the categorical columns.\n",
    "2. Split `X` and `y` into train (`X_train`, `y_train`) and test (`X_test`, `y_test`) datasets.\n",
    "3. Split the datasets using 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top-Left</th>\n",
       "      <th>Top-Middle</th>\n",
       "      <th>Top-Right</th>\n",
       "      <th>Middle-Left</th>\n",
       "      <th>Middle-Middle</th>\n",
       "      <th>Middle-Right</th>\n",
       "      <th>Bottom-Left</th>\n",
       "      <th>Bottom-Middle</th>\n",
       "      <th>Bottom-Right</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Top-Left Top-Middle Top-Right Middle-Left Middle-Middle Middle-Right  \\\n",
       "0        x          x         x           x             o            o   \n",
       "1        x          x         x           x             o            o   \n",
       "2        x          x         x           x             o            o   \n",
       "3        x          x         x           x             o            o   \n",
       "4        x          x         x           x             o            o   \n",
       "\n",
       "  Bottom-Left Bottom-Middle Bottom-Right     Class  \n",
       "0           x             o            o  positive  \n",
       "1           o             x            o  positive  \n",
       "2           o             o            x  positive  \n",
       "3           o             b            b  positive  \n",
       "4           b             o            b  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic_tac_toe = pd.read_csv('../_datasets/tic-tac-toe.csv')\n",
    "tic_tac_toe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (958, 27)\n",
      "y shape:  (958,)\n",
      "\n",
      "X_train shape:  (862, 27)\n",
      "X_test shape:   (96, 27)\n",
      "y_train shape:  (862,)\n",
      "y_test shape:   (96,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Create dummy variables using pandas\n",
    "X = pd.get_dummies(tic_tac_toe.iloc[:, 0:9])  # All observations and the 0-8 (9 total) features\n",
    "y = tic_tac_toe.iloc[:, 9]  # All observations for the 10th (index=9) column\n",
    "\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)\n",
    "\n",
    "# Create training and testing datasets, Use 10% for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)\n",
    "\n",
    "\n",
    "print('\\nX_train shape: ', X_train.shape)\n",
    "print('X_test shape:  ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape:  ', y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, without the holdout set, you cannot truly validate a model. Let's move on to creating two holdout sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two holdout sets\n",
    "  \n",
    "You recently created a simple random forest model to predict Tic-Tac-Toe game wins for your boss, and at her request, you did not do any parameter tuning. Unfortunately, the overall model accuracy was too low for her standards. This time around, she has asked you to focus on model performance.\n",
    "  \n",
    "Before you start testing different models and parameter sets, you will need to split the data into training, validation, and testing datasets. Remember that after splitting the data into training and testing datasets, the validation dataset is created by splitting the training dataset.\n",
    "  \n",
    "The datasets `X` and `y` have been loaded for your use.\n",
    "  \n",
    "1. Create temporary datasets and testing datasets (`X_test`, `y_test`). Use 20% of the overall data for the testing datasets.\n",
    "2. Using the temporary datasets (`X_temp`, `y_temp`), create training (`X_train`, `y_train`) and validation (`X_val`, `y_val`) datasets.\n",
    "3. Use 25% of the temporary data for the validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary training and final testing datasets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "# Create the final training and validation datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have training, validation, and testing datasets, but do you know *when* you need both validation and testing datasets? Keep going! The next exercise will help make sure you understand when to use validation datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use holdout sets\n",
    "  \n",
    "It is important to understand when you would use three datasets (training, validation, and testing) instead of two (training and testing). There is no point in creating an additional dataset split if you are not going to use it.\n",
    "  \n",
    "When should you consider using training, validation, and testing datasets?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] When there is a lot of data. Splitting into three sets helps speed up modeling.\n",
    "- [x] When testing parameters, tuning hyper-parameters, or anytime you are frequently evaluating model performance.\n",
    "- [ ] Only when you are running regression and not classification models.\n",
    "- [ ] Only when you are running classification and not regression models.\n",
    "  \n",
    "Correct! Anytime we are evaluating model performance repeatedly we need to create training, validation, and testing datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy metrics: regression models\n",
    "  \n",
    "Now that we have learned about holdout samples let's discuss accuracy metrics used when validating models — starting with regression models.\n",
    "  \n",
    "**Regression models**\n",
    "  \n",
    "Remember, regression models are built for continuous variables. This could be predicting the number of points a player will score tomorrow, or the number of puppies a dog is about to have!\n",
    "  \n",
    "**Mean absolute error (MAE)**\n",
    "  \n",
    "To assess the performance of a regression model, we can use the mean absolute error. It is the simplest and most intuitive error metric and is the average absolute difference between the predictions ($\\hat y_i$) and the actual values ($y_i$). If your dog had six puppies, but you had predicted only four, the absolute difference would be two. This metric treats all points equally and is not sensitive to outliers. When dealing with applications where we don't want large errors to have a major impact, the mean absolute error can be used. An example could be predicting your car's monthly gas bill, when an outlier may have been caused by a one-time road trip.\n",
    "  \n",
    "- Simplest and most intuitive metric\n",
    "- Treats all points equally\n",
    "- Not sensitive to outliers\n",
    "  \n",
    "$formula:$  \n",
    "  \n",
    "$\\Large {MAE}= \\frac {\\sum_{i=1}^{n} {|y_i - \\hat y_i|}}{n}$\n",
    "  \n",
    "$let:$\n",
    "\n",
    "$MAE =$ mean absolute error  \n",
    "$n =$ number of data points  \n",
    "$y_{i} =$ observed/true values  \n",
    "$\\hat{y}_{i} =$ predicted values  \n",
    "  \n",
    "**Mean squared error (MSE)**\n",
    "  \n",
    "Next is the mean squared error (MSE). It is the most widely used regression error metric for regression models. It is calculated similarly to the mean absolute error, but this time we square the difference term. The MSE allows larger errors to have a larger impact on the model. Using the previous car example, if you knew once a year you might go on a road trip, you might expect to occasionally have a large error and would want your model to pick up on these trips.\n",
    "  \n",
    "- Most widely used regression metric\n",
    "- Allows outlier errors to contribute more to the overall error\n",
    "- Random family road trips could lead to large errors in predictions\n",
    "\n",
    "$formula:$  \n",
    "  \n",
    "$\\Large {MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2$\n",
    "  \n",
    "$let:$\n",
    "  \n",
    "$MSE =$ mean squared error  \n",
    "$n =$ number of data points  \n",
    "$y_{i} =$ observed/true values  \n",
    "$\\hat{y}_{i} =$ predicted values  \n",
    "  \n",
    "**MAE vs. MSE**\n",
    "  \n",
    "Picking between the MAE and the MSE comes down to the application. These results are in different units though and should not be directly compared!\n",
    "  \n",
    "**Mean absolute error**\n",
    "  \n",
    "To practice these metrics, let's use the ultimate Halloween candy data dataset. Here we are predicting the win-percentage of candies in head-to-head match-ups with other candies. Let's assume we have already fit a random forest model and calculated predictions for the test dataset. For the mean absolute error, we can calculate this two ways. A manual calculation, which takes the sum of the absolute differences and divides by the total number of observations, or we can use scikit-learn's `mean_absolute_error()` function. We provide an array of the actual values, followed by an array of the predictions. Both methods produce a single value of 9.99 as the output. We are covering the manual calculations for these functions to understand the results of these error metrics. Notice that we are looking at the test data accuracy. This error means that we are about 10 percentage-points off on average when predicting the win-percentage. As win-percentages range from 0 to 1, this is fairly good.\n",
    "  \n",
    "<img src='../_images/mae-mse-test-metrics.png' text='alt text' width= '740'>\n",
    "  \n",
    "**Mean squared error**\n",
    "  \n",
    "For the mean squared error, we can calculate this manually or with the `mean_squared_error()` function. Both methods produce a value of 141.4. In this example, the mean squared error is a more appropriate accuracy metric, as we want outliers to have more of an impact on the model's performance. For example, if one chocolate bar really underperforms, there may be attributes of that chocolate bar that truly matter other than it being chocolate.\n",
    "  \n",
    "<img src='../_images/mae-mse-test-metrics1.png' text='alt text' width= '740'>\n",
    "\n",
    "**Accuracy for a subset of data**\n",
    "  \n",
    "Sometimes we want to know a model's accuracy for a specific subset, such as how this model performs on only chocolate candies. If column 1 in our test set has 1's for candies containing chocolate, and 0; otherwise, we filter the test array based on these values and run the accuracy metrics. Since the chocolate candies had errors of less than 9 and the non-chocolate candies had errors of 11, the model performed better on chocolate candy.\n",
    "  \n",
    "<img src='../_images/mae-mse-test-metrics2.png' text='alt text' width= '740'>\n",
    "  \n",
    "**Let's practice**\n",
    "  \n",
    "Let's work through a couple of examples on regression accuracy metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error\n",
    "  \n",
    "Communicating modeling results can be difficult. However, most clients understand that on average, a predictive model was off by some number. This makes explaining the mean absolute error easy. For example, when predicting the number of wins for a basketball team, if you predict 42, and they end up with 40, you can easily explain that the error was two wins.\n",
    "  \n",
    "In this exercise, you are interviewing for a new position and are provided with two arrays. `y_test`, the true number of wins for all 30 NBA teams in 2017 and `predictions`, which contains a prediction for each team. To test your understanding, you are asked to both manually calculate the MAE and use sklearn.\n",
    "  \n",
    "1. Manually calculate the MAE using n as the number of observations predicted.\n",
    "2. Calculate the MAE using sklearn.\n",
    "3. Print off both accuracy values using the print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsed this from the exercise to simulate, unattainable otherwise.\n",
    "y_test = np.array([53, 51, 51, 49, 43, 42, 42, 41, 41, 37, 36, 31, 29, 28, 20, 67, 61,\n",
    "       55, 51, 51, 47, 43, 41, 40, 34, 33, 32, 31, 26, 24])\n",
    "\n",
    "predictions = np.array([60, 62, 42, 42, 30, 50, 52, 42, 44, 35, 30, 30, 35, 40, 15, 72, 58,\n",
    "       60, 40, 42, 45, 46, 40, 35, 25, 40, 20, 34, 25, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a manual calculation, the error is 5.9\n",
      "Using scikit-learn, the error is 5.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# Manually calculate the MAE\n",
    "n = len(predictions)\n",
    "mae_one = sum(abs(y_test - predictions)) / n\n",
    "print('With a manual calculation, the error is {}'.format(mae_one))\n",
    "\n",
    "# Use scikit-learn to calculate the MAE\n",
    "mae_two = mean_absolute_error(y_test, predictions)\n",
    "print('Using scikit-learn, the error is {}'.format(mae_two))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions were about six wins off on average. This isn't too bad considering NBA teams play 82 games a year. Let's see how these errors would look if you used the mean squared error instead."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error\n",
    "  \n",
    "Let's focus on the 2017 NBA predictions again. Every year, there are at least a couple of NBA teams that win way more games than expected. If you use the MAE, this accuracy metric does not reflect the bad predictions as much as if you use the MSE. Squaring the large errors from bad predictions will make the accuracy look worse.\n",
    "  \n",
    "In this example, NBA executives want to better predict team wins. You will use the mean squared error to calculate the prediction error. The actual wins are loaded as `y_test` and the predictions as `predictions`.\n",
    "  \n",
    "1. Manually calculate the MSE.\n",
    "2. Calculate the MSE using sklearn.\n",
    "3. Print off both accuracy values using the print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a manual calculation, the error is 49.1\n",
      "Using scikit-learn, the error is 49.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Finish the manual calculation of the MSE\n",
    "n = len(predictions)\n",
    "mse_one = sum((y_test - predictions) ** 2) / n\n",
    "print('With a manual calculation, the error is {}'.format(mse_one))\n",
    "\n",
    "# Use the scikit-learn function to calculate MSE\n",
    "mse_two = mean_squared_error(y_test, predictions)\n",
    "print('Using scikit-learn, the error is {}'.format(mse_two))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! If you run any additional models, you will try to beat an MSE of 49.1, which is the average squared error of using your model. Although the MSE is not as interpretable as the MAE, it will help us select a model that has fewer 'large' errors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on data subsets\n",
    "  \n",
    "In professional basketball, there are two conferences, the East and the West. Coaches and fans often only care about how teams in their own conference will do this year.\n",
    "  \n",
    "You have been working on an NBA prediction model and would like to determine if the predictions were better for the East or West conference. You added a third array to your data called `labels`, which contains an \"E\" for the East teams, and a \"W\" for the West. `y_test` and `predictions` have again been loaded for your use.\n",
    "  \n",
    "1. Create an array `east_teams` that can be used to filter `labels` to East conference teams.\n",
    "2. Create the arrays `true_east` and preds_east by filtering the arrays `y_test` and `predictions`.\n",
    "3. Use the `print()` statements to print the MAE (using scikit-learn) for the East conference. The `mean_absolute_erro`r function has been loaded `as mae`.\n",
    "4. The variable `west_error` contains the MAE for the West teams. Use the `print()` statement to print out the Western conference MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snipped from the exercise, unattainable otherwise\n",
    "labels= np.array(['E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E',\n",
    "       'E', 'E', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W',\n",
    "       'W', 'W', 'W', 'W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for East teams is 6.733333333333333\n",
      "The MAE for West teams is 5.066666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "\n",
    "# Find the East conference teams, ndarray of boolean values (MASK)\n",
    "east_teams = labels == 'E'\n",
    "\n",
    "# Create arrays for the true and predicted values\n",
    "true_east = y_test[east_teams]\n",
    "preds_east = predictions[east_teams]\n",
    "\n",
    "# Find the West conference teams, ndarray of boolean values (MASK)\n",
    "west_teams = labels == 'W'\n",
    "\n",
    "# Create arrays for the true and predicted values\n",
    "true_west = y_test[west_teams]\n",
    "preds_west = predictions[west_teams]\n",
    "\n",
    "# Print the accuracy metrics\n",
    "print('The MAE for East teams is {}'.format(mae(true_east, preds_east)))\n",
    "\n",
    "# Print the west accuracy\n",
    "print('The MAE for West teams is {}'.format(mae(true_west, preds_west)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the past few seasons, the Western teams have generally won the same number of games as the experts have predicted. Teams in the East are just not as predictable as those in the West.\n",
    "  \n",
    "NOTE: The statement in the exercise \"It looks like the Western conference predictions were about two games better on average\" is an incorrect statement while also being misleading. You can not interprete evaluation MAE, MAPE, MSE, or RMSE in such a way, they are used only as distance points between ones-self."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics\n",
    "  \n",
    "We already understand classification models; now let's look at their accuracy metrics.\n",
    "  \n",
    "**Classification metrics**\n",
    "  \n",
    "Classification accuracy metrics are quite a bit different than regression ones. Remember, with classification models; we are predicting what category an observation falls into. There are a lot of accuracy metrics available. There is precision, recall, accuracy, specificity, F1-Score, alternate forms of the F1-score, and several others.\n",
    "  \n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Metric</th>\n",
    "            <th>Purpose</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Precision</td>\n",
    "            <td>Measures the proportion of correctly predicted positive instances out of the total predicted positive instances. It focuses on the accuracy of positive predictions.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Recall (Sensitivity)</td>\n",
    "            <td>Measures the proportion of correctly predicted positive instances out of the total actual positive instances. It focuses on the coverage of positive instances.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Accuracy</td>\n",
    "            <td>Measures the proportion of correctly predicted instances (both positive and negative) out of the total instances. It provides an overall measure of model performance.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Specificity</td>\n",
    "            <td>Measures the proportion of correctly predicted negative instances out of the total actual negative instances. It focuses on the coverage of negative instances.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>F1-Score (and its variations)</td>\n",
    "            <td>Combines precision and recall into a single metric, providing a balanced measure of model performance. It is useful when the data is imbalanced or when both precision and recall are important.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "  \n",
    "**Classification metrics**\n",
    "  \n",
    "We will focus on precision, recall, and accuracy. As each of these are easy to understand and have very practical applications. One way to calculate these metrics is to use the values from the confusion matrix.\n",
    "  \n",
    "**Confusion matrix**\n",
    "  \n",
    "When making predictions, especially if there is a binary outcome, this matrix is one of the first outputs you should review. When we have a binary outcome, the confusion matrix is a 2x2 matrix that shows how your predictions faired across the two outcomes. For example, for predictions of 0 that were actually 0 (or true negatives), we look at the 0, 0 square of the matrix. All of the accuracy metrics from the previous slide can be calculated using the values from this matrix, and it is a great way to visualize the initial results of your classification model.\n",
    "  \n",
    "<html>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Predicted Negative (0)</th>\n",
    "            <th>Predicted Positive (1)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Actual Negative (0)</td>\n",
    "            <td>True Negative (TN) (A0P0)</td>\n",
    "            <td>False Positive (FP) (A0P1)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Actual Positive (1)</td>\n",
    "            <td>False Negative (FN) (A1P0)</td>\n",
    "            <td>True Positive (TP) (A1P1)</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "  \n",
    "**Create confusion matrix with scikit-learn**\n",
    "  \n",
    "We can create a confusion matrix using scikit-learn's function `confusion_matrix()`. When dealing with binary data, this will print out a 2x2 array which represents the confusion matrix. In this matrix, the row index represents the true category, and the column index represents the predicted category. Therefore, the 1, 0 entry of the array represents the number of true 1s that were predicted to be 0, or 8 in this example.\n",
    "  \n",
    "<img src='../_images/confusion-matrix-in-sklearn-evaluation.png' text='alt text' width='740'>\n",
    "  \n",
    "**Accuracy**\n",
    "  \n",
    "Accuracy is the easiest metric to understand and represents the overall ability of your model to correctly predict the correct classification. Using the confusion matrix, we add the values that were predicted 0 and actually are 0 (which are called true negatives), to the values predicted to be 1 that are 1 (called true positives), and then divide by the total number of observations. In this case, our accuracy was 85%. In this example, you can associate a true positive as predicted 1's that are also actually 1's. However, if your categories were win or loss, you might associate a true positive as the number of predicted wins that were actually wins.\n",
    "  \n",
    "<img src='../_images/confusion-matrix-in-sklearn-evaluation1.png' text='alt text' width='720'>\n",
    "  \n",
    "**Precision**\n",
    "  \n",
    "Next is precision or the number of true positives out of all predicted positive values. We correctly predicted 62 true values but also predicted 7 false positives. Therefore, the precision is 62 divided by 69. Precision is used when we don't want to overpredict positive values. If it cost $2,000 to fly-in potential new employee's, a company may only have on-campus interviews with individuals that they really believe are going to join their company. In the example data, almost 9 out of 10 predicted 1's would have joined the company.\n",
    "  \n",
    "<img src='../_images/confusion-matrix-in-sklearn-evaluation2.png' text='alt text' width='720'>\n",
    "  \n",
    "**Recall**\n",
    "  \n",
    "The recall metric is about finding all positive values. Here we correctly predicted 62 true positives and had 8 false negatives. Our recall is 62 out of 70. Recall is used when we can't afford to miss any positive values. For example, even if a patient has a small chance of having cancer, we may want to give them additional tests. The cost of missing a patient who has cancer is far greater than the cost of additional screenings for that patient.\n",
    "  \n",
    "<img src='../_images/confusion-matrix-in-sklearn-evaluation3.png' text='alt text' width='720'>\n",
    "  \n",
    "**Accuracy, precision, recall**\n",
    "  \n",
    "Accuracy, precision, and recall are called similarly. Use the desired accuracy metric function and provide the true and predicted values. A single value will be produced as a result. In this example, we got the same values that we calculated using the confusion matrix.\n",
    "  \n",
    "<img src='../_images/confusion-matrix-in-sklearn-evaluation4.png' text='alt text' width='740'>\n",
    "  \n",
    "**Practice time**\n",
    "  \n",
    "Let's work through a couple of examples using these accuracy metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices\n",
    "  \n",
    "Confusion matrices are a great way to start exploring your model's accuracy. They provide the values needed to calculate a wide range of metrics, including sensitivity, specificity, and the F1-score.\n",
    "  \n",
    "You have built a classification model to predict if a person has a broken arm based on an X-ray image. On the testing set, you have the following confusion matrix:\n",
    "  \n",
    "<html>\n",
    "<body>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th>Predicted Negative (0)</th>\n",
    "            <th>Predicted Positive (1)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Actual Negative (0)</td>\n",
    "            <td>324 (True Negative) (A0P0)</td>\n",
    "            <td>15 (False Positive) (A0P1)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Actual Positive (1)</td>\n",
    "            <td>123 (False Negative) (A1P0)</td>\n",
    "            <td>491 (True Positive) (A1P1)</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "  \n",
    "1. Use the confusion matrix to calculate overall accuracy.\n",
    "2. Use the confusion matrix to calculate precision and recall.\n",
    "3. Use the three print statements to print each accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy is  0.86\n",
      "The precision is  0.97\n",
      "The recall is  0.80\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the accuracy (TN+TP)/(TN+FP+FN+TP)\n",
    "accuracy = (324 + 491) / (953)\n",
    "print(\"The overall accuracy is {0: 0.2f}\".format(accuracy))\n",
    "\n",
    "# Calculate and print the precision (TP)/(TP+FP)\n",
    "precision = (491) / (15 + 491)\n",
    "print(\"The precision is {0: 0.2f}\".format(precision))\n",
    "\n",
    "# Calculate and print the recall (TP)/(FN+TP)\n",
    "recall = (491) / (123 + 491)\n",
    "print(\"The recall is {0: 0.2f}\".format(recall))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, a true positive is a picture of an actual broken arm that was also predicted to be broken. Doctors are okay with a few additional false positives (predicted broken, not actually broken), as long as you don't miss anyone who needs immediate medical attention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices, again\n",
    "  \n",
    "Creating a confusion matrix in Python is simple. The biggest challenge will be making sure you understand the orientation of the matrix. This exercise makes sure you understand the sklearn implementation of confusion matrices. Here, you have created a random forest model using the `tic_tac_toe` dataset `rfc` to predict outcomes of 0 (loss) or 1 (a win) for Player One.\n",
    "  \n",
    "Note: If you read about confusion matrices on another website or for another programming language, the values might be reversed.\n",
    "  \n",
    "1. Import sklearn's function for creating confusion matrices.\n",
    "2. Using the model `rfc`, create category predictions on the test set `X_test`.\n",
    "3. Create a confusion matrix using sklearn.\n",
    "4. Print the value from `cm` that represents the actual 1s that were predicted as 1s (true positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required dataset\n",
    "tic_tac_toe = pd.read_csv('../_datasets/tic-tac-toe.csv')\n",
    "\n",
    "# Create dummy variables using pandas\n",
    "X = pd.get_dummies(tic_tac_toe.iloc[:, 0:9])    # All observations, features: [0,1,2,3,4,5,6,7,8] = 9\n",
    "y = tic_tac_toe.iloc[:, 9]                      # Extract the target, all observations, feature 9\n",
    "y = tic_tac_toe['Class'].apply(lambda x: 1 if x == 'positive' else 0)  # Feature label \"Class\" convert to one-hot\n",
    "\n",
    "# Create training and testing datasets, Use 10% for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=500, random_state=1111)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=500, random_state=1111)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, random_state=1111)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Model instantiation\n",
    "rfc = RandomForestClassifier(n_estimators=500, random_state=1111)  # 500 decision trees\n",
    "\n",
    "# Fitting model\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28  2]\n",
      " [ 0 66]]\n",
      "The number of true negatives is: 28\n",
      "The number of false positives is: 2\n",
      "The number of false negatives is: 0\n",
      "The number of true positives is: 66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Create predictions, y_pred\n",
    "test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, test_predictions)\n",
    "print(cm)\n",
    "\n",
    "# Print the true positives (actual 1s that were predicted 1s)\n",
    "print('The number of true negatives is: {}'.format(cm[0, 0]))\n",
    "print('The number of false positives is: {}'.format(cm[0, 1]))\n",
    "print('The number of false negatives is: {}'.format(cm[1, 0]))\n",
    "print('The number of true positives is: {}'.format(cm[1, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row 1, column 1 represents the number of actual 1s that were predicted 1s (the true positives). Always make sure you understand the orientation of the confusion matrix before you start using it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision vs. recall\n",
    "  \n",
    "The accuracy metrics you use to evaluate your model should always be based on the specific application. For this example, let's assume you are a really sore loser when it comes to playing Tic-Tac-Toe, but only when you are certain that you are going to win.\n",
    "  \n",
    "Choose the most appropriate accuracy metric, either precision or recall, to complete this example. But remember, if you think you are going to win, you better win!\n",
    "  \n",
    "Use `rfc`, which is a random forest classification model built on the `tic_tac_toe` dataset.\n",
    "  \n",
    "1. Import the precision or the recall metric for sklearn. Only one method is correct for the given context.\n",
    "2. Calculate the precision or recall using `y_test` for the true values and `test_predictions` for the predictions.\n",
    "3. Print the final score based on your selected metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision value is 0.97, The recall value is 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "# y_pred\n",
    "test_predictions = rfc.predict(X_test)\n",
    "\n",
    "# Create precision score based on the metric\n",
    "p_score = precision_score(y_test, test_predictions)\n",
    "r_score = recall_score(y_test, test_predictions)\n",
    "\n",
    "# Print the final result\n",
    "print('The precision value is {0:.2f}, The recall value is {1:.2f}'.format(p_score, r_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the correct metric here. Sore-losers can't stand losing when they are certain they will win! For that reason, our model needs to be as precise as possible.\n",
    "  \n",
    "Precision is specifically focused on evaluating the reliability of positive predictions. It answers the question, \"Of all the instances predicted as positive, how many are actually positive?\" A higher precision value indicates a lower rate of false positives, meaning that the model is making fewer incorrect positive predictions.\n",
    "\n",
    "Precision is particularly useful in scenarios where the cost or consequences of false positives are high, such as in medical diagnosis or spam email detection. A high precision value implies that when the model predicts an instance as positive, it is likely to be correct."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bias-variance tradeoff\n",
    "  \n",
    "Let's try to identify when we have a good fitting model.\n",
    "  \n",
    "**Variance**\n",
    "  \n",
    "One way to do this is to consider bias and variance. Variance occurs when a model pays too close attention to the training data and fails to generalize to the testing data. These models perform well on only the training data, but not the testing data, and are considered to be overfit.\n",
    "  \n",
    "- Variance: Following the training data too closely\n",
    "- Fails to generalize to the test data\n",
    "- Low training error, but high testing error\n",
    "- Occurs when models are overfit and have high complexity\n",
    "  \n",
    "**Overfitting models (high variance)**\n",
    "  \n",
    "Overfitting occurs when our model starts to attach meaning to the noise in the training data. In this graphic, you can see the natural quadratic shape of the orange dots. However, our blue prediction line is hugging the data and would likely not extend well to new orange dots. Overfitting is easy to identify though, as the training error will be a lot lower than the testing error.\n",
    "  \n",
    "<img src='../_images/the-bias-trade-off-ml-validation.png' text='alt text' width='700'>\n",
    "  \n",
    "**Bias**\n",
    "  \n",
    "The second term, Bias, occurs when the model fails to find the relationships between the data and the response value. Bias leads to high errors on both the training and testing datasets and is associated with an underfit model.\n",
    "  \n",
    "- Bias: Failing to find the relationship between the data and the response\n",
    "- High training error, high testing error\n",
    "- Occurs when the model is underfit\n",
    "  \n",
    "**Underfitting models (high bias)**\n",
    "  \n",
    "Underfitting occurs when the model could not find the underlying patterns available in the data. This might happen if we don't have enough trees or the trees aren't deep enough. In this example, we have the average of the actual values acting as our prediction. Underfitting is more difficult to identify because the training and testing errors will both be high, and it's difficult to know if we got the most out of the data, or if we can improve the testing error.\n",
    "  \n",
    "<img src='../_images/the-bias-trade-off-ml-validation1.png' text='alt text' width='700'>\n",
    "  \n",
    "**Optimal performance**\n",
    "  \n",
    "When our model is getting the most out of the training data, while still performing on the testing data, we have optimal performance. Notice how the blue line is matching the natural quadratic shape of the data and that it is not touching every orange dot. The blue line is a well fit prediction line for future data. So how do we tell if we have a good fit, or if we are just underfitting?\n",
    "  \n",
    "<img src='../_images/the-bias-trade-off-ml-validation2.png' text='alt text' width='700'>\n",
    "  \n",
    "**Parameters causing over/under fitting**\n",
    "  \n",
    "For random forest models, some parameters that affect performance are `max_depth=` and `max_features=`. One way to check for a poorly fit model is to try additional parameter sets and check both the training and testing error metrics. Notice that the overall training accuracy is a bit higher than the testing accuracy. We might have some past experience with this type of data that suggests we can expect a much higher accuracy and we conclude that we are probably underfitting. As you run more random forest models, you will get a better sense of which parameters you should tweak. But in this case, a `max_depth=4` is probably not deep enough.\n",
    "  \n",
    "<img src='../_images/the-bias-trade-off-ml-validation3.png' text='alt text' width='740'>\n",
    "  \n",
    "**Parameters continued**\n",
    "  \n",
    "This time around, we may have made the `max_depth=` too large and are overfitting. Achieving 100% accuracy on the training dataset while only getting 83% on testing is a clear sign that we are overfitting. We always compare how well the model performed on the data it has seen to the data it has not seen.\n",
    "  \n",
    "<img src='../_images/the-bias-trade-off-ml-validation4.png' text='alt text' width='740'>\n",
    "  \n",
    "**Parameters continued**\n",
    "  \n",
    "Finally, a `max_depth=10` has brought the testing accuracy up, while also bringing it closer to the training accuracy. Indicating that the model is generalizing well to new data while still performing really well overall. We will never know if 86% is the best accuracy possible for this dataset. However, we have explored various parameter sets, checked the difference between the testing and training errors at each stage, and improved our accuracy by almost 10% over the first model that we created.\n",
    "  \n",
    "<img src='../_images/the-bias-trade-off-ml-validation5.png' text='alt text' width='740'>\n",
    "  \n",
    "**Remember, only you can prevent overfitting!**\n",
    "  \n",
    "We will explore parameter tuning later in this course. For now, let's see how changing a single parameter value affects model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error due to under/over-fitting\n",
    "  \n",
    "The candy dataset is prime for overfitting. With only 85 observations, if you use 20% for the testing dataset, you are losing a lot of vital data that could be used for modeling. Imagine the scenario where most of the chocolate candies ended up in the training data and very few in the holdout sample. Our model might only see that chocolate is a vital factor, but fail to find that other attributes are also important. In this exercise, you'll explore how using too many features (columns) in a random forest model can lead to overfitting.\n",
    "  \n",
    "A feature represents which columns of the data are used in a decision tree. The parameter `max_features=` limits the number of features available.\n",
    "  \n",
    "1. Create a random forest model with 25 trees, a `random_state=` of 1111, and `max_features=` of 2. Read the print statements.\n",
    "2. Set `max_features=` to 11 (the number of columns in the dataset). Read the print statements.\n",
    "3. Set `max_features=` equal to 4. Read the print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b216b_row0_col0, #T_b216b_row1_col0, #T_b216b_row2_col0, #T_b216b_row3_col0, #T_b216b_row4_col0, #T_b216b_row5_col0, #T_b216b_row6_col0, #T_b216b_row7_col0, #T_b216b_row8_col0, #T_b216b_row9_col0 {\n",
       "  background-color: #6A6868;\n",
       "}\n",
       "#T_b216b_row0_col1, #T_b216b_row0_col2, #T_b216b_row0_col3, #T_b216b_row0_col4, #T_b216b_row0_col5, #T_b216b_row0_col6, #T_b216b_row0_col7, #T_b216b_row0_col8, #T_b216b_row0_col9, #T_b216b_row0_col10, #T_b216b_row0_col11, #T_b216b_row1_col1, #T_b216b_row1_col2, #T_b216b_row1_col3, #T_b216b_row1_col4, #T_b216b_row1_col5, #T_b216b_row1_col6, #T_b216b_row1_col7, #T_b216b_row1_col8, #T_b216b_row1_col9, #T_b216b_row1_col10, #T_b216b_row1_col11, #T_b216b_row2_col1, #T_b216b_row2_col2, #T_b216b_row2_col3, #T_b216b_row2_col4, #T_b216b_row2_col5, #T_b216b_row2_col6, #T_b216b_row2_col7, #T_b216b_row2_col8, #T_b216b_row2_col9, #T_b216b_row2_col10, #T_b216b_row2_col11, #T_b216b_row3_col1, #T_b216b_row3_col2, #T_b216b_row3_col3, #T_b216b_row3_col4, #T_b216b_row3_col5, #T_b216b_row3_col6, #T_b216b_row3_col7, #T_b216b_row3_col8, #T_b216b_row3_col9, #T_b216b_row3_col10, #T_b216b_row3_col11, #T_b216b_row4_col1, #T_b216b_row4_col2, #T_b216b_row4_col3, #T_b216b_row4_col4, #T_b216b_row4_col5, #T_b216b_row4_col6, #T_b216b_row4_col7, #T_b216b_row4_col8, #T_b216b_row4_col9, #T_b216b_row4_col10, #T_b216b_row4_col11, #T_b216b_row5_col1, #T_b216b_row5_col2, #T_b216b_row5_col3, #T_b216b_row5_col4, #T_b216b_row5_col5, #T_b216b_row5_col6, #T_b216b_row5_col7, #T_b216b_row5_col8, #T_b216b_row5_col9, #T_b216b_row5_col10, #T_b216b_row5_col11, #T_b216b_row6_col1, #T_b216b_row6_col2, #T_b216b_row6_col3, #T_b216b_row6_col4, #T_b216b_row6_col5, #T_b216b_row6_col6, #T_b216b_row6_col7, #T_b216b_row6_col8, #T_b216b_row6_col9, #T_b216b_row6_col10, #T_b216b_row6_col11, #T_b216b_row7_col1, #T_b216b_row7_col2, #T_b216b_row7_col3, #T_b216b_row7_col4, #T_b216b_row7_col5, #T_b216b_row7_col6, #T_b216b_row7_col7, #T_b216b_row7_col8, #T_b216b_row7_col9, #T_b216b_row7_col10, #T_b216b_row7_col11, #T_b216b_row8_col1, #T_b216b_row8_col2, #T_b216b_row8_col3, #T_b216b_row8_col4, #T_b216b_row8_col5, #T_b216b_row8_col6, #T_b216b_row8_col7, #T_b216b_row8_col8, #T_b216b_row8_col9, #T_b216b_row8_col10, #T_b216b_row8_col11, #T_b216b_row9_col1, #T_b216b_row9_col2, #T_b216b_row9_col3, #T_b216b_row9_col4, #T_b216b_row9_col5, #T_b216b_row9_col6, #T_b216b_row9_col7, #T_b216b_row9_col8, #T_b216b_row9_col9, #T_b216b_row9_col10, #T_b216b_row9_col11 {\n",
       "  background-color: #517E76;\n",
       "}\n",
       "#T_b216b_row0_col12, #T_b216b_row1_col12, #T_b216b_row2_col12, #T_b216b_row3_col12, #T_b216b_row4_col12, #T_b216b_row5_col12, #T_b216b_row6_col12, #T_b216b_row7_col12, #T_b216b_row8_col12, #T_b216b_row9_col12 {\n",
       "  background-color: #A02626;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b216b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b216b_level0_col0\" class=\"col_heading level0 col0\" >competitorname</th>\n",
       "      <th id=\"T_b216b_level0_col1\" class=\"col_heading level0 col1\" >chocolate</th>\n",
       "      <th id=\"T_b216b_level0_col2\" class=\"col_heading level0 col2\" >fruity</th>\n",
       "      <th id=\"T_b216b_level0_col3\" class=\"col_heading level0 col3\" >caramel</th>\n",
       "      <th id=\"T_b216b_level0_col4\" class=\"col_heading level0 col4\" >peanutyalmondy</th>\n",
       "      <th id=\"T_b216b_level0_col5\" class=\"col_heading level0 col5\" >nougat</th>\n",
       "      <th id=\"T_b216b_level0_col6\" class=\"col_heading level0 col6\" >crispedricewafer</th>\n",
       "      <th id=\"T_b216b_level0_col7\" class=\"col_heading level0 col7\" >hard</th>\n",
       "      <th id=\"T_b216b_level0_col8\" class=\"col_heading level0 col8\" >bar</th>\n",
       "      <th id=\"T_b216b_level0_col9\" class=\"col_heading level0 col9\" >pluribus</th>\n",
       "      <th id=\"T_b216b_level0_col10\" class=\"col_heading level0 col10\" >sugarpercent</th>\n",
       "      <th id=\"T_b216b_level0_col11\" class=\"col_heading level0 col11\" >pricepercent</th>\n",
       "      <th id=\"T_b216b_level0_col12\" class=\"col_heading level0 col12\" >winpercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b216b_row0_col0\" class=\"data row0 col0\" >100 Grand</td>\n",
       "      <td id=\"T_b216b_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_b216b_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_b216b_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_b216b_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row0_col6\" class=\"data row0 col6\" >1</td>\n",
       "      <td id=\"T_b216b_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row0_col8\" class=\"data row0 col8\" >1</td>\n",
       "      <td id=\"T_b216b_row0_col9\" class=\"data row0 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row0_col10\" class=\"data row0 col10\" >0.732000</td>\n",
       "      <td id=\"T_b216b_row0_col11\" class=\"data row0 col11\" >0.860000</td>\n",
       "      <td id=\"T_b216b_row0_col12\" class=\"data row0 col12\" >66.971725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b216b_row1_col0\" class=\"data row1 col0\" >3 Musketeers</td>\n",
       "      <td id=\"T_b216b_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_b216b_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_b216b_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_b216b_row1_col5\" class=\"data row1 col5\" >1</td>\n",
       "      <td id=\"T_b216b_row1_col6\" class=\"data row1 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row1_col7\" class=\"data row1 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row1_col8\" class=\"data row1 col8\" >1</td>\n",
       "      <td id=\"T_b216b_row1_col9\" class=\"data row1 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row1_col10\" class=\"data row1 col10\" >0.604000</td>\n",
       "      <td id=\"T_b216b_row1_col11\" class=\"data row1 col11\" >0.511000</td>\n",
       "      <td id=\"T_b216b_row1_col12\" class=\"data row1 col12\" >67.602936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b216b_row2_col0\" class=\"data row2 col0\" >One dime</td>\n",
       "      <td id=\"T_b216b_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col6\" class=\"data row2 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col7\" class=\"data row2 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col8\" class=\"data row2 col8\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col9\" class=\"data row2 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row2_col10\" class=\"data row2 col10\" >0.011000</td>\n",
       "      <td id=\"T_b216b_row2_col11\" class=\"data row2 col11\" >0.116000</td>\n",
       "      <td id=\"T_b216b_row2_col12\" class=\"data row2 col12\" >32.261086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b216b_row3_col0\" class=\"data row3 col0\" >One quarter</td>\n",
       "      <td id=\"T_b216b_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col6\" class=\"data row3 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col7\" class=\"data row3 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col8\" class=\"data row3 col8\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col9\" class=\"data row3 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row3_col10\" class=\"data row3 col10\" >0.011000</td>\n",
       "      <td id=\"T_b216b_row3_col11\" class=\"data row3 col11\" >0.511000</td>\n",
       "      <td id=\"T_b216b_row3_col12\" class=\"data row3 col12\" >46.116505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b216b_row4_col0\" class=\"data row4 col0\" >Air Heads</td>\n",
       "      <td id=\"T_b216b_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "      <td id=\"T_b216b_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col6\" class=\"data row4 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col7\" class=\"data row4 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col8\" class=\"data row4 col8\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col9\" class=\"data row4 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row4_col10\" class=\"data row4 col10\" >0.906000</td>\n",
       "      <td id=\"T_b216b_row4_col11\" class=\"data row4 col11\" >0.511000</td>\n",
       "      <td id=\"T_b216b_row4_col12\" class=\"data row4 col12\" >52.341465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b216b_row5_col0\" class=\"data row5 col0\" >Almond Joy</td>\n",
       "      <td id=\"T_b216b_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "      <td id=\"T_b216b_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "      <td id=\"T_b216b_row5_col4\" class=\"data row5 col4\" >1</td>\n",
       "      <td id=\"T_b216b_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row5_col6\" class=\"data row5 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row5_col7\" class=\"data row5 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row5_col8\" class=\"data row5 col8\" >1</td>\n",
       "      <td id=\"T_b216b_row5_col9\" class=\"data row5 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row5_col10\" class=\"data row5 col10\" >0.465000</td>\n",
       "      <td id=\"T_b216b_row5_col11\" class=\"data row5 col11\" >0.767000</td>\n",
       "      <td id=\"T_b216b_row5_col12\" class=\"data row5 col12\" >50.347546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b216b_row6_col0\" class=\"data row6 col0\" >Baby Ruth</td>\n",
       "      <td id=\"T_b216b_row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "      <td id=\"T_b216b_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row6_col3\" class=\"data row6 col3\" >1</td>\n",
       "      <td id=\"T_b216b_row6_col4\" class=\"data row6 col4\" >1</td>\n",
       "      <td id=\"T_b216b_row6_col5\" class=\"data row6 col5\" >1</td>\n",
       "      <td id=\"T_b216b_row6_col6\" class=\"data row6 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row6_col7\" class=\"data row6 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row6_col8\" class=\"data row6 col8\" >1</td>\n",
       "      <td id=\"T_b216b_row6_col9\" class=\"data row6 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row6_col10\" class=\"data row6 col10\" >0.604000</td>\n",
       "      <td id=\"T_b216b_row6_col11\" class=\"data row6 col11\" >0.767000</td>\n",
       "      <td id=\"T_b216b_row6_col12\" class=\"data row6 col12\" >56.914547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_b216b_row7_col0\" class=\"data row7 col0\" >Boston Baked Beans</td>\n",
       "      <td id=\"T_b216b_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_b216b_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "      <td id=\"T_b216b_row7_col4\" class=\"data row7 col4\" >1</td>\n",
       "      <td id=\"T_b216b_row7_col5\" class=\"data row7 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row7_col6\" class=\"data row7 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row7_col7\" class=\"data row7 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row7_col8\" class=\"data row7 col8\" >0</td>\n",
       "      <td id=\"T_b216b_row7_col9\" class=\"data row7 col9\" >1</td>\n",
       "      <td id=\"T_b216b_row7_col10\" class=\"data row7 col10\" >0.313000</td>\n",
       "      <td id=\"T_b216b_row7_col11\" class=\"data row7 col11\" >0.511000</td>\n",
       "      <td id=\"T_b216b_row7_col12\" class=\"data row7 col12\" >23.417824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_b216b_row8_col0\" class=\"data row8 col0\" >Candy Corn</td>\n",
       "      <td id=\"T_b216b_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col3\" class=\"data row8 col3\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col4\" class=\"data row8 col4\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col5\" class=\"data row8 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col6\" class=\"data row8 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col7\" class=\"data row8 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col8\" class=\"data row8 col8\" >0</td>\n",
       "      <td id=\"T_b216b_row8_col9\" class=\"data row8 col9\" >1</td>\n",
       "      <td id=\"T_b216b_row8_col10\" class=\"data row8 col10\" >0.906000</td>\n",
       "      <td id=\"T_b216b_row8_col11\" class=\"data row8 col11\" >0.325000</td>\n",
       "      <td id=\"T_b216b_row8_col12\" class=\"data row8 col12\" >38.010963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b216b_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_b216b_row9_col0\" class=\"data row9 col0\" >Caramel Apple Pops</td>\n",
       "      <td id=\"T_b216b_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_b216b_row9_col2\" class=\"data row9 col2\" >1</td>\n",
       "      <td id=\"T_b216b_row9_col3\" class=\"data row9 col3\" >1</td>\n",
       "      <td id=\"T_b216b_row9_col4\" class=\"data row9 col4\" >0</td>\n",
       "      <td id=\"T_b216b_row9_col5\" class=\"data row9 col5\" >0</td>\n",
       "      <td id=\"T_b216b_row9_col6\" class=\"data row9 col6\" >0</td>\n",
       "      <td id=\"T_b216b_row9_col7\" class=\"data row9 col7\" >0</td>\n",
       "      <td id=\"T_b216b_row9_col8\" class=\"data row9 col8\" >0</td>\n",
       "      <td id=\"T_b216b_row9_col9\" class=\"data row9 col9\" >0</td>\n",
       "      <td id=\"T_b216b_row9_col10\" class=\"data row9 col10\" >0.604000</td>\n",
       "      <td id=\"T_b216b_row9_col11\" class=\"data row9 col11\" >0.325000</td>\n",
       "      <td id=\"T_b216b_row9_col12\" class=\"data row9 col12\" >34.517681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x123e12fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading required dataset\n",
    "candy = pd.read_csv('../_datasets/candy-data.csv')\n",
    "\n",
    "# X/y split\n",
    "X = candy.drop(['competitorname', 'winpercent'], axis=1)\n",
    "y = candy['winpercent']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "# function definition for color code\n",
    "def highlight_cols(x):\n",
    "\n",
    "    # Deep copy df to new - original data is not changed\n",
    "    x = x.copy()\n",
    "\n",
    "    # Select all used feature values to a dark blue-green color\n",
    "    x.loc[:, :] = 'background-color: #517E76'\n",
    "\n",
    "    # Overwrite the feature that we dropped from the X set to a grey color\n",
    "    x[['competitorname']] = 'background-color: #6A6868'\n",
    "\n",
    "    # Overwrite target values to a red color\n",
    "    x[['winpercent']] = 'background-color: #A02626'\n",
    "\n",
    "    # Return color df\n",
    "    return x\n",
    "\n",
    "\n",
    "# Displaying the general X/y split with colors so it can be understood another way\n",
    "display(candy.head(10).style.apply(highlight_cols, axis = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is 3.90\n",
      "The testing error is 9.15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25, random_state=1111, max_features=2)\n",
    "\n",
    "# Fitting model to training data\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and test accuracy\n",
    "print('The training error is {0:.2f}'.format(mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is 3.59\n",
      "The testing error is 10.00\n"
     ]
    }
   ],
   "source": [
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25, random_state=1111, max_features=11)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and test accuracy\n",
    "print('The training error is {0:.2f}'.format(mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is 3.60\n",
      "The testing error is 8.79\n"
     ]
    }
   ],
   "source": [
    "# Update the rfr model\n",
    "rfr = RandomForestRegressor(n_estimators=25, random_state=1111, max_features=4)\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Print the training and test accuracy\n",
    "print('The training error is {0:.2f}'.format(mae(y_train, rfr.predict(X_train))))\n",
    "print('The testing error is {0:.2f}'.format(mae(y_test, rfr.predict(X_test))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Am I underfitting?\n",
    "  \n",
    "You are creating a random forest model to predict if you will win a future game of Tic-Tac-Toe. Using the `tic_tac_toe` dataset, you have created training and testing datasets, `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "You have decided to create a bunch of random forest models with varying amounts of trees (1, 2, 3, 4, 5, 10, 20, and 50). The more trees you use, the longer your random forest model will take to run. However, if you don't use enough trees, you risk underfitting. You have created a for loop to test your model at the different number of trees.\n",
    "  \n",
    "1. For each loop, predict values for both the `X_train` and `X_test` datasets.\n",
    "2. For each loop, append the `accuracy_score()` of the `y_train` dataset and the corresponding predictions to `train_scores`.\n",
    "3. For each loop, append the `accuracy_score()` of the `y_test` dataset and the corresponding predictions to `test_scores`.\n",
    "4. Print the training and testing scores using the print statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables using pandas\n",
    "X = pd.get_dummies(tic_tac_toe.iloc[:, 0:9])\n",
    "y = tic_tac_toe.iloc[:, 9]\n",
    "y = tic_tac_toe['Class'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "# Create training and testing datasets, Using 20% for the test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training scores were: [0.94, 0.93, 0.98, 0.97, 0.99, 1.0, 1.0, 1.0]\n",
      "The testing scores were: [0.83, 0.79, 0.89, 0.91, 0.91, 0.93, 0.97, 0.98]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Seed lists for the loop to append to\n",
    "test_scores, train_scores = [], []\n",
    "\n",
    "for i in [1, 2, 3, 4, 5, 10, 20, 50]:\n",
    "    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    # Create predictions for the X_train and X_test datasets\n",
    "    train_predictions = rfc.predict(X_train)\n",
    "    test_predictions = rfc.predict(X_test)\n",
    "    \n",
    "    # Append the accuracy score for the test and train predictions\n",
    "    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))\n",
    "    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))\n",
    "    \n",
    "# Print the train and test scores\n",
    "print(\"The training scores were: {}\".format(train_scores))\n",
    "print(\"The testing scores were: {}\".format(test_scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with only one tree, both the train and test scores are low. As you add more trees, both errors improve. Even at 50 trees, this still might not be enough. Every time you use more trees, you achieve higher accuracy. At some point though, more trees increase training time, but do not decrease testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Train/test scores for n_estimators='}, xlabel='x'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 5, 10, 20, 50]\n",
    "\n",
    "tmp = pd.DataFrame({'x':x, 'training':train_scores, 'test':test_scores})\n",
    "tmp.set_index('x', inplace=True)\n",
    "tmp.plot(title='Train/test scores for n_estimators=')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
