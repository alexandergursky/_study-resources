{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of deep learning and neural networks\n",
    "  \n",
    "Deep learning is the machine learning technique behind the most exciting capabilities in diverse areas like robotics, natural language processing, image recognition, and artificial intelligence, including the famous AlphaGo. In this course, you'll gain hands-on, practical knowledge of how to use deep learning with Keras 2.0, the latest version of a cutting-edge library for deep learning in Python.\n",
    "  \n",
    "In this chapter, you'll become familiar with the fundamental concepts and terminology used in deep learning, and understand why deep learning techniques are so powerful today. You'll build simple neural networks and generate predictions with them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>numpy.array()</td>\n",
    "    <td>Creates an array. An array is a grid of values and it contains information about the raw data, how to locate an element, and how to interpret an element. It has a grid of elements that can be indexed in various ways.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Schema:  \n",
    "- input array -> **array**: feature values\n",
    "- weights for nodes -> **dictionary**: keys = node_name, values = weight of node/input\n",
    "- node_999_in -> **neural network operation**: (prior_input * weight[]).sum()\n",
    "- node_999_out -> **neural network operation**: activation function\n",
    "- node_hidden_concat -> **array**: concat node_999out nodes into an array\n",
    "- output_in -> **neural network operation**: (prior_input * weight[]).sum()\n",
    "- output_out -> **neural network operation**: output activation function, softmax\n",
    "- Display, or create function for above, then make a loop iter with a loop variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to deep learning\n",
    "  \n",
    "**Imagine you work for a bank**\n",
    "  \n",
    "Imagine you work for a bank, and you need to build a model predicting how many transactions each customer will make next year. You have predictive data or features like each customer’s age, bank balance, whether they are retired and so on. We'll get to deep learning in a moment, but for comparison, consider how a simple linear regression model works for this problem. The linear regression embeds an assumption that the outcome, in this case how many transactions a user makes, is the sum of individual parts. It starts by saying, \"what is the average?\" Then it adds the effect of age. Then the effect of bank balance. And so on. So the linear regression model isn't identifying the interactions between these parts, and how they affect banking activity. \n",
    "  \n",
    "**Example as seen by linear regression**\n",
    "  \n",
    "Say we plot predictions from this model. We draw one line with the predictions for retired people, and another with the predictions for those still working. We put current bank balance on the horizontal axis, and the vertical axis is the predicted number of transactions.\n",
    "  \n",
    "The left graph shows predictions from a model with no interactions. In that model we simply add up the effect of the retirement status, and current bank balance. The lack of interactions is reflected by both lines being parallel. That's probably unrealistic, but it's an assumption of the linear regression model. The graph on the right shows the predictions from a model that allows interactions, and the lines don't need to be parallel. \n",
    "\n",
    "**Interactions**\n",
    "  \n",
    "Neural networks are a powerful modeling approach that accounts for interactions like this especially well. Deep learning, the focus of this course, is the use of especially powerful neural networks. Because deep learning models account for these types of interactions so well, they perform great on most prediction problems you've seen before. But their ability to capture extremely complex interactions also allow them to do amazing things with text, images, videos, audio, source code and almost anything else you could imagine doing data science with.\n",
    "  \n",
    "- Neural Networks account for interactions really well\n",
    "- Deep learning uses especially powerful neural networks\n",
    "  \n",
    "**Course structure**\n",
    "  \n",
    "The first two chapters of this course focus on conceptual knowledge about deep learning. This part will be hard, but it will prepare you to debug and tune deep learning models on conventional prediction problems, and it will lay the foundation for progressing towards those new and exciting applications. You'll see this pay off in the third and fourth chapter.\n",
    "  \n",
    "- Debug and tune deep learning models on conventional prediction problems\n",
    "- Lay the foundation for progressing towards modern applications\n",
    "  \n",
    "**Build and tune deep learning models using keras**\n",
    "  \n",
    "You will write code that looks like this, to build and tune deep learning models using keras, to solve many of the same modeling problems you might have previously solved with scikit-learn. As a start to how deep learning models capture interactions and achieve these amazing results, we'll modify the diagram you saw a moment ago.\n",
    "  \n",
    "**Deep learning models capture interactions**\n",
    "  \n",
    "Here there is an interaction between retirement status and bank balance. Instead of having them separately affecting the outcome, we calculate a function of these variables that accounts for their interaction, and use that to predict the outcome. Even this graphic oversimplifies reality, where most things interact with each in some way, and real neural network models account for far more interactions. So the diagram for a simple neural network looks like this.\n",
    "  \n",
    "**Interactions in neural network**\n",
    "  \n",
    "On the far left, we have something called an input layer. This represents our predictive features like age or income. On the far right we have the output layer. The prediction from our model, in this case, the predicted number of transactions. All layers that are not the input or output layers are called hidden layers. They are called hidden layers because, while the inputs and outputs correspond to visible things that happened in the world, and they can be stored as data, the values in the hidden layer aren't something we have data about, or anything we observe directly from the world. \n",
    "  \n",
    "Nevertheless, each dot, called a node, in the hidden layer, represents an aggregation of information from our input data, and each node adds to the model's ability to capture interactions. So the more nodes we have, the more interactions we can capture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing neural network models to classical regression models\n",
    "  \n",
    "Which of the models in the diagrams has greater ability to account for interactions?\n",
    "  \n",
    "Model 1: Input 2, Hidden 2, Output 1  \n",
    "Model 2: Input 2, Hidden 3, Output 1  \n",
    "  \n",
    "Possible Answers\n",
    "\n",
    "- [ ] Model 1\n",
    "- [x] Model 2\n",
    "- [ ] They are both the same\n",
    "  \n",
    "Correct! Model 2 has more nodes in the hidden layer, and therefore, greater ability to capture interactions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "  \n",
    "We’ll start by showing how neural networks use data to make predictions. This is called the forward propagation algorithm.\n",
    "  \n",
    "**Bank transactions example**\n",
    "  \n",
    "Let's revisit our example predicting how many transactions a user will make at our bank. For simplicity, we'll make predictions based on only the number of children and number of existing accounts.\n",
    "  \n",
    "**Forward propagation**\n",
    "  \n",
    "This graph shows a customer with two children and three accounts. The forward-propagation algorithm will pass this information through the network to make a prediction in the output layer. Lines connect the inputs to the hidden layer. Each line has a weight indicating how strongly that input effects the hidden node that the line ends at. These are the first set of weights. We have one weight from the top input into the top node of the layer, and one weight from the bottom input to the top node of the hidden layer. These weights are the parameters we train or change when we fit a neural network to data, so these weights will be a focus throughout this course. To make predictions for the top node of the hidden layer, we take the value of each node in the input layer, multiply it by the weight that ends at that node, and then sum up all the values. In this case, we get (2 times 1) plus (3 times 1), which is 5. \n",
    "  \n",
    "Now do the same to fill in the value of this node on the bottom. That is (two times (minus one)) plus (three times one). That's one. Finally, repeat this process for the next layer, which is the output layer. That is (five times two) plus (one times -1). That gives an output of 9. We predicted nine transactions. That’s forward-propagation. We moved from the inputs on the left, to the hidden layer in the middle, and then from the hidden layers to the output on the right. \n",
    "  \n",
    "<img src='../_images/forward-propagation-demostrated.png' alt='img' width='400'>\n",
    "  \n",
    "We always use that same multiply then add process. If you're familiar with vector algebra or linear algebra, that operation is a dot product. If you don't know about dot products, that's fine too. That was forward propagation for a single data point. In general, we do forward propagation for one data point at a time. The value in that last layer is the model's prediction for that data point.\n",
    "  \n",
    "**Forward propagation code**\n",
    "  \n",
    "Let's see the code for this. We import Numpy for some of the mathematical operations. We've stored the input data as an array. We then have weights into each node in the hidden layer and to the output. We store the weights going into each node as an array, and we use a dictionary to store those arrays. Let’s start forward propagating. We fill in the top hidden node here, which is called node zero. We multiply the inputs by the weights for that node, and then sum both of those terms together. Notice that we had two weights for node_0. That matches the two items in the array it is multiplied by, which is the input_data. These get converted to a single number by the sum function at the end of the line. We then do the same thing for the bottom node of the hidden layer, which is called node 1. Now, both node zero and node one have numeric values. \n",
    "  \n",
    "<img src='../_images/forward-propagation-demostrated1.png' alt='img' width='520'>\n",
    "  \n",
    "To simplify multiplication, we put those in an array here. If we print out the array, we confirm that those are the values from the hidden layer you saw a moment ago. It can also be instructive to verify this by hand with pen and paper. To get the output, we multiply the values in the hidden layer by the weights for the output. Summing those together gives us 10 minus 1, which is 9. In the exercises, you'll practice performing forward propagation in small neural networks.\n",
    "  \n",
    "<img src='../_images/forward-propagation-demostrated2.png' alt='img' width='520'>\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the forward propagation algorithm\n",
    "  \n",
    "In this exercise, you'll write code to do forward propagation (prediction) for your first neural network:\n",
    "  \n",
    "<img src='../_images/forward-propagation-demostrated3.png' alt='img' width='490'>\n",
    "  \n",
    "Each data point is a customer. The first input is how many accounts they have, and the second input is how many children they have. The model will predict how many transactions the user makes in the next year. You will use this data throughout the first 2 chapters of this course.\n",
    "  \n",
    "The input data has been pre-loaded as `input_data`, and the weights are available in a dictionary called `weights`. The array of weights for the first node in the hidden layer are in `weights['node_0']`, and the array of weights for the second node in the hidden layer are in `weights['node_1']`.\n",
    "  \n",
    "The weights feeding into the output node are available in `weights['output']`.\n",
    "  \n",
    "NumPy will be pre-imported for you as `np` in all exercises.\n",
    "  \n",
    "1. Calculate the value in node 0 by multiplying `input_data` by its weights, `weights['node_0']` and computing their sum. This is the 1st node in the hidden layer.\n",
    "2. Calculate the value in node 1 using `input_data` and `weights['node_1']`. This is the 2nd node in the hidden layer.\n",
    "3. Put the hidden layer values into an array. This has been done for you.\n",
    "4. Generate the prediction by multiplying `hidden_layer_outputs` by `weights['output']` and computing their sum.\n",
    "5. Print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the input layer data: input accounts, input children\n",
    "input_data = np.array([3, 5])\n",
    "\n",
    "# Creating the weights for: input layer -> hidden layer (w/ 2 nodes) -> output\n",
    "weights = {'node_0': np.array([2, 4]), \n",
    "           'node_1': np.array([ 4, -5]), \n",
    "           'output': np.array([2, 7])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39\n"
     ]
    }
   ],
   "source": [
    "# Calculate node 0 value: node_0_value, matmul\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "# Calculate node 1 value: node_1_value, matmul\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_value, node_1_value])\n",
    "\n",
    "# Calculate output: output\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print output\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the network generated a prediction of -39."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "  \n",
    "But creating this multiply-add-process is only half the story for hidden layers. For neural networks to achieve their maximum predictive power, we must apply something called an activation function in the hidden layers.\n",
    "  \n",
    "**Linear vs. non-linear Functions**\n",
    "  \n",
    "An activation function allows the model to capture non-linearities. Non-linearities, as shown on the right here, capture patterns like how going from no children to one child may impact your banking transactions differently than going from three children to four. We have examples of linear functions, straight lines on the left, and non-linear functions on the right. If the relationships in the data aren’t straight-line relationships, we will need an activation function that captures non-linearities.\n",
    "  \n",
    "<img src='../_images/activation-functions-low-level-neural-net.png' alt='img' width='520'>\n",
    "  \n",
    "**Activation functions**\n",
    "  \n",
    "An activation function is something applied to the value coming into a node, which then transforms it into the value stored in that node, or the node output.\n",
    "  \n",
    "**Improving our neural network**\n",
    "  \n",
    "Let's go back to the previous diagram. The top hidden node previously had a value of 5. For a long time, an s-shaped function called tanh was a popular activation function.\n",
    "  \n",
    "<img src='../_images/activation-functions-low-level-neural-net1.png' alt='img' width='520'>\n",
    "  \n",
    "**Activation functions**\n",
    "  \n",
    "If we used the tanh activation function, this node's value would be tanh(5), which is very close to 1. Today, the standard in both industry and research applications is something called ReLU.\n",
    "  \n",
    "<img src='../_images/activation-functions-low-level-neural-net2.png' alt='img' width='520'>\n",
    "  \n",
    "**ReLU (Rectified Linear Activation)**\n",
    "  \n",
    "The ReLU or rectified linear activation function. That's depicted here. Though it has two linear pieces, it's surprisingly powerful when composed together through multiple successive hidden layers, which you will see soon. \n",
    "  \n",
    "<img src='../_images/activation-functions-low-level-neural-net3.png' alt='img' width='520'>\n",
    "  \n",
    "**Activation functions**\n",
    "  \n",
    "The code that incorporates activation functions is shown here. It is the same as the code you saw previously, but we've distinguished the input from the output in each node, which is shown in these lines and then again here And we've applied the tanh function to convert the input to the output. That gives us a prediction of 1.2 transactions. In the exercise, you will use the Rectified Linear Activation function, or ReLU, in your network.\n",
    "  \n",
    "<img src='../_images/activation-functions-low-level-neural-net4.png' alt='img' width='520'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rectified Linear Activation Function\n",
    "  \n",
    "As explained, an \"activation function\" is a function applied at each node. It converts the node's input into some output.\n",
    "  \n",
    "The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.\n",
    "  \n",
    "Here are some examples:  \n",
    "- relu(3) = 3\n",
    "- relu(-3) = 0\n",
    "  \n",
    "1. Fill in the definition of the `relu()` function:\n",
    "2. Use the `max()` function to calculate the value for the output of `relu()`.\n",
    "3. Apply the `relu()` function to `node_0_input` to calculate `node_0_output`.\n",
    "4. Apply the `relu()` function to `node_1_input` to calculate `node_1_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the input layer data: input accounts, input children\n",
    "input_data = np.array([3, 5])\n",
    "\n",
    "# Creating the weights for: input layer -> hidden layer (w/ 2 nodes) -> output\n",
    "weights = {\n",
    "    'node_0': np.array([2, 4]), \n",
    "    'node_1': np.array([ 4, -5]),\n",
    "    'output': np.array([2, 7])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "# Defining the Rectified Linear Activation Function\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculate\n",
    "    return output\n",
    "\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You predicted 52 transactions. Without this activation function, you would have predicted a negative number! The real power of activation functions will come soon when you start tuning model weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the network to many observations/rows of data\n",
    "  \n",
    "You'll now define a function called ``predict_with_network()`` which will generate predictions for multiple data observations, which are pre-loaded as `input_data`. As before, `weights` are also pre-loaded. In addition, the `relu()` function you defined in the previous exercise has been pre-loaded.\n",
    "  \n",
    "1. Define a function called `predict_with_network()` that accepts two arguments - `input_data_row` and `weights` - and returns a prediction from the network as the output.\n",
    "2. Calculate the input and output values for each node, storing them as: `node_0_input`, `node_0_output`, `node_1_input`, and `node_1_output`.\n",
    "- To calculate the input value of a node, multiply the relevant arrays together and compute their sum.\n",
    "- To calculate the output value of a node, apply the `relu()` function to the input value of the node.\n",
    "3. Calculate the model output by calculating `input_to_final_layer` and `model_output` in the same way you calculated the input and output values for the nodes.\n",
    "4. Use a for loop to iterate over `input_data`:\n",
    "- Use your `predict_with_network()` to generate predictions for each row of the `input_data` - `input_data_row`. Append each prediction to `results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 4 arrays each as shape (2,)\n",
    "input_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "    \n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "    \n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "\n",
    "# Iteration\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "    \n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good work, each of the 4 outputs are the predictions for the inputs given."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper networks\n",
    "  \n",
    "The difference between modern deep learning and the historical neural networks that didn’t deliver these amazing results, is the use of models with not just one hidden layer, but with many successive hidden layers. We forward propagate through these successive layers in a similar way to what you saw for a single hidden layer.\n",
    "  \n",
    "**Multiple hidden layers**\n",
    "  \n",
    "Here is a network with two hidden layers. \n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level.png' alt='img' width='520'>\n",
    "  \n",
    "We first fill in the values for hidden layer one as a function of the inputs. Then apply the activation function to fill in the values in these nodes. \n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level1.png' alt='img' width='520'>\n",
    "  \n",
    "Then use values from the first hidden layer to fill in the second hidden layer.\n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level2.png' alt='img' width='520'>\n",
    "  \n",
    "Then we make a prediction based on the outputs of hidden layer two. In practice, it's becoming common to have neural networks that have many, many layers; five layers, ten layers. A few years ago 15 layers was state of the art but this can scale quite naturally to even a thousand layers. \n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level3.png' alt='img' width='520'>\n",
    "  \n",
    "You use the same forward propagation process, but you apply that iterative process more times. Let's walk through the first steps of that. Assume all layers here use the ReLU activation function. We'll start by filling in the top node of the first hidden layer. That will use these two weights. The top weights contributes 3 times 2, or 6. The bottom weight contributes 20. The ReLU activation function on a positive number just returns that number. So we get 26. Now let's do the bottom node of that first hidden layer. We use these two nodes. Using the same process, we get 4 times 3, or 12 from this weight. And -25 from the bottom weight. So the input to this node is 12 minus 25. Recall that, when we apply ReLU to a negative number, we get 0. So this node is 0. We've shown the values for the subsequent layers here. Pause this video, and verify you can calculate the same values at each node. At this point, you understand the mechanics for how neural networks make predictions. Let’s close this chapter with an interesting and important fact about these deep networks.\n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level4.png' alt='img' width='520'>\n",
    "  \n",
    "**Representation learning**\n",
    "  \n",
    "That is, they internally build up representations of the patterns in the data that are useful for making predictions. And they find increasingly complex patterns as we go through successive hidden layers of the network. In this way, neural networks partially replace the need for feature engineering, or manually creating better predictive features. Deep learning is also sometimes called representation learning, because subsequent layers build increasingly sophisticated representations of the raw data, until we get to a stage where we can make predictions. This is easiest to understand from an application to images, which you will see later in this course. \n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level5.png' alt='img' width='520'>\n",
    "  \n",
    "Even if you haven't worked with images, you may find it useful to think through this example heuristically. When a neural network tries to classify an image, the first hidden layers build up patterns or interactions that are conceptually simple. A simple interaction would look at groups of nearby pixels and find patterns like diagonal lines, horizontal lines, vertical lines, blurry areas, etc. Once the network has identified where there are diagonal lines and horizontal lines and vertical lines, subsequent layers combine that information to find larger patterns, like big squares. A later layer might put together the location of squares and other geometric shapes to identify a checkerboard pattern, a face, a car, or whatever is in the image. \n",
    "  \n",
    "- Deep networks internally build representations of patterns in the data\n",
    "- Partially replace the need for feature engineering\n",
    "- Subsequent layers build increasingly sophisticated representations of raw data\n",
    "  \n",
    "**Deep learning**\n",
    "  \n",
    "The cool thing about deep learning is that the modeler doesn’t need to specify those interactions. We never tell the model to look for diagonal lines. Instead, when you train the model, which you’ll learn to do in the next chapter, the network gets weights that find the relevant patterns to make better predictions. Working with images may still seem abstract, but this idea of finding increasingly complex or abstract patterns is a recurring theme when people talk about deep learning, and it will feel more concrete as you work with these networks more.\n",
    "  \n",
    "- Modeler doesn't need to specify the interactions\n",
    "- When you train the model, the neural network gets weights that find the relevant patterns to make better predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation in a deeper network\n",
    "  \n",
    "You now have a model with 2 hidden layers. The values for an input data point are shown inside the input nodes. The weights are shown on the edges/lines. What prediction would this model make on this data point?\n",
    "  \n",
    "Assume the activation function at each node is the *identity function*. That is, each node's output will be the same as its input. So the value of the bottom node in the first hidden layer is -1, and not 0, as it would be if the ReLU activation function was used.\n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level6.png' alt='img' width='520'>\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [x] 0\n",
    "- [ ] 7\n",
    "- [ ] 9\n",
    "  \n",
    "Correct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer neural networks\n",
    "  \n",
    "In this exercise, you'll write code to do forward propagation for a neural network with 2 hidden layers. Each hidden layer has two nodes. The input data has been preloaded as input_data. The nodes in the first hidden layer are called `node_0_0` and node_0_1. Their weights are pre-loaded as `weights['node_0_0']` and `weights['node_0_1']` respectively.\n",
    "  \n",
    "The nodes in the second hidden layer are called `node_1_0` and `node_1_1`. Their weights are pre-loaded as `weights['node_1_0']` and `weights['node_1_1']` respectively.\n",
    "  \n",
    "We then create a model output from the hidden nodes using weights pre-loaded as `weights['output']`.\n",
    "  \n",
    "<img src='../_images/deeper-networks-low-level7.png' alt='img' width='490'>\n",
    "  \n",
    "1. Calculate `node_0_0_input` using its weights `weights['node_0_0']` and the given input_data. Then apply the `relu()` function to get `node_0_0_output`.\n",
    "2. Do the same as above for `node_0_1_input` to get `node_0_1_output`.\n",
    "3. Calculate `node_1_0_input` using its weights `weights['node_1_0']` and the outputs from the first hidden layer - `hidden_0_outputs`. Then apply the `relu()` function to get `node_1_0_output`.\n",
    "4. Do the same as above for `node_1_1_input` to get `node_1_1_output`.\n",
    "5. Calculate `model_output` using its weights `weights['output']` and the outputs from the second hidden layer `hidden_1_outputs` array. Do not apply the `relu()` function to this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_data = np.array([3, 5])\n",
    "\n",
    "# Weights\n",
    "weights = {\n",
    "    'node_0_0': np.array([2, 4]),\n",
    "    'node_0_1': np.array([ 4, -5]),\n",
    "    'node_1_0': np.array([-1, 2]),\n",
    "    'node_1_1': np.array([1, 2]),\n",
    "    'output': np.array([2, 7])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "    \n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "    \n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "    \n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "    \n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "    \n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return model_output\n",
    "\n",
    "\n",
    "# Prediction\n",
    "output = predict_with_network(input_data)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network generated a prediction of 182."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations are learned\n",
    "  \n",
    "How are the weights that determine the features/interactions in Neural Networks created?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] A user chooses them when creating the model.\n",
    "- [x] The model training process sets them to optimize predictive accuracy.\n",
    "- [ ] The weights are random numbers.\n",
    "  \n",
    "Exactly! You will learn more about how Neural Networks optimize their weights in the next chapter!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levels of representation\n",
    "  \n",
    "Which layers of a model capture more complex or \"higher level\" interactions?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] The first layers capture the most complex interactions.\n",
    "- [x] The last layers capture the most complex interactions.\n",
    "- [ ] All layers capture interactions of similar complexity.\n",
    "  \n",
    "Exactly! The last layers capture the most complex interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
