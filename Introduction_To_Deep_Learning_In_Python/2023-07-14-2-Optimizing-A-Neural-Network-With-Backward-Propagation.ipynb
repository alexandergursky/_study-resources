{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a neural network with backward propagation\n",
    "  \n",
    "Learn how to optimize the predictions generated by your neural networks. You'll use a method called backward propagation, which is one of the most important techniques in deep learning. Understanding how it works will give you a strong foundation to build on in the second half of the course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Scikit-Learn Documentation](https://scikit-learn.org/stable/)  \n",
    "[Mean Squared Error Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>numpy.array()</td>\n",
    "    <td>Creates an array. An array is a grid of values and it contains information about the raw data, how to locate an element, and how to interpret an element. It has a grid of elements that can be indexed in various ways.</td>\n",
    "  </tr>\n",
    "    <td>2</td>\n",
    "    <td>sklearn.metrics.mean_squared_error()</td>\n",
    "    <td>Mean squared error regression loss. Measures how close a regression line is to a set of data points.  <br> </br> \n",
    "    Measures the average of the squares of the errors — that is, the average squared difference between the estimated values and the actual value. <br> </br> \n",
    "    As it is derived from the square of <b>Euclidean distance</b>, it is always a positive value that decreases as the error approaches zero.\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "\n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: scikit-learn  \n",
    "Version: 1.3.0  \n",
    "Summary: A set of python modules for machine learning and data mining  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Schema:  \n",
    "- input array -> **array**: feature values\n",
    "- weights for nodes -> **dictionary**: keys = node_name, values = weight of node/input\n",
    "- node_999_in -> **neural network operation**: (prior_input * weight[]).sum()\n",
    "- node_999_out -> **neural network operation**: activation function\n",
    "- node_hidden_concat -> **array**: concat node_999out nodes into an array\n",
    "- output_in -> **neural network operation**: (prior_input * weight[]).sum()\n",
    "- output_out -> **neural network operation**: output activation function, softmax\n",
    "- Display, or create function for above, then make a loop iter with a loop variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimization\n",
    "  \n",
    "You've seen the forward-propagation algorithm that neural networks use to make predictions. However, the mere fact that a model has the structure of a neural network does not guarantee that it will make good predictions. To see the importance of model weights,\n",
    "  \n",
    "**A baseline neural network**\n",
    "  \n",
    "We'll go back to a network you saw in the previous chapter. We'll use a simple example for the sake of explanation. For the moment, we won't use an activation function in this example, or if you prefer, you might think of an activation function that returns the input, sometimes called the identity function. We have values of 2 and 3 for the inputs, and the true value of the target is 13. So, the closer our prediction is to 13, the more accurate this model is for this data point. We use forward propagation to fill in the values of hidden layer. That gives us hidden node values of 5 and 1. Continuing forward propagation, we use those hidden node values to make a prediction\n",
    "of 9. Since the true target value is 13, our error is 13 minus 9, which is 4. \n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net.png' alt='img' width='450'>\n",
    "  \n",
    "Changing any weight will change our prediction. Let’s see what happens if we change the two weights from the hidden layer to the output. In this case, we make the top weight 3 and the bottom weight -2. Now forward propagation gives us a prediction of 13. That is exactly the value we wanted to predict. So, this change in weights improved the model for this data point.\n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net1.png' alt='img' width='450'>\n",
    "  \n",
    "**Predictions with multiple points**\n",
    "  \n",
    "Making accurate predictions gets harder with multiple points. First of all, at any set of weights, we have many values of the error, corresponding to the many points we make predictions for. We use something called a loss function to aggregate all the errors into a single measure of the model's predictive performance. \n",
    "  \n",
    "- Making accurate predictions gets harder with more points\n",
    "- At any set of weights, there are many values of the error corresponding to the many points we make predictions for\n",
    "  \n",
    "**Squared error loss function**\n",
    "  \n",
    "For example, a common loss function for regression tasks is mean-squared error. You square each error, and take the average of that as a measure of model quality. The loss function aggregates all of the errors into a single score. \n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net2.png' alt='img' width='550'>\n",
    "  \n",
    "**Loss function**\n",
    "  \n",
    "For an illustration, consider a model with only two weights, we could plot the model's performance for each set of weights like this. The values of the weights are plotted on the x and y axis, and the loss function is on the vertical or z axis.\n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net3.png' alt='img' width='550'>\n",
    "  \n",
    "Lower values mean a better model, so our goal is to find the weights giving the lowest value for the loss function. We do this with an algorithm called gradient descent. An analogy may be helpful.\n",
    "  \n",
    "- Loss function aggregates errors in predictions from many data points into single number\n",
    "- Measure of model's predictive performance\n",
    "- Lower loss function value means a better model\n",
    "- Goal: Find the weights that give the lowest value for the loss function, Gradient Descent\n",
    "  \n",
    "**Gradient descent**\n",
    "  \n",
    "Imagine you are in a pitch dark field, and you want to find the lowest point. You might feel the ground to see how it slopes, and take a small step downhill. This gives an improvement, but not necessarily the lowest point yet. So you repeat this process until it is uphill in every direction. This is roughly how gradient descent works.\n",
    "  \n",
    "The steps are: Start at a random point, until you are somewhere flat, find the slope, and take a step downhill. Let's look at optimizing a model with a single weight, and then we'll scale up to optimizing multiple weights. \n",
    "  \n",
    "1. Start at random point\n",
    "2. Until you are somewhere flat:\n",
    "- Find the slope\n",
    "- Take a step downhill\n",
    "  \n",
    "**Optimizing a model with a single weight**\n",
    "  \n",
    "We have a curve showing the loss function on the vertical axis, at different values of the weight, which is on the horizontal axis. We are looking for the low point on this curve, because that means our model is as accurate as possible. We have drawn this tangent line to the curve at our current point. The slope of that tangent line captures the slope of the loss function at the our current weight. That slope corresponds to something called the derivative from calculus. \n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net4.png' alt='img' width='550'>\n",
    "  \n",
    "We use this slope to decide what direction we step. In this case, the slope is positive. So if we want to go downhill, we go in the direction opposite the slope, towards lower numbers. If we repeatedly take small steps opposite the slope, recalculating the slope each time, we will eventually get to the minimum value.\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating model errors\n",
    "  \n",
    "For the exercises in this chapter, you'll continue working with the network to predict transactions for a bank.\n",
    "  \n",
    "What is the error (predicted - actual) for the following network using the ReLU activation function when the input data is [3, 2] and the actual value of the target (what you are trying to predict) is 5? It may be helpful to get out a pen and piece of paper to calculate these values.\n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net5.png' alt='img' width='550'>\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] 5\n",
    "- [ ] 6\n",
    "- [x] 11\n",
    "- [ ] 16 ~(Not quite. Remember, you should calculate the error rather than the prediction.)\n",
    "  \n",
    "Well done! The network generates a prediction of 16, which results in an error of 11."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding how weights change model accuracy\n",
    "  \n",
    "Imagine you have to make a prediction for a single data point. The actual value of the target is 7. The weight going from `node_0` to the output is 2, as shown below. If you increased it slightly, changing it to 2.01, would the predictions become more accurate, less accurate, or stay the same?\n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net6.png' alt='img' width='550'>\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] More accurate\n",
    "- [x] Less accurate\n",
    "- [ ] Stay the same\n",
    "  \n",
    "Exactly! Increasing the weight to 2.01 would increase the resulting error from 9 to 9.08, making the predictions less accurate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding how weight changes affect accuracy\n",
    "  \n",
    "Now you'll get to change weights in a real network and see how they affect model accuracy!\n",
    "  \n",
    "Have a look at the following neural network:\n",
    "  \n",
    "<img src='../_images/the-need-for-optimization-neural-net7.png' alt='img' width='485'>\n",
    "  \n",
    "Its weights have been pre-loaded as `weights_0`. Your task in this exercise is to update a single weight in `weights_0` to create `weights_1`, which gives a perfect prediction (in which the predicted value is equal to `target_actual`: 3).\n",
    "  \n",
    "Use a pen and paper if necessary to experiment with different combinations. You'll use the `predict_with_network()` function, which takes an array of data as the first argument, and weights as the second argument.\n",
    "  \n",
    "1. Create a dictionary of weights called `weights_1` where you have changed 1 weight from `weights_0` (You only need to make 1 edit to `weights_0` to generate the perfect prediction).\n",
    "2. Obtain predictions with the new weights using the `predict_with_network()` function with `input_data` and `weights_1`.\n",
    "3. Calculate the error for the new weights by subtracting `target_actual` from `model_output_1`.\n",
    "4. Print to see how the errors compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ReLU function\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculate\n",
    "    return output\n",
    "\n",
    "\n",
    "# Creating neural net\n",
    "def predict_with_network(input_data_point, weights):\n",
    "    # Hidden layer nodes\n",
    "    node_0_input = (input_data_point * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "    \n",
    "    node_1_input = (input_data_point * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "    \n",
    "    # Concat layer to an array for passing to next layer\n",
    "    hidden_layer_values = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Output\n",
    "    input_to_final_layer = (hidden_layer_values * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    return(model_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# The data point you will make a prediction for\n",
    "input_data = np.array([0, 3])\n",
    "\n",
    "# Sample weights\n",
    "weights_0 = {\n",
    "    'node_0': [2, 1],\n",
    "    'node_1': [1, 2],\n",
    "    'output': [1, 1]\n",
    "}\n",
    "\n",
    "# The actual target value, used to calculate the error (y_true ie. y_test)\n",
    "target_actual = 3\n",
    "\n",
    "# Make prediction using original weights\n",
    "model_output_0 = predict_with_network(input_data, weights_0)\n",
    "\n",
    "# Calculate error: error_0\n",
    "error_0 = model_output_0 - target_actual\n",
    "\n",
    "# Create weights that cause the network to make perfect prediction (3): weights_1\n",
    "weights_1 = {\n",
    "    'node_0': [2, 1],\n",
    "    'node_1': [1, 2],\n",
    "    'output': [1, 0]\n",
    "}\n",
    "\n",
    "# Make prediction using new weights: model_output_1\n",
    "model_output_1 = predict_with_network(input_data, weights_1)\n",
    "\n",
    "# Calculate error: error_1\n",
    "error_1 = model_output_1 - target_actual\n",
    "\n",
    "# Print error_0 and error_1\n",
    "print(error_0)\n",
    "print(error_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network now generates a perfect prediction with an error of 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up to multiple data points\n",
    "  \n",
    "You've seen how different weights will have different accuracies on a single prediction. But usually, you'll want to measure model accuracy on many points. You'll now write code to compare model accuracies for two different sets of weights, which have been stored as `weights_0` and `weights_1`.\n",
    "  \n",
    "`input_data` is a list of arrays. Each item in that list contains the data to make a single prediction. `target_actuals` is a list of numbers. Each item in that list is the actual value we are trying to predict.\n",
    "  \n",
    "In this exercise, you'll use the `mean_squared_error()` function from `sklearn.metrics`. It takes the true values and the predicted values as arguments.\n",
    "  \n",
    "You'll also use the preloaded `predict_with_network()` function, which takes an array of data as the first argument, and weights as the second argument.\n",
    "  \n",
    "1. Import `mean_squared_error from` `sklearn.metrics`.\n",
    "2. Using a for loop to iterate over each row of `input_data`:\n",
    "3. Make predictions for each row with `weights_0` using the `predict_with_network()` function and append it to `model_output_0`.\n",
    "4. Do the same for `weights_1`, appending the predictions to `model_output_1`.\n",
    "5. Calculate the mean squared error of `model_output_0` and then `model_output_1` using the `mean_squared_error()` function. The first argument should be the actual values (`target_actuals`), and the second argument should be the predicted values (`model_output_0` or `model_output_1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight set 0\n",
    "weights_0 = {\n",
    "    'node_0': np.array([2, 1]),\n",
    "    'node_1': np.array([1, 2]),\n",
    "    'output': np.array([1, 1])\n",
    "}\n",
    "\n",
    "# Weight set 1\n",
    "weights_1 = {\n",
    "    'node_0': np.array([2, 1]),\n",
    "    'node_1': np.array([1, 1.5]),\n",
    "    'output': np.array([1, 1.5])\n",
    "}\n",
    "\n",
    "# Input feature values\n",
    "input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\n",
    "\n",
    "# y_true ie. y_test\n",
    "target_actuals = [1, 3, 5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error with weights_0: 37.5000\n",
      "Mean squared error with weights_1: 49.8906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Create loop-variable for model_output_0\n",
    "model_output_0 = []\n",
    "# Create loop-variable for model_output_1\n",
    "model_output_1 = []\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_0.append(predict_with_network(row, weights_0))\n",
    "    # Append prediction to model_output_1\n",
    "    model_output_1.append(predict_with_network(row, weights_1))\n",
    "    \n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_0 = mean_squared_error(model_output_0, target_actuals)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_1 = mean_squared_error(model_output_1, target_actuals)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_0: {:.4f}\".format(mse_0))\n",
    "print(\"Mean squared error with weights_1: {:.4f}\".format(mse_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `model_output_1` has a higher mean squared error, so model0 is better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "  \n",
    "With gradient descent, you repeatedly repeatedly found a slope capturing how your loss function changes as a weight changes. You then made a small change to the weight to get to a lower point, and you repeated this until you couldn't go downhill any more.\n",
    "  \n",
    "- If the slope is positive:\n",
    "- - Going opposite the slope means moving to lower numbers\n",
    "- - Subtract the slope from the current value\n",
    "- - Too big a step might lead us astray\n",
    "- Solution: learning rate ($\\eta$ ie. eta)\n",
    "- - Update each weight by subtracting learning rate * slope\n",
    "  \n",
    "If the slope is positive, going opposite the slope means moving to lower numbers. Subtracting the slope from the current value achieves this. But too big a step might lead us far astray. So, instead of directly subtracting the slope, we multiply the slope by a small number, called the learning rate, and we change the weight by the product of that multiplication. Learning rate are frequently around 0.01. This ensures we take small steps, so we reliably move towards the optimal weights. But how do we find the relevant slope for each weight we need to update? Working this out for yourself involves calculus, especially the application of the chain rule. Don't worry if you don't remember or don't know the underlying calculus. We'll explain some basic concepts here, and Keras and TensorFlow do the calculus for us.\n",
    "  \n",
    "**Slope calculation example**\n",
    "  \n",
    "Here is a first example to calculate a slope for a weight, and in this example we will look at a single data point. Weights feed from one node into another, and you always get the slope you need by multiplying three things. First, the slope of the loss function with respect to the value at the node we feed into. Second, the value of the node that feeds into our weight. Third, the slope of the activation function with respect to the value we feed into. Let's start with the slope of the loss function with respect to the value of the node our weight feeds into. In this case, that node is the model's prediction. \n",
    "  \n",
    "<img src='../_images/gradient-descent-in-action-neural-net.png' alt='img' width='550'>\n",
    "  \n",
    "If you work through some calculus, you will find that the slope of the mean-squared loss function with respect to the prediction is 2 times (predicted value - actual value). Which is 2 times the error. Here, the prediction from forward propagation was 6. The actual target value is 10, so the error is 6 minus 10, which is -4. \n",
    "  \n",
    "<img src='../_images/gradient-descent-in-action-neural-net1.png' alt='img' width='550'>\n",
    "  \n",
    "The second thing we multiply is the value at the node we are feeding from. Here, that is 3. Finally, the slope of the activation function at the value we feed into. \n",
    "  \n",
    "<img src='../_images/gradient-descent-in-action-neural-net2.png' alt='img' width='550'>\n",
    "  \n",
    "Since we don't have an activation function here, we can leave that out. So our final result for the slope of the loss if we graphed it against this weight is 2 times -4 times 3, or negative 24. We would now improve this weight by subtracting the learning rate times that slope, minus 24. If the learning rate were point-01, we would update this weight to be 2.24.That gives us a better model. And it would continue improving if we repeated this process. For multiple weights feeding to the output, we repeat this calculation separately for each weight. Then we update both weights simultaneously using their respective derivatives.\n",
    "  \n",
    "<img src='../_images/gradient-descent-in-action-neural-net3.png' alt='img' width='550'>\n",
    "    \n",
    "**Code to calculate slopes and update weights**\n",
    "  \n",
    "Let’s see the code to calculate slopes and update the weights. First, we set up the weights, input data, and a target value to predict. Here is the slope calculation. We uses numpy broadcasting, which multiplies an array by a number so that each entry in the array is multiplied by that number. We multiply the two times the error times the array with the input nodes. This gives us an array that used the 1st node value for the first calculated slope, and the second node value for the 2nd calculated slope. This is exactly what we wanted. Incidentally, the mathematical term for this array of slopes is a \"gradient\", and this is where the name gradient descent comes from. We update the weights by some small step in that direction, where the step size is partially determined by the learning rate. And the new error is 2.5, which is an improvement over the old error, which was 5. Repeating that process from the new values would give further improvements.\n",
    "  \n",
    "```python\n",
    "import numpy as np\n",
    "weights = np.array([1, 2])\n",
    "input_data = np.array([3, 4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds - target\n",
    "print(error)\n",
    "# 5\n",
    "\n",
    "gradient = 2 * input_data * error\n",
    "gradient\n",
    "# array([30, 40])\n",
    "\n",
    "weights_updated = weights - learning_rate * gradient\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "error_updated = preds_updated - target\n",
    "print(error_updated)\n",
    "# 2.5\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating slopes\n",
    "  \n",
    "You're now going to practice calculating slopes. When plotting the mean-squared error loss function against predictions, the slope is `2 * x * (xb-y)`, or `2 * input_data * error`. Note that `x` and `b` may have multiple numbers (`x` is a vector for each data point, and `b` is a vector). In this case, the output will also be a vector, which is exactly what you want.\n",
    "  \n",
    "You're ready to write the code to calculate this slope while using a single data point. You'll use pre-defined weights called `weights` as well as data for a single point called `input_data`. The actual value of the target you want to predict is stored in `target`.\n",
    "  \n",
    "1. Calculate the predictions, `preds`, by multiplying `weights` by the `input_data` and computing their sum.\n",
    "2. Calculate the error, which is `preds` minus `target`. Notice that this error corresponds to `xb-y` in the gradient expression.\n",
    "3. Calculate the slope of the loss function with respect to the prediction. To do this, you need to take the product of `input_data` and `error` and multiply that by `2`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slope intercept**  \n",
    "$y = mx+b$  \n",
    "  \n",
    "**Slope**  \n",
    "$m = (y2 - y1)/(x2 - x1) = \\Delta y/\\Delta x$  \n",
    "  \n",
    "**Slope is equal to $\\tan \\emptyset$**  \n",
    "$\\tan \\emptyset = (y2 - y1)/(x2 - x1)$  \n",
    "$m = \\tan \\emptyset$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([1, 2, 3])\n",
    "weights = np.array([0, 2, 1])\n",
    "target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 28 42]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the predictions: preds\n",
    "preds = (input_data * weights).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Print the slope\n",
    "print(slope)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use this slope to improve the weights of the model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving model weights\n",
    "  \n",
    "You've just calculated the slopes you need. Now it's time to use those slopes to improve your model. If you add the slopes to your weights, you will move in the right direction. However, it's possible to move too far in that direction. So you will want to take a small step in that direction first, using a lower learning rate, and verify that the model is improving.\n",
    "  \n",
    "The weights have been pre-loaded as `weights`, the actual value of the target as target, and the input data as `input_data`. The predictions from the initial weights are stored as `preds`.\n",
    "  \n",
    "1. Set the learning rate to be `0.01` and calculate the error from the original predictions.\n",
    "2. Calculate the updated weights by subtracting the product of `learning_rate` and `slope` from `weights`.\n",
    "3. Calculate the updated predictions by multiplying `weights_updated` with `input_data` and computing their sum.\n",
    "4. Calculate the error for the new predictions. Store the result as `error_updated`.\n",
    "5. Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - learning_rate * slope\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the model weights did indeed decrease the error!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making multiple updates to weights\n",
    "  \n",
    "You're now going to make multiple updates so you can dramatically improve your model weights, and see how the predictions improve with each update.\n",
    "  \n",
    "To keep your code clean, there is a pre-loaded `get_slope()` function that takes `input_data`, `target`, and `weights` as arguments. There is also a `get_mse()` function that takes the same arguments. The `input_data`, `target`, and `weights` have been pre-loaded.\n",
    "  \n",
    "This network does not have any hidden layers, and it goes directly from the input (with 3 nodes) to an output node. Note that `weights` is a single array.\n",
    "  \n",
    "We have also pre-loaded `matplotlib.pyplot`, and the error history will be plotted after you have done your gradient descent steps.\n",
    "  \n",
    "1. Using a for loop to iteratively update `weights`:\n",
    "- Calculate the slope using the `get_slope()` function.\n",
    "- Update the `weights` using a learning rate of `0.01`.\n",
    "- Calculate the mean squared error (`mse`) with the updated `weights` using the `get_mse()` function.\n",
    "- Append `mse` to `mse_hist`.\n",
    "2. Visualize `mse_hist`. What trend do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(input_data, target, weights):\n",
    "    preds = (weights * input_data).sum()\n",
    "    error = preds - target\n",
    "    return error\n",
    "\n",
    "\n",
    "def get_slope(input_data, target, weights):\n",
    "    error = get_error(input_data, target, weights)\n",
    "    slope = 2 * input_data * error\n",
    "    return slope\n",
    "\n",
    "\n",
    "def get_mse(input_data, target, weights):\n",
    "    errors = get_error(input_data, target, weights)\n",
    "    mse = np.mean(errors ** 2)\n",
    "    return mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in range(n_updates):\n",
    "    # Calculate the slope: slope\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    \n",
    "    # Update the weights: weights\n",
    "    weights = weights - learning_rate * slope\n",
    "    \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    \n",
    "    # Append the mse to mse_hist\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the mean squared error decreases as the number of iterations go up."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "  \n",
    "You’ve used gradient descent to optimize weights in a simple model. Now we'll add a technique called “back propagation” to calculate the slopes you need to optimize more complex deep learning models.\n",
    "  \n",
    "**Backpropagation**\n",
    "  \n",
    "Just as forward propagation sends input data through the hidden layers and into the output layer, back propagation takes the error from the output layer and propagates it backward through the hidden layers, towards the input layer.\n",
    "  \n",
    "<img src='../_images/backpropagation-in-action-neural-networks.png' alt='img' width='550'>\n",
    "  \n",
    "It calculates the necessary slopes sequentially from the weights closest to the prediction, through the hidden layers, eventually back to the weights coming from the inputs. We then use these slopes to update our weights as you've seen. Back propagation is tricky. So you should focus on the general structure of the algorithm, rather than trying to memorize every mathematical detail.\n",
    "  \n",
    "- Allows gradient descent to update all weights in neural network (by getting gradients for all weights) \n",
    "- Comes from chain rule of calculus\n",
    "- Important to understand the process, but you will generally use a library that implements this\n",
    "  \n",
    "**Backpropagation process**\n",
    "  \n",
    "In the big picture, we are trying to estimate the slope of the loss function with respect to each weight in our network. You've already seen that we use prediction errors to calculate some of those slopes. So we always do forward propagation to make a prediction and calculate an error before we do back propagation. \n",
    "  \n",
    "<img src='../_images/backpropagation-in-action-neural-networks1.png' alt='img' width='550'>\n",
    "  \n",
    "Here are the results of forward propagation. Node values are in white and weights are in black. We need to be at this step before we can start back-propagation. Notice, we are using the \"relu\" activation function. So any node whose input is negative takes a value of 0, and that happens in the top node of the first hidden layer.\n",
    "  \n",
    "For back-propagation, we go back one layer at a time, and each time we go back a layer, we'll use a formula for slopes that you saw in the last video. Every weight feeds from some input node into some output node. The three things we multiply to get the slope for that weight are1, the value at the weights input node.2, the slope from plotting the loss function against that weight's output node.3, the slope of the activation function at the weight's output. We know the value at the node feeding into this weight. Either it is in an input layer, in which case we have it from the data. Or that node is in a hidden layer, in which case we calculated its value when we did forward propagation. The second item on this list is the slope of the loss function with respect to the output node. We do backward propagation from the right side of our diagram to the left. So we already calculated that slope by the time we to plug it into the current calculation. Finally we need the slope of the activation function at the node it feeds into.\n",
    "  \n",
    "- Tries to estimate the slope of the loss function w.r.t each weight\n",
    "- Do forward propagation to calculate predictions and errors, before backpropagation\n",
    "- Go back one layer at a time\n",
    "- Gradients for weight is product of:\n",
    "1. Node value feeding into that weight\n",
    "2. Slope of loss function w.r.t node it feeds into\n",
    "3. Slope of activation function at the node it feeds into\n",
    "  \n",
    "**ReLU Activation Function**\n",
    "  \n",
    "You can see from this diagram that, for the ReLU function, the slope is 0 if the input into a node is negative. If the input into the node is positive, the output is the same as the input. So the slope would be 1.\n",
    "  \n",
    "<img src='../_images/backpropagation-in-action-neural-networks2.png' alt='img' width='550'>\n",
    "  \n",
    "**Backpropagation process**\n",
    "  \n",
    "So far, we have focused on calculating slopes of the loss function with respect to weights. We also keep track of the slopes of the loss function with respect to node values, because we use those slopes in our calculations of slopes at weights. The slope of the loss function with respect to any node value is the sum of the slopes for every weight coming into that node.\n",
    "  \n",
    "- Need to also keep track of the slopes of the loss function w.r.t node values\n",
    "- Slope of node values are the sum of the slopes for all weights that come out of them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relationship between forward and backward propagation\n",
    "  \n",
    "If you have gone through 4 iterations of calculating slopes (using backward propagation) and then updated weights, how many times must you have done forward propagation?\n",
    "  \n",
    "Possible Answers\n",
    "\n",
    "- [ ] 0\n",
    "- [ ] 1\n",
    "- [x] 4\n",
    "- [ ] 8\n",
    "  \n",
    "Exactly! Each time you generate predictions using forward propagation, you update the weights using backward propagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about backward propagation\n",
    "  \n",
    "If your predictions were all exactly right, and your errors were all exactly 0, the slope of the loss function with respect to your predictions would also be 0. In that circumstance, which of the following statements would be correct?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [x] The updates to all weights in the network would also be 0.\n",
    "- [ ] The updates to all weights in the network would be dependent on the activation functions.\n",
    "- [ ] The updates to all weights in the network would be proportional to values from the input data.\n",
    "  \n",
    "Correct! In this situation, the updates to all weights in the network would indeed also be 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "  \n",
    "Let’s see this back propagation in a deeper network.\n",
    "  \n",
    "**Backpropagation**\n",
    "  \n",
    "Start at the last set of weights. Those are currently 1 and 2. \n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice.png' alt='img' width='550'>\n",
    "  \n",
    "We multiply 3 things. The node values feeding into these weights are 1 and 3.\n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice1.png' alt='img' width='550'>\n",
    "  \n",
    "The relevant slope for the output node is 2 times the error. That’s 6. \n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice2.png' alt='img' width='550'>\n",
    "  \n",
    "And the slope of the activation function is 1, since the output node is positive. So, we have a slope for the top weight of 6, and a slope for the bottom weight of 18.Those slopes we just calculated feed into the formula associated with weights further back in the network. Let's do that calculation one layer back now. We’ve hidden the earlier and later layers, since we don’t need them to calculate the slopes for this layer of the network. This graph uses white to denotes node values, black to denote weight values, and the red shows the calculated slopes of the loss function with respect to that node, which we just finished calculating. This is all the information we need to calculate the slopes of the loss function with respect to the weights in this diagram.\n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice3.png' alt='img' width='550'>\n",
    "  \n",
    "**Calculating slopes associated with any weight**\n",
    "  \n",
    "Recall, the three things we multiply to get slopes associated with any weight: value at the node feeding into the weight,the slope of the activation function for the node being fed into. That slope is 1 in all cases here. The slope of the loss function with respect to the output node. \n",
    "  \n",
    "**Backpropagation**\n",
    "  \n",
    "Let's start with the slopes related to the weights going into the top node. For the top weight going into the top node, we multiply 0 for the input node's value, which is in white. \n",
    "  \n",
    "Times 6 for the output node's slope, which is in red. Times the derivative of the ReLU activation function. That output node has a positive value for the input, so the ReLU activation has a slope of 1. 0 times 6 times 1 is 0. For the other weight going into this node, we have 1 times 6 times the slope of the ReLU activation function at the output node's value. The slope of the activation function is still 1. So, we have 1 times 6 times 1, which is 6. \n",
    "  \n",
    "Here we also show slopes associated with the other two weights. We would multiply them all by a learning rate, and use the results to update the weights in gradient descent. Pause the video and make sure you understand how these last two weights were calculated. \n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice4.png' alt='img' width='550'>\n",
    "  \n",
    "You are through the hardest concepts in this course which are gradient descent and back-propagation. \n",
    "  \n",
    "**Backpropagation: Recap**\n",
    "  \n",
    "As a recap, we start at some random set of weights. We then go through the following iterative process Use forward propagation to make a prediction. Use backward propagation to calculate the slope of the loss function with respect to each weight. Multiply that slope by the learning rate, and subtract that from the current weights. Keep going with that cycle until we get to a flat part. \n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice5.png' alt='img' width='550'>\n",
    "  \n",
    "**Stochastic gradient descent**\n",
    "  \n",
    "For computational efficiency, it is common to calculate slopes on only a subset of the data, called a batch, for each update of the weights. You then use a different batch of data to calculate the next update. Once we have used all our data, we start over again at the beginning of the data. Each time through the full training data is called an epoch. So if we're going through our data for the 3rd time, we'd say we are on the 3rd epoch. When slopes are calculated on one batch at a time, rather than on the full data, that is called stochastic gradient descent, rather than gradient descent, which uses all of the data for each slope calculation. The process will be partially automated for you, but understanding the process will help fix any surprises that come up when building your models.\n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice6.png' alt='img' width='550'>\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A round of backpropagation\n",
    "  \n",
    "In the network shown below, we have done forward propagation, and node values calculated as part of forward propagation are shown in white. The weights are shown in black. Layers after the question mark show the slopes calculated as part of back-prop, rather than the forward-prop values. Those slope values are shown in purple.\n",
    "  \n",
    "This network again uses the ReLU activation function, so the slope of the activation function is 1 for any node receiving a positive value as input. Assume the node being examined had a positive value (so the activation function's slope is 1).\n",
    "  \n",
    "<img src='../_images/backpropagation-in-practice7.png' alt='img' width='550'>\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] 0\n",
    "- [ ] 2\n",
    "- [x] 6\n",
    "- [ ] Not enough information\n",
    "  \n",
    "Well done! The slope needed to update this weight is indeed 6. You're now ready to start building deep learning models with Keras!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#7393B3'>NOTE:</span> To calculate the slope for backpropagation in a neural network, you need to compute the gradient of the loss function with respect to the network's parameters. This gradient provides information about how the loss changes as you vary the parameters. The slope or gradient can be used to update the parameters using an optimization algorithm such as gradient descent.\n",
    "\n",
    "Here's a general outline of the process for calculating the slope in backpropagation:\n",
    "\n",
    "1. Forward Pass:\n",
    "   - Perform a forward pass through the neural network to obtain the predicted output. This involves propagating the input data through the network layer by layer, applying activation functions and combining weights along the way.\n",
    "\n",
    "2. Loss Function:\n",
    "   - Define a loss function that quantifies the difference between the predicted output and the actual output. Common loss functions include mean squared error (MSE) or cross-entropy loss, depending on the problem.\n",
    "\n",
    "3. Backward Pass:\n",
    "   - Start the backward pass by computing the gradient of the loss function with respect to the output of the neural network. This step depends on the specific loss function used.\n",
    "\n",
    "4. Chain Rule:\n",
    "   - Use the chain rule of calculus to propagate the gradient backward through the layers of the network. At each layer, you calculate the gradient of the loss function with respect to the layer's inputs and its parameters (weights and biases).\n",
    "   - The chain rule is a fundamental rule in calculus used to calculate the derivative of a composition of functions. In the context of neural networks and backpropagation, the chain rule is crucial for propagating gradients through the layers of the network.\n",
    "   - Let's say we have a function $y = f(g(x))$, where $x$ is the input, $g(x)$ is an intermediate function, and $f(g)$ is the final function. The chain rule states that the derivative of $y$ with respect to $x$ can be computed as follows: $dy/dx = (df/dg) * (dg/dx)$\n",
    "   - In other words, the derivative of the outer function $f$ with respect to the inner function $g$ is multiplied by the derivative of the inner function $g$ with respect to the original variable $x$.\n",
    "   - The chain rule can be extended to compositions of more than two functions. For example, if we have $y = f(g(h(x)))$, the chain rule can be applied iteratively as follows: $dy/dx = (df/dg) * (dg/dh) * (dh/dx)$\n",
    "   - In the context of backpropagation in neural networks, the chain rule is applied iteratively through the layers of the network to calculate the gradients with respect to the parameters. The chain rule allows us to decompose the derivative of the loss function with respect to the network's output, and propagate it backward through each layer, multiplying by the derivatives of the activation functions and the weights at each step.\n",
    "   - By applying the chain rule recursively, we can calculate the gradients for all the parameters in the network and use them to update the weights and biases during the training process.\n",
    "   - The specific form of the chain rule formula can vary depending on the functions and variables involved. In the context of neural networks, the chain rule is typically applied to compute gradients for backpropagation.\n",
    "\n",
    "5. Parameter Updates:\n",
    "   - Once you have computed the gradients for all the network parameters, you can update the parameters using an optimization algorithm such as gradient descent. The update is typically done by subtracting a fraction of the gradient (scaled by a learning rate) from the current parameter values.\n",
    "\n",
    "This process is iterated over multiple training examples, adjusting the parameters in the direction that minimizes the loss function. The specific implementation details and formulas for computing gradients can vary depending on the network architecture and activation functions used.\n",
    "\n",
    "<span style='color:#7393B3'>NOTE:</span> It's worth noting that libraries like TensorFlow or PyTorch handle the automatic computation of gradients and provide tools for implementing backpropagation efficiently.\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span> The formula for calculating the gradient or slope in backpropagation depends on the specific layer and activation function being used in the neural network. This is the general formula for calculating the gradient with respect to the weights of a layer in a feedforward neural network.\n",
    "  \n",
    "Let's consider a single layer of a neural network with weights denoted by $W$ and biases denoted by $b$. The output of this layer is represented by $z$, and the activation function applied to $z$ is denoted by $a$. We assume that the loss function is denoted by $L$.\n",
    "\n",
    "The formula for calculating the gradient of the loss function with respect to the weights ($dW$) is given by the chain rule:\n",
    "  \n",
    "$formula.$\n",
    "  \n",
    "$\\Large dW = \\frac{dL}{da} * \\frac{da}{dz} * \\frac{dz}{dW}$\n",
    "  \n",
    "$where.$\n",
    "  \n",
    "$dL/da$ : The partial derivative of the loss function with respect to the output of the layer. This term is often provided by the specific loss function being used.\n",
    "  \n",
    "$da/dz$ : The derivative of the activation function with respect to the layer's inputs ($z$). This term depends on the choice of activation function and can vary. For example, for the sigmoid activation function, $da/dz = a * (1 - a)$.\n",
    "  \n",
    "$dz/dW$ : The derivative of the layer's inputs with respect to the weights ($W$). This term depends on the specific layer. In a feedforward network, $dz/dW$ is equal to the input values of the layer.\n",
    "  \n",
    "---\n",
    "  \n",
    "By applying this formula and the chain rule recursively, you can calculate the gradients for all the weights and biases in the network during the backpropagation process. The gradients are then used to update the parameters using an optimization algorithm, such as gradient descent, to minimize the loss function and improve the network's performance.\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span> It's important to note that the formula can differ for different types of layers (e.g., convolutional layers, recurrent layers) and activation functions, and there may be additional terms involved. The specific details will depend on the architecture and design choices of the neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
