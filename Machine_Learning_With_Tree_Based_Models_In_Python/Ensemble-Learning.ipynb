{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "1. Train different models on the same dataset\n",
    "2. Let each model make its predictions\n",
    "3. Meta-model: aggregates predictions of individual models\n",
    "4. Final prediction: more robust and less prone to errors\n",
    "5. Best results: models are skillful in different ways. Meaning that if some models make predictions that are way off, the other models should compensate these errors. In such case, the meta-model's predictions are more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Loading df\n",
    "data = pd.read_csv('../_datasets/indian-liver-patient/indian_liver_patient_preprocessed.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Selecting data\n",
    "X = data.iloc[:,:9].values\n",
    "y = data.iloc[:,10].values\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=SEED)\n",
    "\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNeighborsClassifier(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.678\n",
      "K Nearest Neighbours : 0.661\n",
      "Classification Tree : 0.678\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)    \n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of a voting classifier that takes the outputs of the models defined in the list classifiers and assigns labels by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.661\n"
     ]
    }
   ],
   "source": [
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)   \n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the voting classifier achieves a test set accuracy greater than that achieved by LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Bagging: Bootstrap Aggregation  \n",
    "Bagging uses a technique known as the bootstrap, bagging is used to reduce variance of individual models in the ensemble.  \n",
    "  \n",
    "Voting Classifier  \n",
    "- same traing set\n",
    "- /= algorithms\n",
    "  \n",
    "Bagging\n",
    "- one algorithm\n",
    "- /= subsets of the traing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of dt: 0.63\n",
      "Test set accuracy of bc: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Seed\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(estimator=dt, n_estimators=50, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test0 = accuracy_score(y_test, y_pred_dt)\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print('Test set accuracy of dt: {:.2f}'.format(acc_test0)) \n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single tree dt would have achieved an accuracy of 66% which is 2% lower than bc's accuracy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOB: Out Of Bag\n",
    "- On average, for each model, 63% of the training instances are sampled. The remaining 37% constitutes the OOB instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=SEED)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.690, OOB accuracy: 0.711\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy and the OOB accuracy of bc are both roughly equal to 70%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (RF)\n",
    "Random Forests is an ensemble method that uses a decision tree as a base estimator. In Random Forests, each estimator is trained on a different bootstrap sample having the same size as the training set. Random forests introduces further randomization than bagging when training each of the base estimators. When each tree is trained, only d features can be sampled at each node without replacement, where d is a number smaller than the total number of features.  \n",
    "  \n",
    "Random Forests: Training  \n",
    "- Each tree forming the ensemble is trained on a different bootstrap sample from the training set. In addition, when a tree is trained, at each node, only d features are sampled from all features without replacement. The node is then split using the sampled feature that maximizes information gain. In scikit-learn d defaults to the square-root of the number of features. For example, if there are 100 features, only 10 features are sampled at each node.\n",
    "  \n",
    "Random Forests: Prediction  \n",
    "- Once trained, predictions can be made on new instances. When a new instance is fed to the different base estimators, each of them outputs a prediction. The predictions are then collected by the random forests meta-classifier and a final prediction is made depending on the nature of the problem.  \n",
    "  \n",
    "Random Forests: Classification & Regression  \n",
    "- For classification, the final prediction is made by majority voting. The corresponding scikit-learn class is RandomForestClassifier. For regression, the final prediction is the average of all the labels predicted by the base estimators. The corresponding scikit-learn class is RandomForestRegressor. In general, Random Forests achieves a lower variance than individual trees.  \n",
    "  \n",
    "> **Classification**: Aggrregates predictions by majority voting  \n",
    "> sklearn.ensemble.RandomForestClassifier in scikit-learn\n",
    "> \n",
    "> **Regression**: Aggregates predictions through averaging  \n",
    "> sklearn.ensemble.RandomForestRegressor in scikit-learn\n",
    "  \n",
    "Feature Importance  \n",
    "- When a tree based method is trained, the predictive power of a feature or its importance can be assessed. In scikit-learn, feature importance is assessed by measuring how much the tree nodes use a particular feature to reduce impurity. Note that the importance of a feature is expressed as a percentage indicating the weight of that feature in training and prediction.  \n",
    "- Once you train a tree-based model in scikit-learn, the features importances can be accessed by extracting the feature_importance_ attribute from the model.\n",
    "- To visualize the importance of features as assessed by rf, you can create a pandas series of the features importances and then sort this series and make a horiztonal-barplot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=25, random_state=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=25, random_state=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=25, random_state=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Load df\n",
    "data = pd.read_csv('../_datasets/bikes.csv')\n",
    "\n",
    "# X,y split\n",
    "X = data.loc[:, data.columns != 'cnt'].values  # Selecting all columns except the target\n",
    "y = data['cnt'].values\n",
    "\n",
    "# Seed\n",
    "SEED = 2\n",
    "\n",
    "# Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=SEED)\n",
    "\n",
    "# Instantiate rf with 25 trees\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=SEED)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the test set RMSE evaluation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 54.49\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test,y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a comparison look and train a single CART on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 78.42\n"
     ]
    }
   ],
   "source": [
    "# Import a single CART (any single CART is fine, I just picked this one)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Seed\n",
    "SEED = 2\n",
    "\n",
    "# Initialize \n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# Fit rf to the training set    \n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test_dt = MSE(y_test,y_pred_dt)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test_dt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set RMSE achieved by rf is significantly smaller than that achieved by a single CART"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Features\n",
    "feat = data.loc[:, data.columns != 'cnt']\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= feat.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that hr and workingday are the most important features according to rf. The importances of these two features add up to more than 90%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "Boosting refers to an ensemble method in which many predictors are trained and each predictor learns from the errors of its predecessor. More formally, in boosting many weak learners are combined to form a strong learner. A weak learner is a model doing slightly better than random guessing. For example, a decision tree with a maximum-depth of one, known as a decision-stump, is a weak learner.  \n",
    "In boosting, an ensemble of predictors are trained sequentially and each predictor tries to correct the errors made by its predecessor.  \n",
    "  \n",
    "- **AdaBoost**: stands for Adaptive Boosting. In AdaBoost, each predictor pays more attention to the instances wrongly predicted by its predecessor by constantly changing the weights of training instances.  \n",
    "- Each predictor is assigned a coefficient alpha that weighs its contribution in the ensemble's final prediction. Alpha depends on the predictor's training error.  \n",
    "  \n",
    "---\n",
    "  \n",
    "AdaBoost: Training\n",
    "- Once all the predictors in the ensemble are trained, the label of a new instance can be predicted depending on the nature of the problem.  \n",
    "- There are N predictors in total. \n",
    "- First, predictor1 is trained on the initial dataset (X,y), and the training error for predictor1 is determined. This error can then be used to determine alpha1 which is predictor1's coefficient. \n",
    "- Alpha1 is then used to determine the weights W(2) of the training instances for predictor2. When the weighted instances are used to train predictor2, this predictor is forced to pay more attention to the incorrectly predicted instances. When the weighted instances are used to train predictor2, this predictor is forced to pay more attention to the incorrectly predicted instances.\n",
    "- This process is repeated sequentially, until the N predictors forming the ensemble are trained.\n",
    "  \n",
    "AdaBoost: Learning Rate\n",
    "- An important parameter used in training is the learning rate, eta. Eta is a number between 0 and 1; it is used to shrink the coefficient alpha of a trained predictor. It's important to note that there's a trade-off between eta and the number of estimators. A smaller value of eta should be compensated by a greater number of estimators.\n",
    "- Eta is denoted by **η** or **H**\n",
    "  \n",
    "AdaBoost: Prediction\n",
    "- Classification: Each predictor predicts the label of the new instance and the ensemble's prediction is obtained by weighted majority voting.  \n",
    "- Regression: The same procedure is applied and the ensemble's prediction is obtained by performing a weighted average.\n",
    "- It's important to note that individual predictors need not to be CARTs. However CARTs are used most of the time in boosting because of their high variance.\n",
    "> **Classification**: Aggrregates predictions by majority voting  \n",
    "> sklearn.ensemble.AdaBoostClassifier in scikit-learn\n",
    "> \n",
    "> **Regression**: Aggregates predictions through weighted averaging  \n",
    "> sklearn.ensemble.AdaboostRegressor in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Loading df\n",
    "data = pd.read_csv('../_datasets/indian-liver-patient/indian_liver_patient_preprocessed.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Selecting data\n",
    "X = data.iloc[:,:9].values\n",
    "y = data.iloc[:,10].values\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=SEED)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(estimator=dt, n_estimators=180, random_state=SEED)\n",
    "\n",
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This untuned AdaBoost classifier achieved an ROC AUC score of 0.66"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GB)\n",
    "In gradient boosting, each predictor in the ensemble corrects its predecessor's error. In contrast to AdaBoost, the weights of the training instances are not tweaked. Instead, each predictor is trained using the residual errors of its predecessor as labels.  \n",
    "- Does not tweak the weights of training instances\n",
    "- Each predictor is trained using its predecessor's residual errors as labels.\n",
    "- Gradiant Boosted trees use a CART as the base learner.\n",
    "  \n",
    "---\n",
    "Gradient Boosted Trees for Regression: Training\n",
    "- The ensemble consists of N trees. Tree1 is trained using the features matrix X and the dataset labels y. The predictions labeled y1hat are used to determine the training set residual errors r1.\n",
    "- Tree2 is then trained using the features matrix X and the residual errors r1 of Tree1 as labels. The predicted residuals r1hat are then used to determine the residuals of residuals which are labeled r2.\n",
    "- This process is repeated until all of the N trees forming the ensemble are trained.\n",
    "  \n",
    "Gradient Boosted: Shrinkage\n",
    "- An important parameter used in training gradient boosted trees is shrinkage.\n",
    "- In this context, shrinkage refers to the fact that the prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate eta which is a number between 0 and 1.\n",
    "- Similarly to AdaBoost, there's a trade-off between eta and the number of estimators. Decreasing the learning rate needs to be compensated by increasing the number of estimators in order for the ensemble to reach a certain performance.\n",
    "  \n",
    "Gradient Boosted Trees: Prediction\n",
    "- Once all trees in the ensemble are trained, prediction can be made. When a new instance is available, each tree predicts a label and the final ensemble prediction is given.\n",
    "- In scikit-learn, the class for a gradient boosting regressor is GradientBoostingRegressor.\n",
    "- The class implementing gradient-boosted-classification in scikit-learn is GradientBoostingClassifier.\n",
    "> **Classification**: Aggrregates predictions by majority voting  \n",
    "> sklearn.ensemble.GradientBoostingRegressor in scikit-learn\n",
    "> \n",
    "> **Regression**: Aggregates predictions through weighted averaging  \n",
    "> sklearn.ensemble.GradientBoostingClassifier in scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Train the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Load df\n",
    "data = pd.read_csv('../_datasets/bikes.csv')\n",
    "\n",
    "# Seed\n",
    "SEED = 2\n",
    "\n",
    "# X,y split\n",
    "X = data.loc[:, data.columns != 'cnt'].values  # Selecting all columns except the target\n",
    "y = data['cnt'].values\n",
    "\n",
    "# Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= SEED)\n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth=4, \n",
    "            n_estimators=200,\n",
    "            random_state=SEED)\n",
    "\n",
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 49.537\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test**(1/2)\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting: Cons\n",
    "- GB involves an exhaustive search procedure\n",
    "- Each CART (Classification and Regression Tree) is trained to find the best split points and features\n",
    "- May lead to CART's using the same split points and maybe the same features\n",
    "  \n",
    "Solution: Stochastic Gradient Boosting (SGB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Boosting (SGB)\n",
    "In stochastic gradient boosting, each CART is trained on a random subset of the training data. This subset is sampled without replacement. Furthermore, at the level of each node, features are sampled without replacement when choosing the best split-points. As a result, this creates further diversity in the ensemble and the net effect is adding more variance to the ensemble of trees.\n",
    "- Each tree is trained on a random subset of rows within the training data\n",
    "- The sampled instances (40% - 80% of the training set) are sampled without replacement\n",
    "- Features are sampled (without replacement) when choosing split points\n",
    "- Result: further ensemble diversity\n",
    "- Effect: adding further variance to the ensemble of trees\n",
    "---\n",
    "  \n",
    "Stochastic Gradient Boosting: Training\n",
    "- First, instead of providing all the training instances to a tree, only a fraction of these instances are provided through sampling without replacement. The sampled data is then used for training a tree.\n",
    "- However, not all features are considered when a split is made. Instead, only a certain randomly sampled fraction of these features are used for this purpose.  \n",
    "- Once a tree is trained, predictions are made and the residual errors can be computed. These residual errors are multiplied by the learning rate eta and are fed to the next tree in the ensemble.  \n",
    "- This procedure is repeated sequentially until all the trees in the ensemble are trained.\n",
    "- The prediction procedure for a new instance in stochastic gradient boosting is similar to that of gradient boosting.\n",
    "- SGB is preformed by using **sklearn.ensemble.GradientBoostingRegressor**, Then throwing the hyperparameter 'subsample=' into the instantiation. Setting subsample=0.8 would make each tree randomly sample 80% of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with SGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Load df\n",
    "data = pd.read_csv('../_datasets/bikes.csv')\n",
    "\n",
    "# Seed\n",
    "SEED = 2\n",
    "\n",
    "# X,y split\n",
    "X = data.loc[:, data.columns != 'cnt'].values  # Selecting all columns except the target\n",
    "y = data['cnt'].values\n",
    "\n",
    "# Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= SEED)\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(\n",
    "    max_depth=4, \n",
    "    subsample=0.9,\n",
    "    max_features=0.75,\n",
    "    n_estimators=200,\n",
    "    random_state=SEED\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 47.260\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "\n",
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set RMSE of gb: 49.537  \n",
    "Test set RMSE of sgbr: 47.260  \n",
    "  \n",
    "The stochastic gradient boosting regressor achieves a lower test set RMSE than the gradient boosting regressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
