{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features\n",
    "  \n",
    "Every day you read about the amazing breakthroughs in how the newest applications of machine learning are changing the world. Often this reporting glosses over the fact that a huge amount of data munging and feature engineering must be done before any of these fancy models can be used. In this course, you will learn how to do just that. You will work with Stack Overflow Developers survey, and historic US presidential inauguration addresses, to understand how best to preprocess and engineer features from categorical, continuous, and unstructured data. This course will give you hands-on experience on how to prepare any data for your own machine learning models.\n",
    "  \n",
    "In this chapter, you will explore what feature engineering is and how to get started with applying it to real-world data. You will load, explore and visualize a survey response dataset, and in doing so you will learn about its underlying data types and why they have an influence on how you should engineer your features. Using the pandas package you will create new features from both categorical and continuous columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why generate features?\n",
    "  \n",
    "  \n",
    "**Feature Engineering**\n",
    "  \n",
    "Feature engineering is the act of taking raw data and extracting features from it that are suitable for tasks like machine learning. Most machine learning algorithms work with tabular data. When we talk about features, we are referring to the information stored in the columns of these tables. For example, if we were looking at information on houses, the features would be things like square foot, number of rooms, etc. This course is designed for data scientists who want to expand their knowledge of how to incorporate feature engineering into their data science workflow.\n",
    "  \n",
    "<img src='../_images/what-are-features-what-do-they-look-like.png' text='alt text' width='500'>\n",
    "  \n",
    "**Different types of data**\n",
    "  \n",
    "Most machine learning algorithms require their input data to be represented as a vector or a matrix, and many assume that the data is distributed normally. In the real world, more often than not you will receive data that is not in this format. You will also need to work with many different types of data, some data types you will often encounter are: continuous variables, categorical data, ordinal data, boolean values, and dates and times. Dealing with these is manageable, but requires a well thought out approach. Feature engineering is often overlooked in machine learning discussions, but any real-world practitioner will confirm that data manipulation and feature engineering is the most important aspect of the project.\n",
    "  \n",
    "<img src='../_images/common-different-types-of-data-in-ml.png' text='alt text' width='500'>\n",
    "  \n",
    "**Course structure**\n",
    "  \n",
    "Over the span of this course, we will be addressing how to deal with many different types of data and how to convert them into a format that can be easily used for machine learning. In the first chapter, you will ingest and create basic features from tabular data. In the second chapter, you will learn how to deal with data that has missing values. You will then move on to transforming your data so that it conforms to statistical assumptions often necessary for machine learning models, and finally, you will convert free form text into tabular data so it can be used with machine learning models.\n",
    "  \n",
    "**Pandas**\n",
    "  \n",
    "Now lets jump straight in with some examples. During this course we will be leveraging the pandas package substantially as it is very useful when working with data in tabular form. It is a common practice to import pandas using the pd alias. You can use the `pd.read_csv()` function to import a CSV file and use the `.head()` method to quickly look at the first few rows of the DataFrame.\n",
    "  \n",
    "**Dataset**\n",
    "  \n",
    "For the first three chapters of this course, you will be working with a modified subset of the Stackoverflow survey response data. This dataset records the details and preferences of hundreds of users of the StackOverflow website.\n",
    "  \n",
    "**Column names**\n",
    "  \n",
    "To see the features used in this subset, you can use the DataFrame columns attribute to print the names of all the columns in the DataFrame.\n",
    "  \n",
    "**Column types**\n",
    "  \n",
    "To print the data type of each column, you can use the `df.dtypes` attribute. Here you can see three different data types - integers, floats and objects - in pandas objects are columns that contain strings.\n",
    "  \n",
    "**Selecting specific data types**\n",
    "  \n",
    "Knowing the types of each column can be very useful if you are performing analysis based on a subset of specific data types. To do this, you can use the `.select_dtypes()` method and pass a list of relevant data types to the include argument. For example, if you want to select only the integer columns, call the `pd.select_dtypes()` method on df and set `include=['int']`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting to know your data\n",
    "Pandas is one the most popular packages used to work with tabular data in Python. It is generally imported using the alias pd and can be used to load a CSV (or other delimited files) using `pd.read_csv()`.\n",
    "  \n",
    "You will be working with a modified subset of the [Stackoverflow survey response data](https://insights.stackoverflow.com/survey/2018/#overview) in the first three chapters of this course. This dataset records the details, and preferences of thousands of users of the StackOverflow website.\n",
    "  \n",
    "1. Import the pandas library as pd.\n",
    "2. `so_survey_csv` contains the URL to a CSV file. Import it using `pd.read_csv()` into `so_survey_df`.\n",
    "3. Print the first five rows of `so_survey_df`.\n",
    "4. Print the data type of each column in `so_survey_df`.\n",
    "5. Question  \n",
    "What type of data is the `ConvertedSalary` column?  \n",
    "*Possible answers*\n",
    "  \n",
    "- [ ] Datetime\n",
    "- [x] Numeric\n",
    "- [ ] String\n",
    "- [ ] Boolean\n",
    "  \n",
    "Correct! `ConvertedSalary` contains floats which are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SurveyDate</th>\n",
       "      <th>FormalEducation</th>\n",
       "      <th>ConvertedSalary</th>\n",
       "      <th>Hobby</th>\n",
       "      <th>Country</th>\n",
       "      <th>StackOverflowJobsRecommend</th>\n",
       "      <th>VersionControl</th>\n",
       "      <th>Age</th>\n",
       "      <th>Years Experience</th>\n",
       "      <th>Gender</th>\n",
       "      <th>RawSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2/28/18 20:20</td>\n",
       "      <td>Bachelor's degree (BA. BS. B.Eng.. etc.)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Git</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/28/18 13:26</td>\n",
       "      <td>Bachelor's degree (BA. BS. B.Eng.. etc.)</td>\n",
       "      <td>70841.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sweeden</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Git;Subversion</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>Male</td>\n",
       "      <td>70,841.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/6/18 3:37</td>\n",
       "      <td>Bachelor's degree (BA. BS. B.Eng.. etc.)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Sweeden</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Git</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/9/18 1:06</td>\n",
       "      <td>Some college/university study without earning ...</td>\n",
       "      <td>21426.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sweeden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zip file back-ups</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>Male</td>\n",
       "      <td>21,426.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4/12/18 22:41</td>\n",
       "      <td>Bachelor's degree (BA. BS. B.Eng.. etc.)</td>\n",
       "      <td>41671.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>UK</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Git</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>Male</td>\n",
       "      <td>£41,671.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SurveyDate                                    FormalEducation  \\\n",
       "0  2/28/18 20:20           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
       "1  6/28/18 13:26           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
       "2    6/6/18 3:37           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
       "3    5/9/18 1:06  Some college/university study without earning ...   \n",
       "4  4/12/18 22:41           Bachelor's degree (BA. BS. B.Eng.. etc.)   \n",
       "\n",
       "   ConvertedSalary Hobby       Country  StackOverflowJobsRecommend  \\\n",
       "0              NaN   Yes  South Africa                         NaN   \n",
       "1          70841.0   Yes       Sweeden                         7.0   \n",
       "2              NaN    No       Sweeden                         8.0   \n",
       "3          21426.0   Yes       Sweeden                         NaN   \n",
       "4          41671.0   Yes            UK                         8.0   \n",
       "\n",
       "      VersionControl  Age  Years Experience Gender   RawSalary  \n",
       "0                Git   21                13   Male         NaN  \n",
       "1     Git;Subversion   38                 9   Male   70,841.00  \n",
       "2                Git   45                11    NaN         NaN  \n",
       "3  Zip file back-ups   46                12   Male   21,426.00  \n",
       "4                Git   39                 7   Male  £41,671.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data\n",
    "so_survey_df = pd.read_csv('../_datasets/Combined_DS_v10.csv')\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "so_survey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SurveyDate                     object\n",
      "FormalEducation                object\n",
      "ConvertedSalary               float64\n",
      "Hobby                          object\n",
      "Country                        object\n",
      "StackOverflowJobsRecommend    float64\n",
      "VersionControl                 object\n",
      "Age                             int64\n",
      "Years Experience                int64\n",
      "Gender                         object\n",
      "RawSalary                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the data type of each column\n",
    "print(so_survey_df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting specific data types\n",
    "  \n",
    "Often a dataset will contain columns with several different data types (like the one you are working with). The majority of machine learning models require you to have a consistent data type across features. Similarly, most feature engineering techniques are applicable to only one type of data at a time. For these reasons among others, you will often want to be able to access just the columns of certain types when working with a DataFrame.\n",
    "  \n",
    "The DataFrame (`so_survey_df`) from the previous exercise is available in your workspace.\n",
    "  \n",
    "1. Create a subset of `so_survey_df` consisting of only the numeric (int and float) columns.\n",
    "2. Print the column names contained in `so_survey_df_num`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ConvertedSalary', 'StackOverflowJobsRecommend', 'Age',\n",
      "       'Years Experience'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create subset of only the numberic columns\n",
    "so_numeric_df = so_survey_df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Print the column names contained in so_numeric_df\n",
    "print(so_numeric_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lesson, you will learn the most common ways of dealing with categorical data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Categorical Variables\n",
    "  \n",
    "Categorical variables are used to represent groups that are qualitative in nature. Some examples are colors, such as blue, red, black etc. or country of birth, such as Ireland, England or USA. While these can easily be understood by a human, you will need to encode categorical features as numeric values to use them in your machine learning models.\n",
    "  \n",
    "**Encoding categorical features**\n",
    "  \n",
    "As an example, here is a table which consists of the country of residence of different respondents in the Stackoverflow survey. To get from qualitative inputs to quantitative features, one may naively think that assigning every category in a column a number would suffice, for example India could be 1, USA 2 etc. But these categories are unordered, so assigning this order may greatly penalize the effectiveness of your model. Thus, you cannot allocate arbitrary numbers to each category as that would imply some form of ordering in the categories.\n",
    "  \n",
    "<img src='../_images/encoding-categorical-features-example.png' text='alt text' width='500'>\n",
    "  \n",
    "**Encoding categorical features**\n",
    "  \n",
    "Instead, values can be encoded by creating additional binary features corresponding to whether each value was picked or not as shown in the table on the right. In doing so your model can leverage the information of what country is given, without inferring any order between the different options.\n",
    "  \n",
    "<img src='../_images/encoding-categorical-features-example1.png' text='alt text' width='500'>\n",
    "  \n",
    "**Encoding categorical features**\n",
    "  \n",
    "There are two main approaches when representing categorical columns in this way:\n",
    "  \n",
    "- one-hot-encoding\n",
    "- dummy encoding\n",
    "  \n",
    "These are very similar and often confused. In fact, by default, pandas performs one-hot encoding when you use the `pd.get_dummies()` function.\n",
    "  \n",
    "**One-hot encoding**\n",
    "  \n",
    "One-hot-encoding converts n categories into n features as shown here. You can use the `pd.get_dummies()` function to one-hot encode columns. The function takes a DataFrame and a list of categorical columns you want converted into one hot encoded columns, and returns an updated DataFrame with these columns included. Specifying a prefix with the prefix argument can improve readability like the letter C for country has been used here.\n",
    "  \n",
    "<img src='../_images/encoding-categorical-features-example2.png' text='alt text' width='500'>\n",
    "  \n",
    "**Dummy encoding**\n",
    "  \n",
    "On the other hand, dummy encoding creates $Nth-1$ features for $N$ categories, omitting the first category. Notice that this time there is no feature for France, the first category. In dummy encoding, the base value, France in this case, is encoded by the absence of all other countries as you can see on the last row here and its value is represented by the intercept. For dummy encoding, you can use the same `pd.get_dummies()` function with an additional argument, `drop_first=` set to `True` as shown here.\n",
    "  \n",
    "<img src='../_images/encoding-categorical-features-example3.png' text='alt text' width='500'>\n",
    "  \n",
    "**One-hot vs. dummies**\n",
    "  \n",
    "Both these methods have different advantages. One-hot-encoding generally creates much more explainable features, as each country will have its own weight that can be observed after training. But one must be aware that one hot encoding may create features that are entirely collinear due to the same information being represented multiple times.\n",
    "  \n",
    "- One-hot-encoding: Generally creates much more explainable features\n",
    "- Dummy-encoding: Necessary information without duplication\n",
    "  \n",
    "**One-hot vs. dummies**\n",
    "  \n",
    "Take for example a simpler categorical column recording the sex of the survey takers. By recording a 1 for male the information of whether the person is female is already known when the male column is 0. This double representation can lead to instability in your models and dummy values would be more appropriate.\n",
    "  \n",
    "<img src='../_images/encoding-categorical-features-example4.png' text='alt text' width='380'>\n",
    "  \n",
    "**Limiting your columns**\n",
    "  \n",
    "However, both one-hot-encoding and dummy-encoding may result in a huge number of columns being created if there are too many different categories in a column. In these cases, you may want to only create columns for the most common values. You can check the number of occurrences of different features in a column using the `.value_counts()` method on a specific column.\n",
    "  \n",
    "**Limiting your columns**\n",
    "  \n",
    "Once you have your counts of occurrences, you can use it to limit what values you will include by first creating a mask of the values that occur less than $N$ times. A mask is a list of booleans outlining which values in a column should be affected. First we find the categories that occur less than n times using the index attribute and wrap this inside the `.isin()` method. After you create the mask, you can use it to replace these categories that occur less than n times with a value of your choice as shown here.\n",
    "  \n",
    "<img src='../_images/encoding-categorical-features-example5.png' text='alt text' width='500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding and dummy variables\n",
    "  \n",
    "To use categorical variables in a machine learning model, you first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables. In this exercise, you will create both types of encoding, and compare the created column sets. We will continue using the same DataFrame from previous lesson loaded as `so_survey_df` and focusing on its Country column.\n",
    "  \n",
    "1. One-hot encode the Country column, adding \"OH\" as a prefix for each column.\n",
    "2. Create dummy variables for the Country column, adding \"DM\" as a prefix for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby',\n",
      "       'StackOverflowJobsRecommend', 'VersionControl', 'Age',\n",
      "       'Years Experience', 'Gender', 'RawSalary', 'OH_France', 'OH_India',\n",
      "       'OH_Ireland', 'OH_Russia', 'OH_South Africa', 'OH_Spain', 'OH_Sweeden',\n",
      "       'OH_UK', 'OH_USA', 'OH_Ukraine'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convert the Country column to a one hot encoded DataFrame\n",
    "one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\n",
    "\n",
    "# Print the columns names\n",
    "print(one_hot_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby',\n",
      "       'StackOverflowJobsRecommend', 'VersionControl', 'Age',\n",
      "       'Years Experience', 'Gender', 'RawSalary', 'DM_India', 'DM_Ireland',\n",
      "       'DM_Russia', 'DM_South Africa', 'DM_Spain', 'DM_Sweeden', 'DM_UK',\n",
      "       'DM_USA', 'DM_Ukraine'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convert the Country column to a one hot encoded DataFrame\n",
    "dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\n",
    "\n",
    "# Print the columns names\n",
    "print(dummy.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the column for France was missing when you created dummy variables? Now you can choose to use one-hot encoding or dummy variables where appropriate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with uncommon categories\n",
    "  \n",
    "Some features can have many different categories but a very uneven distribution of their occurrences. Take for example Data Science's favorite languages to code in, some common choices are Python, R, and Julia, but there can be individuals with bespoke choices, like FORTRAN, C etc. In these cases, you may not want to create a feature for each value, but only the more common occurrences.\n",
    "  \n",
    "1. Extract the Country column of `so_survey_df` as a series and assign it to `countries`.\n",
    "2. Find the counts of each category in the newly created `countries` series.\n",
    "3. Create a mask for values occurring less than 10 times in `country_counts`.\n",
    "4. Print the first 5 rows of the mask.\n",
    "5. Label values occurring less than the mask cutoff as 'Other'.\n",
    "6. Print the new category counts in `countries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Africa    166\n",
      "USA             164\n",
      "Spain           134\n",
      "Sweeden         119\n",
      "France          115\n",
      "Russia           97\n",
      "UK               95\n",
      "India            95\n",
      "Ukraine           9\n",
      "Ireland           5\n",
      "Name: Country, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a series out of the Country columns\n",
    "countries = so_survey_df.Country.copy()\n",
    "\n",
    "# Get the counts of each category\n",
    "country_counts = countries.value_counts()\n",
    "\n",
    "# Print the count values for each category\n",
    "print(country_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1    False\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "Name: Country, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for only categories that occur less than 10 times\n",
    "mask = countries.isin(country_counts[country_counts < 10].index)\n",
    "\n",
    "# Print the top 5 rows in the mask series\n",
    "print(mask.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Africa    166\n",
      "USA             164\n",
      "Spain           134\n",
      "Sweeden         119\n",
      "France          115\n",
      "Russia           97\n",
      "UK               95\n",
      "India            95\n",
      "Other            14\n",
      "Name: Country, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Label all other categories as Other\n",
    "countries.loc[mask] = 'Other'\n",
    "\n",
    "# Print the updated category counts\n",
    "print(countries.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: By using `.copy()` when assigning the countries series, you create an explicit copy of the column to prevent the warning. Additionally, we replace the use of `countries[mask]` with `countries.loc[mask]` to modify the original DataFrame column instead of working with a copy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good work, now you can work with large datasets while grouping low frequency categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric variables\n",
    "  \n",
    "As mentioned in the previous lesson, most machine learning models will require your data to be in numeric format. However, even if your raw data is all numeric, there is still a lot you can do to improve your features.\n",
    "  \n",
    "**Types of numeric features**\n",
    "  \n",
    "Numeric features can be used to represent a huge array of different characteristics and measurements. Pretty much anything that can be quantitatively measured can be recorded as numeric data. For example, age, the price of an item, counts, and even spatial data such as coordinates. Depending on the use case, numeric features can be treated in several different ways. We will work through a few of the considerations and possible feature engineering steps to keep in mind when dealing with numeric data.\n",
    "  \n",
    "**Does size matter?**\n",
    "  \n",
    "One of the first questions you should ask when working with numeric features is whether the magnitude of the feature is its most important trait, or just its direction. For example, if you had a dataset of restaurant health and safety ratings containing the number of times a restaurant had major violations, you might care far more about whether the restaurant had any major violations at all (as you would rather not take any chances), over whether it was a repeat offender. Looking at this toy dataset containing restaurant IDs and the number of times they had major violations, we can see that some restaurants have no major violations but many have one or more. We will be creating a new binary column representing whether or not a restaurant committed any violation.\n",
    "  \n",
    "<img src='../_images/does-size-matter-restaurant-violations.png' text='alt text' width='500'>\n",
    "  \n",
    "**Binarizing numeric variables**\n",
    "  \n",
    "Here we first create a new column `Binary_Violation` and set it to zero. Then, we use the `.loc[]` notation to find all rows where `Number_of_Violations` is greater than zero and set the `Binary_Violation` column to 1.\n",
    "  \n",
    "`df['Binary_Violation'] = 0`  \n",
    "`df.loc[df['Number_of_Violations'] > 0, 'Binary_Violation'] = 1`  \n",
    "  \n",
    "**Binarizing numeric variables**\n",
    "  \n",
    "As you can see here, all rows where `Number_of_Violations` is equal to 0 are also zeros in `Binary_Violation`. However, for all rows where `Number_of_Violations` is greater than zero is 1 in `Binary_Violation`.\n",
    "  \n",
    "<img src='../_images/does-size-matter-restaurant-violations1.png' text='alt text' width='500'>\n",
    "  \n",
    "**Binning numeric variables**\n",
    "  \n",
    "An extension of this is perhaps you wish to group a numeric variable into more than two bins. This is often useful for variables such as age, wage brackets, etc where exact numbers are less relevant than the general magnitude of the value. Consider the same dataset of restaurant health and safety ratings containing the number of times a restaurant has had major violations. This time we will be creating three groups;  \n",
    "- Group 1 for restaurants with no offenses,  \n",
    "- Group 2 for restaurants with one or two offenses,  \n",
    "- Group 3 for all restaurants with three or more offenses,  \n",
    "  \n",
    "Bins are created by using the `pd.cut()` function. You can define the intervals using the bins argument as shown here, which in this case is a list of 4 values. You can also pass a list of labels like so.\n",
    "  \n",
    "<img src='../_images/binning-numeric-variables-encoding.png' text='alt text' width='500'>\n",
    "  \n",
    "**Binning numeric variables**\n",
    "  \n",
    "Note as we want to include 0 in the first bin, we must set the leftmost edge to lower than that, so all values between $-\\infty$ and 0 are labeled as 1, all values equal to 1 or 2 are labeled as 2, and values greater than 2 are labeled as 3.\n",
    "  \n",
    "<img src='../_images/binning-numeric-variables-encoding1.png' text='alt text' width='500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizing columns\n",
    "  \n",
    "While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. In the `so_survey_df` data, you have a large number of survey respondents that are working voluntarily (without pay). You will create a new column titled `Paid_Job` indicating whether each person is paid (their salary is greater than zero).\n",
    "  \n",
    "1. Create a new column called `Paid_Job` filled with zeros.\n",
    "2. Replace all the `Paid_Job` values with a 1 where the corresponding `ConvertedSalary` is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paid_Job</th>\n",
       "      <th>ConvertedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>70841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>21426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>41671.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Paid_Job  ConvertedSalary\n",
       "0         0              NaN\n",
       "1         1          70841.0\n",
       "2         0              NaN\n",
       "3         1          21426.0\n",
       "4         1          41671.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Paid_Job column filled with zeros\n",
    "so_survey_df['Paid_Job'] = 0\n",
    "\n",
    "# Replace all the Paid_Job values where ConvertedSalary is > 0, if element1(condition) True do element2\n",
    "so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
    "\n",
    "# Print the first five rows of the columns\n",
    "so_survey_df[['Paid_Job', 'ConvertedSalary']].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good work, binarizing columns can also be useful for your target variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning values\n",
    "  \n",
    "For many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages.\n",
    "  \n",
    "Bins are created using `pd.cut(df['column_name'], bins)` where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries.\n",
    "  \n",
    "1. Bin the value of the `ConvertedSalary` column in `so_survey_df` into 5 equal bins, in a new column called `equal_binned`.\n",
    "2. Bin the `ConvertedSalary` column using the boundaries in the list bins and label the bins using `labels=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>equal_binned</th>\n",
       "      <th>ConvertedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(-2000.0, 400000.0]</td>\n",
       "      <td>70841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(-2000.0, 400000.0]</td>\n",
       "      <td>21426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(-2000.0, 400000.0]</td>\n",
       "      <td>41671.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          equal_binned  ConvertedSalary\n",
       "0                  NaN              NaN\n",
       "1  (-2000.0, 400000.0]          70841.0\n",
       "2                  NaN              NaN\n",
       "3  (-2000.0, 400000.0]          21426.0\n",
       "4  (-2000.0, 400000.0]          41671.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bin the continuous variable ConvertedSalary into 5 bins\n",
    "so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], bins=5)\n",
    "\n",
    "# Print the first 5 rows of the equal_binned column\n",
    "so_survey_df[['equal_binned', 'ConvertedSalary']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boundary_binned</th>\n",
       "      <th>ConvertedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>70841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Low</td>\n",
       "      <td>21426.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Low</td>\n",
       "      <td>41671.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  boundary_binned  ConvertedSalary\n",
       "0             NaN              NaN\n",
       "1          Medium          70841.0\n",
       "2             NaN              NaN\n",
       "3             Low          21426.0\n",
       "4             Low          41671.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the boundaries of the bins\n",
    "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
    "\n",
    "# List of bin labels\n",
    "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
    "\n",
    "# Bin the continous variable ConvertedSalary using these boundaries\n",
    "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'], bins=bins, labels=labels)\n",
    "\n",
    "# Print the first 5 rows of the boundary_binned column\n",
    "so_survey_df[['boundary_binned', 'ConvertedSalary']].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct, now you can bin columns with equal spacing and predefined boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
