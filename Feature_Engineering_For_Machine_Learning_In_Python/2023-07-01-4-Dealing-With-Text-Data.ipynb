{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Text Data\n",
    "  \n",
    "Finally, in this chapter, you will work with unstructured text data, understanding ways in which you can engineer columnar features out of a text corpus. You will compare how different approaches may impact how much context is being extracted from a text, and how to balance the need for context, without too many features being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Encoding\n",
    "  \n",
    "So far in this course you have dealt with data that, while sometimes messy, has been generally columnar in nature. When you are faced with text data this is often not going to be the case.\n",
    "  \n",
    "**Standardizing your text**\n",
    "  \n",
    "Data that is not in a predefined form is called unstructured data, and free text data is a good example of this. Before you can leverage text data in a machine learning model you must first transform it into a series of columns of numbers or vectors. There are many different approaches to doing this and in this chapter we will go through the most common approaches. In this chapter, you will be working with the United States inaugural address dataset, which contains the text for each President's inaugural speech. With George Washington's shown here. It is clear that free text like this is not in tabular form.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example.png' text='alt text' width='500'>\n",
    "  \n",
    "**Dataset**\n",
    "  \n",
    "Before any text analytics can be performed, you must ensure that the text data is in a format that can be used. The speeches have been loaded as a pandas DataFrame called `speech_df`, with the body of the text in the 'text' column as can be seen by looking at the top five rows using the `.head()` method as shown.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example1.png' text='alt text' width='500'>\n",
    "  \n",
    "**Removing unwanted characters**\n",
    "  \n",
    "Most bodies of text will have non letter characters such as punctuation, that will need to be removed before analysis. This can be achieved by using the replace() method along with the str accessor. We have used this in an earlier chapter, but instead of specifying the exact characters you wish to replace, this time you will use patterns called regular expressions. Now unless you go through the text of all speeches, it is difficult to determine which non-letter characters are present in the data. So the easiest way to deal with this to specify a pattern which replaces all non letter characters as shown here. The pattern lowercase a to lowercase z followed by uppercase A to uppercase Z inside square brackets basically indicates include all letter characters. Placing a caret before this pattern inside square brackets negates this, that is, says all non letter characters. So we use the `.replace()` method with this pattern to replace all non letter characters with a white-space as shown here.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example2.png' text='alt text' width='500'>\n",
    "  \n",
    "**Removing unwanted characters**\n",
    "  \n",
    "Here you can see the text of the first speech before and after processing. Notice that the hyphen and the colon are missing.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example3.png' text='alt text' width='500'>\n",
    "  \n",
    "**Standardize the case**\n",
    "  \n",
    "Once all unwanted characters have been removed you will want to standardize the remaining characters in your text so that they are all lower case. This will ensure that the same word with and without capitalization will not be counted as separate words. You can use the `.lower()` method to achieve this as shown here.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example4.png' text='alt text' width='500'>\n",
    "  \n",
    "**Length of text**\n",
    "  \n",
    "Later in this chapter you will work through the creation of features based on the content of different texts, but often there is value in the fundamental characteristics of a passage, such as its length. Using the `.len()` method, you can calculate the number of characters in each speech.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example5.png' text='alt text' width='500'>\n",
    "  \n",
    "**Word counts**\n",
    "  \n",
    "Along with the pure character length of the speech, you may want to know how many words are contained in it. The most straight forward way to do this is to split the speech based an any white-spaces, and then count how many words there are after the split. First, you will need to split the text with with the `.split()` method as shown here and\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example6.png' text='alt text' width='500'>\n",
    "  \n",
    "**Word counts**\n",
    "  \n",
    "then chain the `.len()` method to count the total number of words in each speech.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example7.png' text='alt text' width='500'>\n",
    "  \n",
    "**Average length of word**\n",
    "  \n",
    "Finally, one other stat you can calculate is the average word length. Since you already have the total number of characters and the word count, you can simply divide them to obtain the average word length.\n",
    "  \n",
    "<img src='../_images/text-encoding-free-text-example8.png' text='alt text' width='500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up your text\n",
    "  \n",
    "Unstructured text data cannot be directly used in most analyses. Multiple steps need to be taken to go from a long free form string to a set of numeric columns in the right format that can be ingested by a machine learning model. The first step of this process is to standardize the data and eliminate any characters that could cause problems later on in your analytic pipeline.\n",
    "\n",
    "In this chapter you will be working with a new dataset containing the inaugural speeches of the presidents of the United States loaded as `speech_df`, with the speeches stored in the text column.\n",
    "  \n",
    "1. Print the first 5 rows of the `text` column to see the free text fields.\n",
    "2. Replace all non letter characters in the `text` column with a whitespace.\n",
    "3. Make all characters in the newly created `text_clean` column lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Inaugural Address</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>First Inaugural Address</td>\n",
       "      <td>Thursday, April 30, 1789</td>\n",
       "      <td>Fellow-Citizens of the Senate and of the House...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>Second Inaugural Address</td>\n",
       "      <td>Monday, March 4, 1793</td>\n",
       "      <td>Fellow Citizens:  I AM again called upon by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Adams</td>\n",
       "      <td>Inaugural Address</td>\n",
       "      <td>Saturday, March 4, 1797</td>\n",
       "      <td>WHEN it was first perceived, in early times, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>First Inaugural Address</td>\n",
       "      <td>Wednesday, March 4, 1801</td>\n",
       "      <td>Friends and Fellow-Citizens:  CALLED upon to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>Second Inaugural Address</td>\n",
       "      <td>Monday, March 4, 1805</td>\n",
       "      <td>PROCEEDING, fellow-citizens, to that qualifica...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name         Inaugural Address                      Date  \\\n",
       "0  George Washington   First Inaugural Address  Thursday, April 30, 1789   \n",
       "1  George Washington  Second Inaugural Address     Monday, March 4, 1793   \n",
       "2         John Adams         Inaugural Address   Saturday, March 4, 1797   \n",
       "3   Thomas Jefferson   First Inaugural Address  Wednesday, March 4, 1801   \n",
       "4   Thomas Jefferson  Second Inaugural Address     Monday, March 4, 1805   \n",
       "\n",
       "                                                text  \n",
       "0  Fellow-Citizens of the Senate and of the House...  \n",
       "1  Fellow Citizens:  I AM again called upon by th...  \n",
       "2  WHEN it was first perceived, in early times, t...  \n",
       "3  Friends and Fellow-Citizens:  CALLED upon to u...  \n",
       "4  PROCEEDING, fellow-citizens, to that qualifica...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataframe\n",
    "speech_df = pd.read_csv('../_datasets/inaugural_speeches.csv')\n",
    "speech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Fellow-Citizens of the Senate and of the House...\n",
       "1    Fellow Citizens:  I AM again called upon by th...\n",
       "2    WHEN it was first perceived, in early times, t...\n",
       "3    Friends and Fellow-Citizens:  CALLED upon to u...\n",
       "4    PROCEEDING, fellow-citizens, to that qualifica...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 5 rows of the text column\n",
    "speech_df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellow citizens of the senate and of the house...\n",
      "1    fellow citizens   i am again called upon by th...\n",
      "2    when it was first perceived  in early times  t...\n",
      "3    friends and fellow citizens   called upon to u...\n",
      "4    proceeding  fellow citizens  to that qualifica...\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Replace all non letter characters with a whitespace\n",
    "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "\n",
    "# Change to lower case\n",
    "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of text_clean column\n",
    "print(speech_df['text_clean'].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now your text strings have been standardized and cleaned up. You can now use this new column (`text_clean`) to extract information about the speeches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level text features\n",
    "  \n",
    "Once the text has been cleaned and standardized you can begin creating features from the data. The most fundamental information you can calculate about free form text is its size, such as its length and number of words. In this exercise (and the rest of this chapter), you will focus on the cleaned/transformed text column (`text_clean`) you created in the last exercise.\n",
    "  \n",
    "1. Record the character length of each speech in the `char_count` column.\n",
    "2. Record the word count of each speech in the `word_count` column.\n",
    "3. Record the average word length of each speech in the `avg_word_length` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>char_cnt</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fellow citizens of the senate and of the house...</td>\n",
       "      <td>8616</td>\n",
       "      <td>1432</td>\n",
       "      <td>6.016760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fellow citizens   i am again called upon by th...</td>\n",
       "      <td>787</td>\n",
       "      <td>135</td>\n",
       "      <td>5.829630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when it was first perceived  in early times  t...</td>\n",
       "      <td>13871</td>\n",
       "      <td>2323</td>\n",
       "      <td>5.971158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>friends and fellow citizens   called upon to u...</td>\n",
       "      <td>10144</td>\n",
       "      <td>1736</td>\n",
       "      <td>5.843318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>proceeding  fellow citizens  to that qualifica...</td>\n",
       "      <td>12902</td>\n",
       "      <td>2169</td>\n",
       "      <td>5.948363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  char_cnt  word_cnt  \\\n",
       "0  fellow citizens of the senate and of the house...      8616      1432   \n",
       "1  fellow citizens   i am again called upon by th...       787       135   \n",
       "2  when it was first perceived  in early times  t...     13871      2323   \n",
       "3  friends and fellow citizens   called upon to u...     10144      1736   \n",
       "4  proceeding  fellow citizens  to that qualifica...     12902      2169   \n",
       "\n",
       "   avg_word_length  \n",
       "0         6.016760  \n",
       "1         5.829630  \n",
       "2         5.971158  \n",
       "3         5.843318  \n",
       "4         5.948363  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the length of each text\n",
    "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "\n",
    "# Count the number of words in each text\n",
    "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features may appear basic but can be quite useful in ML models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Representation\n",
    "  \n",
    "Once high level information has been recorded you can begin creating features based on the actual content of each text.\n",
    "  \n",
    "**Text to columns**\n",
    "  \n",
    "The most common approach to this is to create a column for each word and record the number of times each particular word appears in each text. This results in a set of columns equal in width to the number of unique words in the dataset, with counts filling each entry. Taking just one sentence we can see that \"of\" occurs 3 tines, \"the\" 2 times and the other words once.\n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text.png' text='alt text' width='500'>\n",
    "  \n",
    "**Initializing the vectorizer**\n",
    "  \n",
    "While you could of course write a script to do this counting yourself, scikit-learn already has this functionality built in with its `CountVectorizer` class.\n",
    "  \n",
    "`from sklearn.feature_extraction.text import CountVectorizer`  \n",
    "  \n",
    "Then instantiate it by assigning it to a variable name, `cv` in this case.  \n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text1.png' text='alt text' width='500'>\n",
    "  \n",
    "**Specifying the vectorizer**\n",
    "  \n",
    "It may have become apparent that creating a column for every word will result in far too many values for analyses. Thankfully, you can specify arguments when initializing your `CountVectorizer` to limit this. For example, you can specify the minimum number of texts that a word must be contained in using the argument `min_df`. If a float is given, the word must appear in at least this percent of documents. This threshold eliminates words that occur so rarely that they would not be useful when generalizing to new texts. Conversely, `max_df` limits words to only ones that occur below a certain percentage of the data. This can be useful to remove words that occur too frequently to be of any value.\n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text2.png' text='alt text' width='500'>\n",
    "  \n",
    "**Fit the vectorizer**\n",
    "  \n",
    "Once the vectorizer has been instantiated you can then fit it on the data you want to create your features around. This is done by calling the `.fit()` method on relevant column.\n",
    "  \n",
    "`cv.fit(speech_df['text_clean'])`  \n",
    "  \n",
    "**Transforming your text**\n",
    "  \n",
    "Once the vectorizer has been fit you can call the `.transform()` method on the column you want to transform. This outputs a sparse array, with a row for every text and a column for every word that has been counted.\n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text3.png' text='alt text' width='500'>\n",
    "  \n",
    "**Transforming your text**\n",
    "  \n",
    "To transform this to a non sparse array you can use the `.toarray()` method.\n",
    "  \n",
    "`cv_transformed.toarray()`  \n",
    "  \n",
    "**Getting the features**\n",
    "  \n",
    "You may notice that the output is an array, with no concept of column names. To get the names of the features that have been generated you can call the `.get_feature_names()` method on the vectorizer which returns a list of the features generated, in the same order that the columns of the converted array are in.\n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text4.png' text='alt text' width='500'>\n",
    "  \n",
    "**Fitting and transforming**\n",
    "  \n",
    "As an aside, while fitting and transforming separately can be useful, particularly when you need to transform a different dataset than the one that you fit the vectorizer to, you can accomplish both steps at once using the `.fit_transform()` method.\n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text5.png' text='alt text' width='500'>\n",
    "  \n",
    "Putting it all together\n",
    "  \n",
    "Now that you have an array containing the count values of each of the words of interest, and a way to get the feature names you can combine these in a DataFrame as shown here. The `.add_prefix()` method allows you to be able to distinguish these columns in the future.\n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text6.png' text='alt text' width='500'>\n",
    "  \n",
    "`out Counts_aback Counts_abandon Counts_abandonment 0 1 0 0 1 0 0 1 2 0 1 0 3 0 1 0 4 0 0 0 `\n",
    "  \n",
    "**Updating your DataFrame**\n",
    "  \n",
    "You can now combine this DataFrame with your original DataFrame so they can be used to generate future analytical models using pandas `pd.concat()` method. Checking the DataFrames shape shows the new much wider size. Remember to specify the axis argument to 1 as you want column bind these DataFrames.\n",
    "  \n",
    "<img src='../_images/word-count-representation-ml-with-text7.png' text='alt text' width='500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words (I)\n",
    "  \n",
    "Once high level information has been recorded you can begin creating features based on the actual content of each text. One way to do this is to approach it in a similar way to how you worked with categorical variables in the earlier lessons.\n",
    "  \n",
    "For each unique word in the dataset a column is created.\n",
    "For each entry, the number of times this word occurs is counted and the count value is entered into the respective column.\n",
    "  \n",
    "These \"count\" columns can then be used to train machine learning models.\n",
    "  \n",
    "1. `from sklearn.feature_extraction.text import CountVectorizer`\n",
    "2. Instantiate `CountVectorizer` and assign it to `cv`.\n",
    "3. Fit the vectorizer to the `text_clean` column.\n",
    "4. Print the feature names generated by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandon' 'abandoned' 'abandonment' 'abate' 'abdicated' 'abeyance'\n",
      " 'abhorring' 'abide' 'abiding' 'abilities']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer\n",
    "cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "print(cv.get_feature_names_out()[:10])  # Extracting the first 10 feature names\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this vectorizer can be applied to both the text it was trained on, and new texts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words (II)\n",
    "  \n",
    "Once the vectorizer has been fit to the data, it can be used to transform the text to an array representing the word counts. This array will have a row per block of text and a column for each of the features generated by the vectorizer that you observed in the last exercise.\n",
    "  \n",
    "The vectorizer to you fit in the last exercise (`cv`) is available in your workspace.\n",
    "  \n",
    "1. Apply the vectorizer to the `text_clean` column.\n",
    "2. Convert this transformed (sparse) array into a `numpy` array with counts.\n",
    "3. Print the dimensions of this `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Apply the vectorizer\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 9043)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of cv_array\n",
    "print(cv_array.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speeches have 9043 unique words, which is a lot! In the next exercise, you will see how to create a limited set of features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting your features\n",
    "  \n",
    "As you have seen, using the `CountVectorizer` with its default settings creates a feature for every single word in your corpus. This can create far too many features, often including ones that will provide very little analytical value.\n",
    "  \n",
    "For this purpose `CountVectorizer` has parameters that you can set to reduce the number of features:\n",
    "  \n",
    "- `min_df=` : Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts.\n",
    "  \n",
    "- `max_df=` : Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as \"and\" or \"the\".\n",
    "  \n",
    "1. Limit the number of features in the `CountVectorizer` by setting the minimum number of documents a word can appear to 20% and the maximum to 80%.\n",
    "2. Fit and apply the vectorizer on `text_clean` column in one step.\n",
    "3. Convert this transformed (sparse) array into a `numpy` array with counts.\n",
    "4. Print the dimensions of the new reduced array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 818)\n",
      "[[ 0  0  0 ...  5  0  9]\n",
      " [ 0  0  0 ...  0  0  1]\n",
      " [ 0  0  0 ...  0  0  1]\n",
      " ...\n",
      " [ 0  1  0 ... 14  1  3]\n",
      " [ 0  0  0 ...  5  1  0]\n",
      " [ 0  0  0 ... 14  1 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Specify arguments to limit the number of features generated\n",
    "cv = CountVectorizer(min_df=0.2, max_df=0.8)  # Word must appear at least 20% of the time in courpus, also can not appear more than 80% of the time\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the array shape\n",
    "print(cv_array.shape)\n",
    "print(cv_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the number of features (unique words) greatly reduced from 9043 to 818?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to DataFrame\n",
    "  \n",
    "Now that you have generated these count based features in an array you will need to reformat them so that they can be combined with the rest of the dataset. This can be achieved by converting the array into a `pandas` DataFrame, with the feature names you found earlier as the column names, and then concatenate it with the original DataFrame.\n",
    "  \n",
    "The numpy array (`cv_array`) and the vectorizer (`cv`) you fit in the last exercise are available in your workspace.\n",
    "  \n",
    "1. Create a DataFrame `cv_df` containing the `cv_array` as the values and the feature names as the column names.\n",
    "2. Add the prefix `Counts_` to the column names for ease of identification.\n",
    "3. Concatenate this DataFrame (`cv_df`) to the original DataFrame (`speech_df`) column wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Inaugural Address</th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>char_cnt</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>Counts_abiding</th>\n",
       "      <th>Counts_ability</th>\n",
       "      <th>...</th>\n",
       "      <th>Counts_women</th>\n",
       "      <th>Counts_words</th>\n",
       "      <th>Counts_work</th>\n",
       "      <th>Counts_wrong</th>\n",
       "      <th>Counts_year</th>\n",
       "      <th>Counts_years</th>\n",
       "      <th>Counts_yet</th>\n",
       "      <th>Counts_you</th>\n",
       "      <th>Counts_young</th>\n",
       "      <th>Counts_your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>First Inaugural Address</td>\n",
       "      <td>Thursday, April 30, 1789</td>\n",
       "      <td>Fellow-Citizens of the Senate and of the House...</td>\n",
       "      <td>fellow citizens of the senate and of the house...</td>\n",
       "      <td>8616</td>\n",
       "      <td>1432</td>\n",
       "      <td>6.016760</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Washington</td>\n",
       "      <td>Second Inaugural Address</td>\n",
       "      <td>Monday, March 4, 1793</td>\n",
       "      <td>Fellow Citizens:  I AM again called upon by th...</td>\n",
       "      <td>fellow citizens   i am again called upon by th...</td>\n",
       "      <td>787</td>\n",
       "      <td>135</td>\n",
       "      <td>5.829630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Adams</td>\n",
       "      <td>Inaugural Address</td>\n",
       "      <td>Saturday, March 4, 1797</td>\n",
       "      <td>WHEN it was first perceived, in early times, t...</td>\n",
       "      <td>when it was first perceived  in early times  t...</td>\n",
       "      <td>13871</td>\n",
       "      <td>2323</td>\n",
       "      <td>5.971158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>First Inaugural Address</td>\n",
       "      <td>Wednesday, March 4, 1801</td>\n",
       "      <td>Friends and Fellow-Citizens:  CALLED upon to u...</td>\n",
       "      <td>friends and fellow citizens   called upon to u...</td>\n",
       "      <td>10144</td>\n",
       "      <td>1736</td>\n",
       "      <td>5.843318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>Second Inaugural Address</td>\n",
       "      <td>Monday, March 4, 1805</td>\n",
       "      <td>PROCEEDING, fellow-citizens, to that qualifica...</td>\n",
       "      <td>proceeding  fellow citizens  to that qualifica...</td>\n",
       "      <td>12902</td>\n",
       "      <td>2169</td>\n",
       "      <td>5.948363</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 826 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name         Inaugural Address                      Date  \\\n",
       "0  George Washington   First Inaugural Address  Thursday, April 30, 1789   \n",
       "1  George Washington  Second Inaugural Address     Monday, March 4, 1793   \n",
       "2         John Adams         Inaugural Address   Saturday, March 4, 1797   \n",
       "3   Thomas Jefferson   First Inaugural Address  Wednesday, March 4, 1801   \n",
       "4   Thomas Jefferson  Second Inaugural Address     Monday, March 4, 1805   \n",
       "\n",
       "                                                text  \\\n",
       "0  Fellow-Citizens of the Senate and of the House...   \n",
       "1  Fellow Citizens:  I AM again called upon by th...   \n",
       "2  WHEN it was first perceived, in early times, t...   \n",
       "3  Friends and Fellow-Citizens:  CALLED upon to u...   \n",
       "4  PROCEEDING, fellow-citizens, to that qualifica...   \n",
       "\n",
       "                                          text_clean  char_cnt  word_cnt  \\\n",
       "0  fellow citizens of the senate and of the house...      8616      1432   \n",
       "1  fellow citizens   i am again called upon by th...       787       135   \n",
       "2  when it was first perceived  in early times  t...     13871      2323   \n",
       "3  friends and fellow citizens   called upon to u...     10144      1736   \n",
       "4  proceeding  fellow citizens  to that qualifica...     12902      2169   \n",
       "\n",
       "   avg_word_length  Counts_abiding  Counts_ability  ...  Counts_women  \\\n",
       "0         6.016760               0               0  ...             0   \n",
       "1         5.829630               0               0  ...             0   \n",
       "2         5.971158               0               0  ...             0   \n",
       "3         5.843318               0               0  ...             0   \n",
       "4         5.948363               0               0  ...             0   \n",
       "\n",
       "   Counts_words  Counts_work  Counts_wrong  Counts_year  Counts_years  \\\n",
       "0             0            0             0            0             1   \n",
       "1             0            0             0            0             0   \n",
       "2             0            0             0            2             3   \n",
       "3             0            1             2            0             0   \n",
       "4             0            0             0            2             2   \n",
       "\n",
       "   Counts_yet  Counts_you  Counts_young  Counts_your  \n",
       "0           0           5             0            9  \n",
       "1           0           0             0            1  \n",
       "2           0           0             0            1  \n",
       "3           2           7             0            7  \n",
       "4           2           4             0            4  \n",
       "\n",
       "[5 rows x 826 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with the feature names\n",
    "cv_df = pd.DataFrame(cv_array, columns = cv.get_feature_names_out()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "speech_df_new.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new features combined with the orginial DataFrame they can be now used for ML models or analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Representation\n",
    "  \n",
    "While counts of occurrences of words can be a good first step towards encoding your text to build models, it has some limitations. The main issue is counts will be much higher for very common even when they occur across all texts, providing little value as a distinguishing feature.\n",
    "  \n",
    "**Introducing TF-IDF**\n",
    "  \n",
    "Take for example the counts of the word \"the\" shown here, with plentiful occurrences in every row. To limit these common words from overpowering your model some form of normalization can be used. One of the most effective approaches to do this is called \"*Term Frequency Inverse Document Frequency*\" or TF-IDF.\n",
    "  \n",
    "<img src='../_images/tf-idf-dealing-with-text-data-ml.png' text='alt text' width='500'>\n",
    "  \n",
    "**TF-IDF**\n",
    "  \n",
    "TF-IDF divides number of times a word occurs in the document by a measure of what proportion of the documents a word occurs in all documents. This has the effect of reducing the value of common words, while increasing the weight of words that do not occur in many documents.\n",
    "  \n",
    "$\\Large \\text{TF-IDF} = \\frac{\\frac{{\\text{(count of word occurrences)}}}{{\\text{(total words in a document)}}}} {\\log{\\left( \\frac{{\\text{(number of documents word is in)}}}{{\\text{(total number of documents)}}} \\right)}}$\n",
    "  \n",
    "**Importing the vectorizer**\n",
    "  \n",
    "To use a TF-IDF vectorizer, the approach is very similar to how you applied a count vectorizer.  \n",
    "  \n",
    "`from sklearn.feature_extraction.text import TfidfVectorizer`  \n",
    "  \n",
    "Then assign it to a variable name. Lets use `tv` in this case.\n",
    "  \n",
    "<img src='../_images/tf-idf-dealing-with-text-data-ml1.png' text='alt text' width='500'>\n",
    "  \n",
    "**Max features and stopwords**\n",
    "  \n",
    "Similar to when you were working with the `CountVectorizer` where you could limit the number of features created by specifying arguments when initializing `TfidfVectorizer`, you can specify the maximum number of features using `max_features=` which will only use the 100 most common words. We will also specify the vectorizer to omit a set of `stop_words=`, these are a predefined list of the most common english words such as \"and\" or \"the\". You can use scikit-learn's built in list, load your own, or use lists provided by other python libraries.\n",
    "  \n",
    "**Fitting your text**\n",
    "  \n",
    "Once the vectorizer has been specified you can fit it, and apply it to the text that you want to transform. Note that here we are fitting and transforming the train data, a subset of the original data.\n",
    "  \n",
    "<img src='../_images/tf-idf-dealing-with-text-data-ml2.png' text='alt text' width='500'>\n",
    "  \n",
    "**Putting it all together**\n",
    "  \n",
    "As before, you combine the TF-IDF values along with the feature names in a DataFrame as shown here.\n",
    "  \n",
    "<img src='../_images/tf-idf-dealing-with-text-data-ml3.png' text='alt text' width='500'>\n",
    "  \n",
    "**Inspecting your transforms**\n",
    "  \n",
    "After transforming your data you should always check how the different words are being valued, and see which words are receiving the highest scores through the process. This will help you understand if the features being generated make sense or not. One ad-hoc method is to isolate a single row of the transformed DataFrame (`tv_df` in this case), using the `.iloc[]` accessor, and then sorting the values in the row in descending order as shown here. These top ranked values make sense for the text of a presidential speech.\n",
    "  \n",
    "<img src='../_images/tf-idf-dealing-with-text-data-ml4.png' text='alt text' width='500'>\n",
    "  \n",
    "**Applying the vectorizer to new data**\n",
    "  \n",
    "So how do you apply this transformation on the test set? As mentioned before, you should preprocess your test data using the transformations made on the train data only. To ensure that the same features are created you should use the same vectorizer that you fit on the training data. So first transform the test data using the `tv` vectorizer and then recreate the test dataset by combining the TF-IDF values, feature names, and other columns.\n",
    "  \n",
    "<img src='../_images/tf-idf-dealing-with-text-data-ml5.png' text='alt text' width='500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "  \n",
    "While counts of occurrences of words can be useful to build models, words that occur many times may skew the results undesirably. To limit these common words from overpowering your model a form of normalization can be used. In this lesson you will be using *Term frequency-inverse document frequency* (Tf-idf) as was discussed in the video.  \n",
    "  \n",
    "**Tf-idf has the effect of reducing the value of common words, while increasing the weight of words that do not occur in many documents.**\n",
    "  \n",
    "1. `from sklearn.feature_extraction.text import TfidfVectorizer`\n",
    "2. Instantiate `TfidfVectorizer` while limiting the number of features to 100 and removing English stop words.\n",
    "3. Fit and apply the vectorizer on `text_clean` column in one step.\n",
    "4. Create a DataFrame `tv_df` containing the weights of the words and the feature names as the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TFIDF_action</th>\n",
       "      <th>TFIDF_administration</th>\n",
       "      <th>TFIDF_america</th>\n",
       "      <th>TFIDF_american</th>\n",
       "      <th>TFIDF_americans</th>\n",
       "      <th>TFIDF_believe</th>\n",
       "      <th>TFIDF_best</th>\n",
       "      <th>TFIDF_better</th>\n",
       "      <th>TFIDF_change</th>\n",
       "      <th>TFIDF_citizens</th>\n",
       "      <th>...</th>\n",
       "      <th>TFIDF_things</th>\n",
       "      <th>TFIDF_time</th>\n",
       "      <th>TFIDF_today</th>\n",
       "      <th>TFIDF_union</th>\n",
       "      <th>TFIDF_united</th>\n",
       "      <th>TFIDF_war</th>\n",
       "      <th>TFIDF_way</th>\n",
       "      <th>TFIDF_work</th>\n",
       "      <th>TFIDF_world</th>\n",
       "      <th>TFIDF_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136012</td>\n",
       "      <td>0.203593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045929</td>\n",
       "      <td>0.052694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261016</td>\n",
       "      <td>0.266097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092436</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.073018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026112</td>\n",
       "      <td>0.060460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032030</td>\n",
       "      <td>0.021214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062823</td>\n",
       "      <td>0.070529</td>\n",
       "      <td>0.024339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063643</td>\n",
       "      <td>0.073018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.117831</td>\n",
       "      <td>0.045471</td>\n",
       "      <td>0.053335</td>\n",
       "      <td>0.223369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039277</td>\n",
       "      <td>0.095729</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.041334</td>\n",
       "      <td>0.039761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067393</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>0.091514</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082667</td>\n",
       "      <td>0.164256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121605</td>\n",
       "      <td>0.030338</td>\n",
       "      <td>0.094225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054752</td>\n",
       "      <td>0.062817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TFIDF_action  TFIDF_administration  TFIDF_america  TFIDF_american  \\\n",
       "0      0.000000              0.133415       0.000000        0.105388   \n",
       "1      0.000000              0.261016       0.266097        0.000000   \n",
       "2      0.000000              0.092436       0.157058        0.073018   \n",
       "3      0.000000              0.092693       0.000000        0.000000   \n",
       "4      0.041334              0.039761       0.000000        0.031408   \n",
       "\n",
       "   TFIDF_americans  TFIDF_believe  TFIDF_best  TFIDF_better  TFIDF_change  \\\n",
       "0              0.0       0.000000    0.000000      0.000000      0.000000   \n",
       "1              0.0       0.000000    0.000000      0.000000      0.000000   \n",
       "2              0.0       0.000000    0.026112      0.060460      0.000000   \n",
       "3              0.0       0.090942    0.117831      0.045471      0.053335   \n",
       "4              0.0       0.000000    0.067393      0.039011      0.091514   \n",
       "\n",
       "   TFIDF_citizens  ...  TFIDF_things  TFIDF_time  TFIDF_today  TFIDF_union  \\\n",
       "0        0.229644  ...      0.000000    0.045929          0.0     0.136012   \n",
       "1        0.179712  ...      0.000000    0.000000          0.0     0.000000   \n",
       "2        0.106072  ...      0.032030    0.021214          0.0     0.062823   \n",
       "3        0.223369  ...      0.048179    0.000000          0.0     0.094497   \n",
       "4        0.273760  ...      0.082667    0.164256          0.0     0.121605   \n",
       "\n",
       "   TFIDF_united  TFIDF_war  TFIDF_way  TFIDF_work  TFIDF_world  TFIDF_years  \n",
       "0      0.203593   0.000000   0.060755    0.000000     0.045929     0.052694  \n",
       "1      0.199157   0.000000   0.000000    0.000000     0.000000     0.000000  \n",
       "2      0.070529   0.024339   0.000000    0.000000     0.063643     0.073018  \n",
       "3      0.000000   0.036610   0.000000    0.039277     0.095729     0.000000  \n",
       "4      0.030338   0.094225   0.000000    0.000000     0.054752     0.062817  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')  # Only use 100 most common words, stop_words are in english\n",
    "\n",
    "# Fit the vectorizer and transform the data\n",
    "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "tv_df = pd.DataFrame(tv_transformed.toarray(), columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "tv_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that counting the word occurences and calculating the Tf-idf weights are very similar? This is one of the reasons scikit-learn is very popular, a consistent API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Tf-idf values\n",
    "  \n",
    "After creating Tf-idf features you will often want to understand what are the most highest scored words for each corpus. This can be achieved by isolating the row you want to examine and then sorting the the scores from high to low.\n",
    "  \n",
    "The DataFrame from the last exercise (`tv_df`) is available in your workspace.\n",
    "  \n",
    "1. Assign the first row of `tv_df` to `sample_row`.\n",
    "2. `sample_row` is now a series of weights assigned to words. Sort these values to print the top 5 highest-rated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF_government    0.367430\n",
      "TFIDF_public        0.333237\n",
      "TFIDF_present       0.315182\n",
      "TFIDF_duty          0.238637\n",
      "TFIDF_country       0.229644\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Isolate the row to be examined\n",
    "sample_row = tv_df.iloc[0]  # .iloc[] goes by rows initially, and first row is index=0\n",
    "\n",
    "# Print the top 5 words of the sorted output\n",
    "print(sample_row.sort_values(ascending=False).head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think these scores make sense for the corresponding words?\n",
    "  \n",
    "Yes, as the corpus we have is presidential inaugural speeches, so the presence of words like 'government', 'public', and 'country' to name a few make sense to occur as frequently as they do. With occurance frequencies between 23% and 36.7%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming unseen data\n",
    "  \n",
    "When creating vectors from text, any transformations that you perform before training a machine learning model, you also need to apply on the new unseen (test) data. To achieve this follow the same approach from the last chapter: *fit the vectorizer only on the training data, and apply it to the test data*.\n",
    "  \n",
    "For this exercise the `speech_df` DataFrame has been split in two:\n",
    "  \n",
    "`train_speech_df` : The training set consisting of the first 45 speeches.  \n",
    "`test_speech_df` : The test set consisting of the remaining speeches.  \n",
    "  \n",
    "1. Instantiate `TfidfVectorizer`.\n",
    "2. Fit the vectorizer and apply it to the `text_clean` column.\n",
    "3. Apply the same vectorizer on the `text_clean` column of the test data.\n",
    "4. Create a DataFrame of these new features from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_speech_df = speech_df.iloc[:45]   # First 45 speeches, all rows up to 45th, 0-based indexing (0 to 44)\n",
    "test_speech_df = speech_df.iloc[45:]    # Starting at the 45th index take the remaining rows from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TFIDF_action</th>\n",
       "      <th>TFIDF_administration</th>\n",
       "      <th>TFIDF_america</th>\n",
       "      <th>TFIDF_american</th>\n",
       "      <th>TFIDF_authority</th>\n",
       "      <th>TFIDF_best</th>\n",
       "      <th>TFIDF_business</th>\n",
       "      <th>TFIDF_citizens</th>\n",
       "      <th>TFIDF_commerce</th>\n",
       "      <th>TFIDF_common</th>\n",
       "      <th>...</th>\n",
       "      <th>TFIDF_subject</th>\n",
       "      <th>TFIDF_support</th>\n",
       "      <th>TFIDF_time</th>\n",
       "      <th>TFIDF_union</th>\n",
       "      <th>TFIDF_united</th>\n",
       "      <th>TFIDF_war</th>\n",
       "      <th>TFIDF_way</th>\n",
       "      <th>TFIDF_work</th>\n",
       "      <th>TFIDF_world</th>\n",
       "      <th>TFIDF_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>0.233954</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024648</td>\n",
       "      <td>0.079050</td>\n",
       "      <td>0.033313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299983</td>\n",
       "      <td>0.134749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547457</td>\n",
       "      <td>0.036862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019296</td>\n",
       "      <td>0.092567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052851</td>\n",
       "      <td>0.066817</td>\n",
       "      <td>0.078999</td>\n",
       "      <td>0.277701</td>\n",
       "      <td>0.126126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126987</td>\n",
       "      <td>0.134669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080272</td>\n",
       "      <td>0.042907</td>\n",
       "      <td>0.054245</td>\n",
       "      <td>0.096203</td>\n",
       "      <td>0.225452</td>\n",
       "      <td>0.043884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037094</td>\n",
       "      <td>0.067428</td>\n",
       "      <td>0.267012</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>0.039990</td>\n",
       "      <td>0.061516</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>0.077301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098819</td>\n",
       "      <td>0.210690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056262</td>\n",
       "      <td>0.030073</td>\n",
       "      <td>0.038020</td>\n",
       "      <td>0.235998</td>\n",
       "      <td>0.237026</td>\n",
       "      <td>0.061516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221561</td>\n",
       "      <td>0.156644</td>\n",
       "      <td>0.028442</td>\n",
       "      <td>0.087505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023428</td>\n",
       "      <td>0.187313</td>\n",
       "      <td>0.131913</td>\n",
       "      <td>0.040016</td>\n",
       "      <td>0.021389</td>\n",
       "      <td>0.081124</td>\n",
       "      <td>0.119894</td>\n",
       "      <td>0.299701</td>\n",
       "      <td>0.153133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TFIDF_action  TFIDF_administration  TFIDF_america  TFIDF_american  \\\n",
       "0      0.000000              0.029540       0.233954        0.082703   \n",
       "1      0.000000              0.000000       0.547457        0.036862   \n",
       "2      0.000000              0.000000       0.126987        0.134669   \n",
       "3      0.037094              0.067428       0.267012        0.031463   \n",
       "4      0.000000              0.000000       0.221561        0.156644   \n",
       "\n",
       "   TFIDF_authority  TFIDF_best  TFIDF_business  TFIDF_citizens  \\\n",
       "0         0.000000    0.000000        0.000000        0.022577   \n",
       "1         0.000000    0.036036        0.000000        0.015094   \n",
       "2         0.000000    0.131652        0.000000        0.000000   \n",
       "3         0.039990    0.061516        0.050085        0.077301   \n",
       "4         0.028442    0.087505        0.000000        0.109959   \n",
       "\n",
       "   TFIDF_commerce  TFIDF_common  ...  TFIDF_subject  TFIDF_support  \\\n",
       "0             0.0      0.000000  ...            0.0       0.000000   \n",
       "1             0.0      0.000000  ...            0.0       0.019296   \n",
       "2             0.0      0.046997  ...            0.0       0.000000   \n",
       "3             0.0      0.000000  ...            0.0       0.098819   \n",
       "4             0.0      0.023428  ...            0.0       0.023428   \n",
       "\n",
       "   TFIDF_time  TFIDF_union  TFIDF_united  TFIDF_war  TFIDF_way  TFIDF_work  \\\n",
       "0    0.115378     0.000000      0.024648   0.079050   0.033313    0.000000   \n",
       "1    0.092567     0.000000      0.000000   0.052851   0.066817    0.078999   \n",
       "2    0.075151     0.000000      0.080272   0.042907   0.054245    0.096203   \n",
       "3    0.210690     0.000000      0.056262   0.030073   0.038020    0.235998   \n",
       "4    0.187313     0.131913      0.040016   0.021389   0.081124    0.119894   \n",
       "\n",
       "   TFIDF_world  TFIDF_years  \n",
       "0     0.299983     0.134749  \n",
       "1     0.277701     0.126126  \n",
       "2     0.225452     0.043884  \n",
       "3     0.237026     0.061516  \n",
       "4     0.299701     0.153133  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer to the training data and transform it\n",
    "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "\n",
    "# Transform test data\n",
    "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "\n",
    "# Create new features for the test set\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "test_tv_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct, the vectorizer should only be fit on the train set, never on your test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words and N-grams\n",
    "  \n",
    "So far you have looked at individual words on their own without any context or word order, this approach is called a bag-of-words model, as the words are treated as if they are being drawn from a bag with no concept of order or grammar. While analyzing the occurrences of individual words can be a valuable way to create features from a piece of text, you will notice that individual words can loose all their context/meaning when viewed independently.\n",
    "  \n",
    "**Issues with bag of words**\n",
    "  \n",
    "Take for example the word 'happy' shown here. One would assume it was used in a positive context, but if in reality it was used in the phrase 'not happy' this assumption would be incorrect. Similarly if the phrase was extended to 'never not happy' the connotation changes again. One common method to retain at least some concept of word order in a text is to instead use multiple consecutive words like pairs (bi-gram) or three consecutive words (tri-grams). This maintains at least some ordering information while at the same time allowing for the creation of a reasonable set of features.\n",
    "  \n",
    "<img src='../_images/issues-with-bag-of-words-n-grams.png' text='alt text' width='500'>\n",
    "  \n",
    "**Using N-grams**\n",
    "  \n",
    "To leverage n-grams in your own models an additional argument \"ngram_range\", can be specified when instantiating your TF-IDF vectorizer. The values assigned to the argument are the minimum and maximum length of n-grams to be included. In this case you would only be looking at bi-grams (n-grams with two words) Printing the bi-gram features created we can see the pairs of words instead of single words.\n",
    "  \n",
    "<img src='../_images/issues-with-bag-of-words-n-grams2.png' text='alt text' width='500'>\n",
    "  \n",
    "**Finding common words**\n",
    "  \n",
    "As mentioned in the last video, when creating new features, you should always take time to check your work, and ensure that the features you are creating make sense. A good way to check your n-grams is to see what are the most common values being recorded. This can be done by summing the values of your DataFrame of count values that you created using the `.sum()` method.\n",
    "  \n",
    "<img src='../_images/issues-with-bag-of-words-n-grams3.png' text='alt text' width='500'>\n",
    "  \n",
    "**Finding common words**\n",
    "  \n",
    "After sorting the values in descending order you can see the most commonly occurring values. It comes as no surprise that the most commonly occurring bi-gram in a dataset of US president's speeches is \"United States\" which indicates that the features being created make sense.\n",
    "  \n",
    "<img src='../_images/issues-with-bag-of-words-n-grams4.png' text='alt text' width='500'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using longer n-grams\n",
    "  \n",
    "So far you have created features based on individual words in each of the texts. This can be quite powerful when used in a machine learning model but you may be concerned that by looking at words individually a lot of the context is being ignored. To deal with this when creating models you can use n-grams which are sequence of $n$ words grouped together. \n",
    "  \n",
    "For example:\n",
    "  \n",
    "**bigrams**: Sequences of two consecutive words  \n",
    "**trigrams**: Sequences of two consecutive words  \n",
    "  \n",
    "These can be automatically created in your dataset by specifying the `ngram_range=` argument as a tuple (`n1`, `n2`) where all n-grams in the `n1` to `n2` range are included.\n",
    "  \n",
    "1. `from sklearn.feature_extraction.text import CountVectorizer`\n",
    "2. Instantiate `CountVectorizer` while considering only trigrams.\n",
    "3. Fit the vectorizer and apply it to the `text_clean` column in one step.\n",
    "4. Print the feature names generated by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability preserve protect', 'agriculture commerce manufactures',\n",
       "       'america ideal freedom', 'amity mutual concession',\n",
       "       'anchor peace home', 'ask bow heads', 'best ability preserve',\n",
       "       'best interests country', 'bless god bless', 'bless united states'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a trigram vectorizer\n",
    "cv_trigram_vec = CountVectorizer(\n",
    "    max_features=100, \n",
    "    stop_words='english', \n",
    "    ngram_range=(3, 3)\n",
    ")\n",
    "\n",
    "# Fit and apply trigram vectorizer\n",
    "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the trigram features\n",
    "cv_trigram_vec.get_feature_names_out()[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that by taking sequential word pairings, some context is preserved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most common words\n",
    "  \n",
    "Its always advisable once you have created your features to inspect them to ensure that they are as you would expect. This will allow you to catch errors early, and perhaps influence what further feature engineering you will need to do.\n",
    "  \n",
    "The vectorizer (`cv`) you fit in the last exercise and the sparse array consisting of word counts (`cv_trigram`) is available in your workspace.\n",
    "  \n",
    "1. Create a DataFrame of the features (word counts).\n",
    "2. Add the counts of word occurrences and print the top 5 most occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counts_constitution united states    20\n",
       "Counts_people united states          13\n",
       "Counts_mr chief justice              10\n",
       "Counts_preserve protect defend       10\n",
       "Counts_president united states        8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame of the features\n",
    "cv_tri_df = pd.DataFrame(cv_trigram.toarray(), columns=cv_trigram_vec.get_feature_names_out()).add_prefix('Counts_')\n",
    "\n",
    "# Print the top 5 words in the sorted descending output\n",
    "cv_tri_df.sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that the most common trigram is constitution united states makes a lot of sense for US presidents speeches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
