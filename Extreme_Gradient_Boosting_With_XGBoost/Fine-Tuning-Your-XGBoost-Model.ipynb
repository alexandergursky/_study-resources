{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning your XGBoost model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why tune your model?\n",
    "### Tuning the number of boosting rounds\n",
    "  \n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use `xgb.cv()` inside a for loop and build one model per `num_boost_round` parameter. Working with the Ames housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Load df\n",
    "df = pd.read_csv('../_datasets/ames_housing_trimmed_processed.csv')\n",
    "\n",
    "# X/y split\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.299752\n",
      "1                   10  34774.194090\n",
      "2                   15  32895.099185\n"
     ]
    }
   ],
   "source": [
    "# SEED\n",
    "SEED = 123\n",
    "\n",
    "# Creating the DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data= X, label= y)\n",
    "\n",
    "# Creating the parameter dictionary for each tree\n",
    "params = {\n",
    "    'objective' : 'reg:squarederror',\n",
    "    'max_depth' : 3\n",
    "}\n",
    "\n",
    "# Creating a list containing the number of boosting rounds\n",
    "number_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list for storing the final round RMSE per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in number_rounds:\n",
    "    # Preforming cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain= housing_dmatrix,\n",
    "        params= params,\n",
    "        nfold= 3,\n",
    "        num_boost_round= curr_num_rounds,\n",
    "        metrics= 'rmse',\n",
    "        as_pandas= True,\n",
    "        seed=SEED\n",
    "    )\n",
    "    # Append final round RMSE to the empty list\n",
    "    final_rmse_per_round.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "\n",
    "# Printing the resulting dataframe\n",
    "# zip() is a generator that combines these two together, we then create a list from it.\n",
    "# Lastly we use this list as the data structure, giving each a column name.\n",
    "number_rounds_rmses = list(zip(number_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(number_rounds_rmses, columns=['num_boosting_rounds', 'rmse']))\n",
    "\n",
    "# Increasing the number of boosting rounds decreases the RMSE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within `xgb.cv()`. This is done using a technique called **early stopping**.  \n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (**\"rmse\"** in our case) does not improve for a given number of rounds.  \n",
    "Here you will use the `early_stopping_rounds` parameter in `xgb.cv()` with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when `num_boost_rounds` is reached, then early stopping does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.635216      403.633062   142640.653507     705.559723\n",
      "1     103057.033818       73.768079   104907.664683     111.117033\n",
      "2      75975.967655      253.727043    79262.056654     563.766693\n",
      "3      57420.530642      521.658273    61620.137859    1087.693428\n",
      "4      44552.956483      544.170426    50437.560906    1846.446643\n",
      "5      35763.948865      681.796675    43035.659539    2034.471115\n",
      "6      29861.464164      769.571418    38600.880800    2169.796804\n",
      "7      25994.675122      756.520639    36071.817710    2109.795408\n",
      "8      23306.836299      759.237848    34383.186387    1934.547433\n",
      "9      21459.770256      745.624640    33509.140338    1887.375358\n",
      "10     20148.721060      749.612186    32916.806725    1850.893437\n",
      "11     19215.382607      641.387200    32197.833474    1734.456654\n",
      "12     18627.388962      716.256240    31770.852340    1802.154296\n",
      "13     17960.695080      557.043324    31482.782172    1779.124406\n",
      "14     17559.736640      631.413137    31389.990252    1892.320326\n",
      "15     17205.713357      590.171774    31302.883291    1955.165882\n",
      "16     16876.571801      703.631953    31234.058914    1880.706205\n",
      "17     16597.662170      703.677363    31318.347820    1828.860754\n",
      "18     16330.460661      607.274258    31323.634893    1775.909992\n",
      "19     16005.972387      520.470815    31204.135450    1739.076237\n",
      "20     15814.300847      518.604822    31089.863868    1756.022175\n",
      "21     15493.405856      505.616461    31047.997697    1624.673447\n",
      "22     15270.734205      502.018639    31056.916210    1668.043691\n",
      "23     15086.381896      503.913078    31024.984403    1548.985086\n",
      "24     14917.608289      486.206137    30983.685376    1663.131135\n",
      "25     14709.589477      449.668262    30989.476981    1686.667218\n",
      "26     14457.286251      376.787759    30952.113767    1613.172390\n",
      "27     14185.567149      383.102597    31066.901381    1648.534545\n",
      "28     13934.066721      473.465580    31095.641882    1709.225578\n",
      "29     13749.644941      473.670743    31103.886799    1778.879849\n",
      "30     13549.836644      454.898742    30976.084872    1744.514518\n",
      "31     13413.484678      399.603422    30938.469354    1746.053330\n",
      "32     13275.915700      415.408595    30931.000055    1772.469405\n",
      "33     13085.878211      493.792795    30929.056846    1765.541040\n",
      "34     12947.181279      517.790033    30890.629160    1786.510472\n",
      "35     12846.027264      547.732747    30884.493051    1769.728787\n",
      "36     12702.378727      505.523140    30833.542124    1691.002007\n",
      "37     12532.244170      508.298300    30856.688154    1771.445485\n",
      "38     12384.055037      536.224929    30818.016568    1782.785175\n",
      "39     12198.443769      545.165604    30839.393263    1847.326671\n",
      "40     12054.583621      508.841802    30776.965294    1912.780332\n",
      "41     11897.036784      477.177932    30794.702627    1919.675130\n",
      "42     11756.221708      502.992363    30780.956160    1906.820178\n",
      "43     11618.846752      519.837483    30783.754746    1951.260120\n",
      "44     11484.080227      578.428500    30776.731276    1953.447810\n",
      "45     11356.552654      565.368946    30758.543732    1947.454939\n",
      "46     11193.557745      552.298986    30729.971937    1985.699239\n",
      "47     11071.315547      604.090125    30732.663173    1966.997252\n",
      "48     10950.778492      574.862853    30712.241251    1957.750615\n",
      "49     10824.865446      576.665678    30720.853939    1950.511037\n"
     ]
    }
   ],
   "source": [
    "# Creating the housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data= X, label= y)\n",
    "\n",
    "# Creating the parameter dictionary for each tree\n",
    "params = {\n",
    "    'objective' : 'reg:squarederror',\n",
    "    'max_depth' : 4\n",
    "}\n",
    "\n",
    "# Preform cross-validation with early-stopping\n",
    "cv_results = xgb.cv(\n",
    "    dtrain= housing_dmatrix,\n",
    "    nfold= 3,\n",
    "    params= params,\n",
    "    metrics= 'rmse',\n",
    "    early_stopping_rounds= 10,\n",
    "    num_boost_round= 50,\n",
    "    as_pandas= True,\n",
    "    seed= SEED\n",
    ")\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of XGBoost's hyperparameters\n",
    "\n",
    "Common tree tunable parameters  \n",
    "- `eta`: learning rate/eta  \n",
    "The learning rate affects how quickly the model fits the residual error using additional base learners. A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an XGBoost model with a high learning rate.  \n",
    "  \n",
    "- `gamma`: min loss reduction to create new tree split  \n",
    "Has an effect on how strongly regularized the trained model will be.  \n",
    "  \n",
    "- `lambda`: L2 regularization on leaf weights  \n",
    "Has an effect on how strongly regularized the trained model will be.  \n",
    "  \n",
    "- `alpha`: L1 regularization on leaf weights  \n",
    "Has an effect on how strongly regularized the trained model will be.  \n",
    "  \n",
    "- `max_depth`: max depth per tree  \n",
    "Max_depth must be a positive integer value and affects how deeply each tree is allowed to grow during any given boosting round.  \n",
    "  \n",
    "- `subsample`: % of samples used per tree  \n",
    "Subsample must be a value between 0 and 1 and is the fraction of the total training set that can be used for any given boosting round. If the value is low, then the fraction of your training data used per boosting round would be low and you may run into underfitting problems, a value that is very high can lead to overfitting as well.  \n",
    "  \n",
    "- `colsample_bytree`: % of features used per tree  \n",
    "Colsample_bytree is the fraction of features you can select from during any given boosting round and must also be a value between 0 and 1. A large value means that almost all features can be used to build a tree during a given boosting round, whereas a small value means that the fraction of features that can be selected from is very small. In general, smaller colsample_bytree values can be thought of as providing additional regularization to the model, whereas using all columns may in certain cases overfit a trained model.\n",
    "  \n",
    "Linear tunable parameters\n",
    "- `lambda`: L2 reg on weights  \n",
    "  \n",
    "- `alpha`: L1 reg on weights  \n",
    "  \n",
    "- `lambda_bias`: L2 reg term on bias\n",
    "  \n",
    "Its important to mention that the number of boosting rounds (that is, either the number of trees you build or the number of linear base learners you construct) is itself a tunable parameter, ie. you can also tune the number of estimators used for both base model types."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning eta\n",
    "  \n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning \"`eta`\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"`eta`\" penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.402543\n",
      "1  0.010  179932.183986\n",
      "2  0.100   79759.411808\n"
     ]
    }
   ],
   "source": [
    "# Creating the DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data= X, label= y)\n",
    "\n",
    "# Creating the parameter dictionary for each tree in the boosting round\n",
    "params = {\n",
    "    'objective' : 'reg:squarederror',\n",
    "    'max_depth' : 3\n",
    "}\n",
    "\n",
    "# Creating list of eta values and an empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta\n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    # Appending the eta value to the grid, each iter it changes to the value which is in the eta_vals list.\n",
    "    params['eta'] = curr_val\n",
    "\n",
    "    # performing cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain= housing_dmatrix,\n",
    "        params = params,\n",
    "        nfold= 3,\n",
    "        early_stopping_rounds= 5,\n",
    "        num_boost_round= 10,\n",
    "        metrics= 'rmse',\n",
    "        seed= SEED,\n",
    "        as_pandas= True\n",
    "    )\n",
    "\n",
    "    # Append the final round rmse to best_rmse\n",
    "    # The tail() method is used to select the last 5 values of this column (by default, tail() returns the last 5 rows of a DataFrame).\n",
    "    # The values attribute is used to retrieve the underlying NumPy array of these 5 selected rows. Since we are selecting 5 rows, \n",
    "    # this will return a NumPy array with 5 elements. We then use [-1] to extract the last (i.e., most recent) element from the array, \n",
    "    # which corresponds to the mean RMSE of the most recently evaluated model.\n",
    "    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "\n",
    "# Print the resulting Dataframe\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns= ['eta', 'best_rmse']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth\n",
    "  \n",
    "In this exercise, your job is to tune `max_depth`, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.469464\n",
      "1          5  35596.599504\n",
      "2         10  36065.547345\n",
      "3         20  36739.576068\n"
     ]
    }
   ],
   "source": [
    "# Create DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data= X, label= y)\n",
    "\n",
    "# Create parameter dictionary\n",
    "params = {\n",
    "    'objective' : 'reg:squarederror',\n",
    "}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "\n",
    "# Empty list to store values\n",
    "best_rmse = []\n",
    "\n",
    "# Loop for tuning max_depth, systematically varying the parameter\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    # Loop param max_depth\n",
    "    params['max_depth'] = curr_val\n",
    "\n",
    "    # Preform cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain= housing_dmatrix,\n",
    "        params= params,\n",
    "        nfold= 2,\n",
    "        early_stopping_rounds= 5,\n",
    "        num_boost_round= 10,\n",
    "        seed= SEED,\n",
    "        as_pandas= True\n",
    "    )\n",
    "\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "\n",
    "# Print results\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)), columns=['max_depth', 'best_rmse']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning colsample_bytree\n",
    "  \n",
    "Now, it's time to tune \"`colsample_bytree`\". You've already seen this if you've ever worked with scikit-learn's `RandomForestClassifier` or `RandomForestRegressor`, where it just was called `max_features`. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, `colsample_bytree` must be specified as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  51386.578876\n",
      "1               0.5  36585.351887\n",
      "2               0.8  36093.663501\n",
      "3               1.0  35836.044343\n"
     ]
    }
   ],
   "source": [
    "# Creating DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data= X, label= y)\n",
    "\n",
    "# Creating param dictionary\n",
    "params = {\n",
    "    'objective' : 'reg:squarederror',\n",
    "    'max_depth' : 3\n",
    "}\n",
    "\n",
    "# Creating list of hyperparameter values for colsample_bytree\n",
    "cosample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "\n",
    "# Empty list for storage\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value\n",
    "for curr_val in cosample_bytree_vals:\n",
    "\n",
    "    # Append varying colsample_bytree value to grid\n",
    "    params['colsample_bytree'] = curr_val\n",
    "\n",
    "    cv_results = xgb.cv(\n",
    "        dtrain= housing_dmatrix,\n",
    "        params= params,\n",
    "        nfold= 2,\n",
    "        num_boost_round= 10,\n",
    "        early_stopping_rounds= 5,\n",
    "        metrics= 'rmse',\n",
    "        as_pandas= True,\n",
    "        seed= SEED\n",
    "    )\n",
    "\n",
    "    # Append final round to best_rmse\n",
    "    best_rmse.append(cv_results['test-rmse-mean'].tail().values[-1])\n",
    "\n",
    "# Display results\n",
    "print(pd.DataFrame(list(zip(cosample_bytree_vals, best_rmse)), columns=['colsample_bytree', 'best_rmse']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several other individual parameters that you can tune, such as \"`subsample`\", which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of grid search and random search\n",
    "\n",
    "Grid Search: Review  \n",
    "Grid search is a method of exhaustively searching through a collection of possible parameter values. For example, if you have 2 hyperparameters you would like to tune, and 4 possible values for each hyperparameter, then a grid search over that parameter space would try all 16 possible parameter configurations. In a grid search, you try every parameter configuration, evaluate some metric for that configuration, and pick the parameter configuration that gave you the best value for the metric you were using, which in our case will be the root mean squared error.\n",
    "- Search exhaustively over a given set of hyperparameters, once per set of hyperparameters\n",
    "- Number of models = number of distinct values per hyperparameter multiplied across each hyperparameter\n",
    "- Pick final model hyperparameter values that give best cross-validated evaluation metric value\n",
    "  \n",
    "Random Search: Review  \n",
    "Random search is significantly different from grid search in that the number of models that you are required to iterate over doesn't grow as you expand the overall hyperparameter space. In random search, you get to decide how many models, or iterations, you want to try out before stopping. Random search simply involves drawing a random combination of possible hyperparameter values from the range of allowable hyperparameters a set number of times. Each time, you train a model with the selected hyperparameters, evaluate the performance of that model, and then rinse and repeat. When you've created the number of models you had specified initially, you simply pick the best one. \n",
    "- Create a (possibly infinte) range of hyperparameter values per hyperparameter that you would like to search over\n",
    "- Set the number of iterations you would like for the random search to continue\n",
    "- During each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters\n",
    "- After you've reached the maximum number of iterations, select the hyperparameter configuration with the best evaluated score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search with XGBoost\n",
    "  \n",
    "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's `GridSearch` and `RandomizedSearch` capabilities with internal cross-validation using the `GridSearchCV` and `RandomizedSearchCV` functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with `GridSearchCV`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "Best parameters found: {'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 50}\n",
      "Lowest RMSE found: 30355.698207097197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Creating the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree' : [0.3, 0.7],\n",
    "    'n_estimators' : [50],\n",
    "    'max_depth' : [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Preform grid search\n",
    "grid_mse = GridSearchCV(\n",
    "    param_grid= gbm_param_grid,\n",
    "    estimator= gbm,\n",
    "    scoring= 'neg_mean_squared_error',\n",
    "    cv= 4,\n",
    "    verbose= 1\n",
    ")\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print('Best parameters found: {}'.format(grid_mse.best_params_))\n",
    "print('Lowest RMSE found: {}'.format(np.sqrt(np.abs(grid_mse.best_score_))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search with XGBoost\n",
    "  \n",
    "Often, `GridSearchCV` can be really time consuming, so in practice, you may want to use `RandomizedSearchCV` instead, as you will do in this exercise. The good news is you only have to make a few modifications to your `GridSearchCV` code to do `RandomizedSearchCV`. The key difference is you have to specify a `param_distributions` parameter instead of a `param_grid` parameter.\n",
    "  \n",
    "GBM stands for \"**Gradient Boosting Machine**\" or \"**Gradient Boosting Model**\". It is a type of ensemble machine learning algorithm used for supervised learning tasks, particularly for regression and classification problems. In the context of the code snippet provided, gbm refers to an instance of the `XGBRegressor` class from the XGBoost library, which is an implementation of the gradient boosting algorithm for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Best parameters found: {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found: 29998.4522530019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "# n_estimators is used in the gbm_param_grid dictionary to specify a range of \n",
    "# values for the number of boosting rounds (i.e., the number of decision trees) \n",
    "# that should be used in the XGBRegressor algorithm. In this case, only one value, 25, is given.\n",
    "gbm_param_grid = {\n",
    "    'n_estimators' : [25],\n",
    "    'max_depth' : range(2,12)  # 2,3,4,5,6,7,8,9,10,11 (10 values total)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor\n",
    "# n_estimators is also used as a fixed value in the instantiation of the gbm \n",
    "# XGBRegressor object. Here, n_estimators is set to 10, which is the initial number \n",
    "# of boosting rounds that the XGBRegressor algorithm will use.\n",
    "gbm = xgb.XGBRegressor(n_estimators= 10)\n",
    "\n",
    "# Preform random search\n",
    "randomized_mse = RandomizedSearchCV(\n",
    "    param_distributions= gbm_param_grid,\n",
    "    estimator= gbm,\n",
    "    scoring= 'neg_mean_squared_error',\n",
    "    n_iter= 5,\n",
    "    cv= 4,\n",
    "    verbose= 1\n",
    ")\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print('Best parameters found: {}'.format(randomized_mse.best_params_))\n",
    "print('Lowest RMSE found: {}'.format(np.sqrt(np.abs(randomized_mse.best_score_))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits of grid search and random search\n",
    "  \n",
    "Grid Search\n",
    "- Number of models you must build with every additionary new parameter grows very quickly  \n",
    "  \n",
    "Random Search\n",
    "- Parameter space to explore can be massive\n",
    "Randomly jumping throughtout the space looking for a \"best\" results becomes a waiting game"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
