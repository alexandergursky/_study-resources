{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Parameters\n",
    "  \n",
    "Building powerful machine learning models depends heavily on the set of hyperparameters used. But with increasingly complex models with lots of options, how do you efficiently find the best settings for your particular problem? In this course you will get practical experience in using some common methodologies for automated hyperparameter tuning in Python using Scikit Learn. These include Grid Search, Random Search & advanced optimization methodologies including Bayesian & Genetic algorithms . You will use a dataset predicting credit card defaults as you build skills to dramatically increase the efficiency and effectiveness of your machine learning model building.\n",
    "  \n",
    "In this introductory chapter you will learn the difference between hyperparameters and parameters. You will practice extracting and analyzing parameters, setting hyperparameter values for several popular machine learning algorithms. Along the way you will learn some best practice tips & tricks for choosing which hyperparameters to tune and what values to set & build learning curves to analyze your hyperparameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Scikit Learn Documentation](https://scikit-learn.org/stable/)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pandas.get_dummies</td>\n",
    "    <td>A function from the Pandas library used to perform one-hot encoding on categorical data.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>sklearn.model_selection.train_test_split</td>\n",
    "    <td>A function from scikit-learn used to split a dataset into training and testing sets for model evaluation.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>sklearn.linear_model.LogisticRegression</td>\n",
    "    <td>A class from scikit-learn representing a logistic regression classifier for binary classification.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>sklearn.ensemble.RandomForestClassifier</td>\n",
    "    <td>A class from scikit-learn representing a random forest classifier for ensemble learning.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>sklearn.metrics.confusion_matrix</td>\n",
    "    <td>A function from scikit-learn used to compute a confusion matrix for classification evaluation.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>sklearn.metrics.accuracy_score</td>\n",
    "    <td>A function from scikit-learn used to calculate the accuracy score for classification predictions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>sklearn.neighbors.KNeighborsClassifier</td>\n",
    "    <td>A class from scikit-learn representing a k-nearest neighbors classifier.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>sklearn.ensemble.GradientBoostingClassifier</td>\n",
    "    <td>A class from scikit-learn representing a gradient boosting classifier for ensemble learning.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>numpy.linspace</td>\n",
    "    <td>A function from NumPy used to create an array of evenly spaced values over a specified range.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>plt.gca()</td>\n",
    "    <td>A function from Matplotlib used to get the current Axes instance.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: scikit-learn  \n",
    "Version: 1.3.0  \n",
    "Summary: A set of python modules for machine learning and data mining  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning in python\n",
    "  \n",
    "**Introduction**\n",
    "  \n",
    "So why study this course? Today algorithms are getting more and more complex, and so the number of hyperparameters to choose from increases. It becomes increasingly important to learn how to efficiently find optimal combinations, as this search will likely take up a large portion of your time. Often it is quite easy to simply run Scikit Learn functions on the default settings or perhaps code from a tutorial or book without really digging under the hood. However, what lies underneath is of vital importance to good model building. You may be surprised by what you find!\n",
    "  \n",
    "**The dataset**\n",
    "  \n",
    "This course will use a dataset about credit card defaults. It contains a number of variables related to the demographics and financial history of a group of people. The target column shows whether or not they defaulted on their next loan payment. It has already been pre-processed and split ready to model. Note that at times we will take smaller samples to ensure we can run the code. You can find out more about it at the link in the slides.\n",
    "  \n",
    "- The dataset relates to credit card defaults\n",
    "- It contains variables related to the financial history of some consumers in Taiwan. It has 30,000 users and 24 attributes\n",
    "- Target is whether they defaulted on their loan\n",
    "- Dataset has already been preprocessed and at times we will take smaller samples to demonstrate a concept\n",
    "  \n",
    "**Parameters Overview**\n",
    "  \n",
    "To understand hyperparameters, let's first start with parameters. What are parameters? Parameters are components of the final model that are learned through the modeling process. Crucially, you do not set these. You cannot set these. The algorithm discovers them through undertaking its steps.\n",
    "  \n",
    "- A parameter is a component of the model learned during the modeling process\n",
    "- You **do not** set these manually (in-fact you can't)\n",
    "- The algorithm discovers these\n",
    "  \n",
    "**Parameters in Logistic Regression**\n",
    "  \n",
    "To make this concrete, consider a simple logistic regression model. We create the estimator and fit to the data with default settings. Since the logistic regression model is a linear model, we will get beta coefficients on our variables. These are found in the `.coef_ `property of our logistic regression object. However, if we print these out we can see it is a bit messy.\n",
    "  \n",
    "<center><img src='../_images/parameter-overview-for-tuning.png' alt='img' width='740'></center>\n",
    "  \n",
    "Let us clean this up by creating a list of original variable names, zipping this up with the coefficients and formatting into a neat DataFrame for easy viewing.\n",
    "  \n",
    "<center><img src='../_images/parameter-overview-for-tuning1.png' alt='img' width='740'></center>\n",
    "  \n",
    "We can now sort the DataFrame and print the top 3 results for brevity. Do you recall setting PAY_0 to have a coefficient of 0.000751? I don't. The coefficients are parameters because we did not set them ourselves and were learned during the modeling process. In our data, the PAY variables relate to how many months people have previously delayed their payments. We can see that having a high number of months of delayed payments, makes someone more likely to default next month.\n",
    "  \n",
    "<center><img src='../_images/parameter-overview-for-tuning2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Where to find Parameters**\n",
    "  \n",
    "To know what parameters an algorithm will produce, you need to Know a bit about the algorithm itself and how it works. And consult the Scikit-Learn documentation to see where the parameter is stored in the returned object. The parameters are found in the documentation for that particular algorithm under the 'Attributes' section, not the parameters section\n",
    "  \n",
    "**Parameters in Random Forest**\n",
    "  \n",
    "So what are the parameters in tree-based models that do not have linear coefficients? The parameters of this model are in the nodes of the trees used to build the model such as what feature was split on and at what value. To demonstrate, let us firstly build a random forest estimator & fit to our data, setting the `max_depth=` to be quite low only for visualization purposes. Then we can pull out a single tree, found in the random forest estimator, `.estimators_` attribute to visualize. For simplicity we will just show the image but you can explore visualizing this yourself using the mentioned packages.\n",
    "  \n",
    "<center><img src='../_images/parameter-overview-for-tuning3.png' alt='img' width='740'></center>\n",
    "  \n",
    "Here we see a graph of the nodes including the variables and values used in the splits. We can see that the very first split was on the variable PAY_4 and it sent samples left or right depending if they had a value above or below for this variable. Do you remember setting this decision? I certainly don't!\n",
    "  \n",
    "<center><img src='../_images/parameter-overview-for-tuning4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Extracting Node Decisions**\n",
    "  \n",
    "So how do we pull out the splits we saw here visually in a programmatic way? Let's say, the left, second-from-top node. The tree we pulled out is a Scikit Learn 'tree' object so we can find the variable it split on by indexing into the `.feature` attribute of this tree and matching up with our X_train columns to get the name. The level used to split is then found in the `.threshold` attribute. And we can then print this out.\n",
    "  \n",
    "<center><img src='../_images/parameter-overview-for-tuning5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's do some exercises to further explore the parameters of these models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters in Logistic Regression\n",
    "  \n",
    "Now that you have had a chance to explore what a parameter is, let us apply this knowledge. It is important to be able to review any new algorithm and identify which elements are parameters and hyperparameters.\n",
    "  \n",
    "Which of the following is a parameter for the Scikit-Learn logistic regression model? Here we mean conceptually based on the theory introduced in this course. NOT what the Scikit-Learn documentation calls a parameter or attribute.\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] `n_jobs=`\n",
    "- [x] `.coef_`\n",
    "- [ ] `class_weight=`\n",
    "- [ ] `LogisticRegression()`\n",
    "  \n",
    "Yes! `.coef_` contains the important information about coefficients on our variables in the model. We do not set this, it is learned by the algorithm through the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Logistic Regression parameter\n",
    "  \n",
    "You are now going to practice extracting an important parameter of the logistic regression model. The logistic regression has a few other parameters you will not explore here but you can review them in the [scikit-learn.org](https://scikit-learn.org/) documentation for the `LogisticRegression()` module under 'Attributes'.\n",
    "  \n",
    "This parameter is important for understanding the direction and magnitude of the effect the variables have on the target.\n",
    "  \n",
    "In this exercise we will extract the coefficient parameter (found in the coef_ attribute), zip it up with the original column names, and see which variables had the largest positive effect on the target variable.\n",
    "  \n",
    "You will have available:\n",
    "  \n",
    "- A logistic regression model object named `log_reg_clf`\n",
    "- The `X_train` DataFrame\n",
    "- `sklearn` and `pandas` have been imported for you.\n",
    "  \n",
    "1. Create a list of the original column names used in the training DataFrame.\n",
    "2. Extract the coefficients of the logistic regression estimator.\n",
    "3. Create a DataFrame of coefficients and variable names & view it.\n",
    "4. Print out the top 3 'positive' variables based on the coefficient size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913</td>\n",
       "      <td>3102</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682</td>\n",
       "      <td>1725</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239</td>\n",
       "      <td>14027</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990</td>\n",
       "      <td>48233</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617</td>\n",
       "      <td>5670</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  PAY_6  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "0     -2     -2       3913       3102        689          0          0   \n",
       "1      0      2       2682       1725       2682       3272       3455   \n",
       "2      0      0      29239      14027      13559      14331      14948   \n",
       "3      0      0      46990      48233      49291      28314      28959   \n",
       "4      0      0       8617       5670      35835      20940      19146   \n",
       "\n",
       "   BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0          0         0       689         0         0         0         0   \n",
       "1       3261         0      1000      1000      1000         0      2000   \n",
       "2      15549      1518      1500      1000      1000      1000      5000   \n",
       "3      29547      2000      2019      1200      1100      1069      1000   \n",
       "4      19131      2000     36681     10000      9000       689       679   \n",
       "\n",
       "   default payment next month  \n",
       "0                           1  \n",
       "1                           1  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card = pd.read_csv('../_datasets/credit-card-full.csv')\n",
    "print(credit_card.shape)\n",
    "credit_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
       "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default payment next month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "      <th>SEX_2</th>\n",
       "      <th>EDUCATION_1</th>\n",
       "      <th>EDUCATION_2</th>\n",
       "      <th>EDUCATION_3</th>\n",
       "      <th>EDUCATION_4</th>\n",
       "      <th>EDUCATION_5</th>\n",
       "      <th>EDUCATION_6</th>\n",
       "      <th>MARRIAGE_1</th>\n",
       "      <th>MARRIAGE_2</th>\n",
       "      <th>MARRIAGE_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913</td>\n",
       "      <td>3102</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682</td>\n",
       "      <td>1725</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239</td>\n",
       "      <td>14027</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990</td>\n",
       "      <td>48233</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617</td>\n",
       "      <td>5670</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6  BILL_AMT1  \\\n",
       "0   1      20000   24      2      2     -1     -1     -2     -2       3913   \n",
       "1   2     120000   26     -1      2      0      0      0      2       2682   \n",
       "2   3      90000   34      0      0      0      0      0      0      29239   \n",
       "3   4      50000   37      0      0      0      0      0      0      46990   \n",
       "4   5      50000   57     -1      0     -1      0      0      0       8617   \n",
       "\n",
       "   BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0       3102        689          0          0          0         0       689   \n",
       "1       1725       2682       3272       3455       3261         0      1000   \n",
       "2      14027      13559      14331      14948      15549      1518      1500   \n",
       "3      48233      49291      28314      28959      29547      2000      2019   \n",
       "4       5670      35835      20940      19146      19131      2000     36681   \n",
       "\n",
       "   PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  SEX_2  \\\n",
       "0         0         0         0         0                           1      1   \n",
       "1      1000      1000         0      2000                           1      1   \n",
       "2      1000      1000      1000      5000                           0      1   \n",
       "3      1200      1100      1069      1000                           0      1   \n",
       "4     10000      9000       689       679                           0      0   \n",
       "\n",
       "   EDUCATION_1  EDUCATION_2  EDUCATION_3  EDUCATION_4  EDUCATION_5  \\\n",
       "0            0            1            0            0            0   \n",
       "1            0            1            0            0            0   \n",
       "2            0            1            0            0            0   \n",
       "3            0            1            0            0            0   \n",
       "4            0            1            0            0            0   \n",
       "\n",
       "   EDUCATION_6  MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  \n",
       "0            0           1           0           0  \n",
       "1            0           0           1           0  \n",
       "2            0           0           1           0  \n",
       "3            0           1           0           0  \n",
       "4            0           1           0           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the catcols to OHE\n",
    "credit_card = pd.get_dummies(\n",
    "    credit_card,\n",
    "    columns=['SEX', 'EDUCATION', 'MARRIAGE'],\n",
    "    drop_first=True,\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "print(credit_card.shape)\n",
    "credit_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# X/y split\n",
    "X = credit_card.drop(['ID', 'default payment next month'], axis=1)\n",
    "y = credit_card['default payment next month']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model instantiation\n",
    "log_reg_clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Model training\n",
    "log_reg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Variable   Coefficient\n",
      "0     LIMIT_BAL -3.193314e-06\n",
      "1           AGE -1.655583e-02\n",
      "2         PAY_0  1.195122e-03\n",
      "3         PAY_2  9.122857e-04\n",
      "4         PAY_3  8.301253e-04\n",
      "5         PAY_4  7.886379e-04\n",
      "6         PAY_5  7.400653e-04\n",
      "7         PAY_6  6.945895e-04\n",
      "8     BILL_AMT1 -6.891468e-06\n",
      "9     BILL_AMT2  5.203004e-06\n",
      "10    BILL_AMT3  5.143446e-07\n",
      "11    BILL_AMT4 -1.377247e-06\n",
      "12    BILL_AMT5  4.305172e-06\n",
      "13    BILL_AMT6  2.720268e-06\n",
      "14     PAY_AMT1 -2.957224e-05\n",
      "15     PAY_AMT2 -1.362358e-05\n",
      "16     PAY_AMT3 -6.141152e-06\n",
      "17     PAY_AMT4 -1.053157e-05\n",
      "18     PAY_AMT5 -5.711059e-06\n",
      "19     PAY_AMT6 -5.765152e-06\n",
      "20        SEX_2 -3.973673e-04\n",
      "21  EDUCATION_1 -1.075416e-04\n",
      "22  EDUCATION_2 -2.916334e-04\n",
      "23  EDUCATION_3 -1.039199e-04\n",
      "24  EDUCATION_4 -7.444240e-06\n",
      "25  EDUCATION_5 -2.125747e-05\n",
      "26  EDUCATION_6 -2.935579e-06\n",
      "27   MARRIAGE_1 -1.091452e-04\n",
      "28   MARRIAGE_2 -4.140390e-04\n",
      "29   MARRIAGE_3 -9.106385e-06\n",
      "  Variable  Coefficient\n",
      "2    PAY_0     0.001195\n",
      "3    PAY_2     0.000912\n",
      "4    PAY_3     0.000830\n"
     ]
    }
   ],
   "source": [
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = X_train.columns\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame(\n",
    "    {\n",
    "    \"Variable\" : original_variables, \n",
    "    \"Coefficient\": model_coefficients\n",
    "    }\n",
    ")\n",
    "\n",
    "print(coefficient_df)\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by='Coefficient', axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You have succesfully extracted and reviewed a very important parameter for the Logistic Regression Model. The coefficients of the model allow you to see which variables are having a larger or smaller impact on the outcome. Additionally the sign lets you know if it is a positive or negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a Random Forest parameter\n",
    "  \n",
    "You will now translate the work previously undertaken on the logistic regression model to a random forest model. A parameter of this model is, for a given tree, how it decided to split at each level.\n",
    "  \n",
    "This analysis is not as useful as the coefficients of logistic regression as you will be unlikely to ever explore every split and every tree in a random forest model. However, it is a very useful exercise to peek under the hood at what the model is doing.\n",
    "  \n",
    "In this exercise we will extract a single tree from our random forest model, visualize it and programmatically extract one of the splits.\n",
    "  \n",
    "You have available:\n",
    "  \n",
    "- A random forest model object, `rf_clf`\n",
    "- An image of the top of the chosen decision tree, `tree_viz_image`\n",
    "- The `X_train` DataFrame & the `original_variables` list\n",
    "  \n",
    "1. Extract the 7th tree (6th index) from the random forest model.\n",
    "2. Visualize this tree (`tree_viz_image`) to see the split decisions.\n",
    "3. Extract the feature & level of the top split.\n",
    "4. Print out the feature and level together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This node split on feature PAY_2, at a value of 1.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(max_depth=4, criterion='gini', n_estimators=10)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "# imgplot = plt.imshow(chosen_tree)\n",
    "# plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:#7393B3'>NOTE:</span>  Script to make the visual below, current computer (MacBook Pro 13-inch, 2016, Four Thunderbolt 3 Ports, 2.9 GHz Dual-Core Intel Core i5, 8 GB 2133 MHz LPDDR3, Intel Iris Graphics 550 1536 MB) outdated with a list of issues: \n",
    "  \n",
    "---\n",
    "  \n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import os\n",
    "import pydot\n",
    "\n",
    "rf_clf = RandomForestClassifier(max_depth=4, criterion='gini', n_estimators=10)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Convert tree to dot object\n",
    "export_graphviz(chosen_tree,\n",
    "                out_file='tree6.dot',\n",
    "                feature_names=X_train.columns,\n",
    "                filled=True,\n",
    "                rounded=True)\n",
    "(graph, ) = pydot.graph_from_dot_file('tree6.dot')\n",
    "\n",
    "# Convert dot to png\n",
    "graph.write_png('tree_viz_image.png')\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "tree_viz_image = plt.imread('tree_viz_image.png')\n",
    "plt.figure(figsize = (16,10))\n",
    "plt.imshow(tree_viz_image, aspect='auto')\n",
    "plt.axis('off')\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print('This node split on feature {}, at a value of {}'.format(split_column_name, split_value))\n",
    "```\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! You visualized and extracted some of the parameters of a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Overview\n",
    "  \n",
    "In the previous lesson, you learned what parameters are. You will now learn what exactly hyperparameters are, how to find and set them, as well as some tips and tricks for prioritizing your efforts. Let's get started.\n",
    "  \n",
    "**What is a hyperparameter**\n",
    "  \n",
    "Hyperparameters are something that you set before the modeling process begins. You can think of them like the knobs and dials on an old radio. You tune the different dials and buttons and hope that a nice tune comes out. The algorithm does not learn the value of these during the modeling process. This is the crucial differentiator between hyperparameters and parameters. Whether you set it or the algorithm learns it and informs you.\n",
    "  \n",
    "**Hyperparameters in Random Forest**\n",
    "  \n",
    "We can easily see the hyperparameters by creating an instance of the estimator and printing it out. Here we create the estimator with default settings and call the `print()` function on our estimator. Those are all our different knobs and dials we can set for our model. There are a lot! But what do they all mean? For this we need to turn to the Scikit-Learn documentation.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-overview.png' alt='img' width='740'></center>\n",
    "  \n",
    "**A single hyperparameter**\n",
    "  \n",
    "Let us take the example of the '`n_estimators=`' hyperparameter. We can see in the documentation that it tells us the data type and the default value And it also provides a definition of what it means.\n",
    "  \n",
    "**Setting hyperparameters**\n",
    "  \n",
    "We can set the hyperparameters when we create the estimator object. The default number of trees seems a little low, so let us set that to be 100. Whilst we are at it, let us also set the `criterion=` to be `'entropy'`. If we print out the model We can see the other default values remain the same, but those we set explicitly overrode the default values.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-overview1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Hyperparameters in Logistic Regression**\n",
    "  \n",
    "What about our logistic regression model, what were the hyperparameters for that? We follow the same steps. Firstly we create a logistic regression estimator. Then we print it out We can see there are less hyperparameters for this model than for the Random Forest.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-overview2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Hyperparameter Importance**\n",
    "  \n",
    "Some are more important than others. But before we outline important ones, there are some hyperparameters that definitely will *not* help model performance. These are related to computational decisions or what information to retain for analysis. With the random forest classifier, these hyperparameters will not assist model performance. How many cores to use will only speed up modeling time (`n_jobs=`), A random seed (`random_state=`), and whether to print out information as the modeling occurs (`verbose=`) also won't assist. Hence some hyperparameters you don't need to 'train' during your work.\n",
    "  \n",
    "**Random Forest: Important Hyperparameters**\n",
    "  \n",
    "There are some generally accepted important hyperparameters to tune for a Random Forest model. The `n_estimators=` (how many trees in the forest) should be set to a high value, 500 or 1000 or even more is not uncommon (noting that there are computational costs to higher values). The `max_features=` controls how many features to consider when splitting, which is vital to ensure tree diversity. The next two (`max_depth=` and `min_sample_leaf=`) control overfitting of individual trees. The '`criterion=`' hyperparameter may have a small impact but it is not generally a primary hyperparameter to consider. Remember, this is just a guide and your particular problem may require attention on other hyperparameters.\n",
    "  \n",
    "**How to find hyperparameters that matter?**\n",
    "  \n",
    "There are hundreds of machine learning algorithms out there and learning which hyperparameters matter is knowledge you will build over time from a variety of sources. For example, there are some great academic papers where people have tried many combinations of hyperparameters for a specific algorithm on many datasets. These can be a very informative read! You can also find great blogs and tutorials online and consult the Scikit-Learn documentation. Of course, one of the best ways to learn is just more practical experience! It is important you research this yourself to build your knowledge base for efficient modeling.\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's explore some hyperparameters in the exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters in Random Forests\n",
    "  \n",
    "As you saw, there are many different hyperparameters available in a Random Forest model using Scikit-Learn. Here you can remind yourself how to differentiate between a hyperparameter and a parameter, and easily check whether something is a hyperparameter.\n",
    "  \n",
    "Which of the following is a hyperparameter for the Scikit-Learn random forest model?\n",
    "  \n",
    "Possible answers\n",
    "  \n",
    "- [x] `oob_score`\n",
    "- [ ] `classes_`\n",
    "- [ ] `trees`\n",
    "- [ ] `random_level`\n",
    "  \n",
    "That's correct! `oob_score=` set to `True` or `False` decides whether to use out-of-bag samples to estimate the generalization accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Random Forest Hyperparameters\n",
    "  \n",
    "Understanding what hyperparameters are available and the impact of different hyperparameters is a core skill for any data scientist. As models become more complex, there are many different settings you can set, but only some will have a large impact on your model.\n",
    "  \n",
    "You will now assess an existing random forest model (it has some bad choices for hyperparameters!) and then make better choices for a new random forest model and assess its performance.\n",
    "  \n",
    "You will have available:\n",
    "  \n",
    "`X_train`, `X_test`, `y_train`, `y_test` DataFrames\n",
    "An existing pre-trained random forest estimator, `rf_clf_old`\n",
    "The predictions of the existing random forest estimator on the test set, `rf_old_predictions`\n",
    "  \n",
    "1. Print out the hyperparameters of the existing random forest classifier by printing the estimator and then create a confusion matrix and accuracy score from it. The test set `y_test` and the old predictions `rf_old_predictions` will be quite useful!\n",
    "2. Create a new random forest classifier with a better `n_estimators=` (try 500) then fit this to the data and obtain predictions.\n",
    "3. Assess the performance of the new random forest classifier. Create the confusion matrix and accuracy score and print them out. How does this compare to the first model you were given?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=5, random_state=42)\n",
      "Confusion Matrix: \n",
      "\n",
      " [[6351  639]\n",
      " [1249  761]] \n",
      " Accuracy Score: \n",
      "\n",
      " 0.7902222222222223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "rf_clf_old = RandomForestClassifier(\n",
    "    min_samples_leaf=1, \n",
    "    min_samples_split=2, \n",
    "    n_estimators=5, \n",
    "    oob_score=False, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_clf_old.fit(X_train, y_train)\n",
    "rf_old_predictions = rf_clf_old.predict(X_test)\n",
    "\n",
    "# Print out the old estimator, notice which hyperparameter is badly set\n",
    "print(rf_clf_old)\n",
    "\n",
    "# Get confusion matrix & accuracy for the old rf_model\n",
    "print('Confusion Matrix: \\n\\n {} \\n Accuracy Score: \\n\\n {}'.format(\n",
    "    confusion_matrix(y_test, rf_old_predictions),\n",
    "    accuracy_score(y_test, rf_old_predictions)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "\n",
      " [[6620  370]\n",
      " [1282  728]]\n",
      "Accuracy Score: \n",
      "\n",
      " 0.8164444444444444\n"
     ]
    }
   ],
   "source": [
    "# Create a new random forest classifier with better hyperparameters\n",
    "rf_clf_new = RandomForestClassifier(n_estimators=500, n_jobs=-1)  # Added multiprocessing per computation\n",
    "\n",
    "# Fit this to the data and obtain predictions\n",
    "rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Assess the new model (using new predictions!)\n",
    "print('Confusion Matrix: \\n\\n', confusion_matrix(y_test, rf_new_predictions))\n",
    "print('Accuracy Score: \\n\\n', accuracy_score(y_test, rf_new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We got a nice ~5% accuracy boost just from changing the `n_estimators=`. You have had your first taste of hyperparameter tuning for a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of KNN\n",
    "  \n",
    "To apply the concepts learned in the prior exercise, it is good practice to try out learnings on a new algorithm. The k-nearest-neighbors algorithm is not as popular as it used to be but can still be an excellent choice for data that has groups of data that behave similarly. Could this be the case for our credit card users?\n",
    "  \n",
    "In this case you will try out several different values for one of the core hyperparameters for the knn algorithm and compare performance.\n",
    "  \n",
    "You will have available:\n",
    "  \n",
    "`X_train`, `X_test`, `y_train`, `y_test` DataFrames\n",
    "  \n",
    "1. Build a knn estimator for the following values of `n_neighbors=` [5,10,20].\n",
    "2. Fit each to the training data and produce predictions.\n",
    "3. Get an accuracy score for each model and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of 5, 10, 20 neighbors was 0.7495555555555555, 0.7711111111111111, 0.7738888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "X_train_array = X_train.to_numpy()\n",
    "X_test_array = X_test.to_numpy()\n",
    "\n",
    "# Build a knn estimator for each value of n_neighbours\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10, n_jobs=-1)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20, n_jobs=-1)\n",
    "\n",
    "# Fit each to the training data & produce predictions\n",
    "knn_5_predictions = knn_5.fit(X_train_array, y_train).predict(X_test_array)\n",
    "knn_10_predictions = knn_10.fit(X_train_array, y_train).predict(X_test_array)\n",
    "knn_20_predictions = knn_20.fit(X_train_array, y_train).predict(X_test_array)\n",
    "\n",
    "# Get an accuracy score for each of the models\n",
    "knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)\n",
    "knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)\n",
    "knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)\n",
    "print(\"The accuracy of 5, 10, 20 neighbors was {}, {}, {}\".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! You successfully tested 3 different options for 1 hyperparameter, but it was pretty exhausting. Next, we will try to find a way to make this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Values\n",
    "  \n",
    "In this lesson we will look more in depth at what values to set for different hyperparameters and begin automating our work.\n",
    "  \n",
    "Previously you learned that some hyperparameters are likely better to start your tuning with than others. What we didn't discuss was what values should you try. This will be specific to the algorithm and to the hyperparameter itself But there does exist best practice around this. Let's walk through some top tips for deciding ranges of values to try for different hyperparameters.\n",
    "  \n",
    "**Conflicting Hyperparameter Choices**\n",
    "  \n",
    "It is firstly important to know what values NOT to set as they may conflict. You will see in the Scikit-Learn documentation for the Logistic Regression Algorithm, that some values of the hyperparameter '`penalty=`' conflict with some values of the hyperparameter '`solver=`' Another example from the `ElasticNet` algorithm demonstrates a softer conflict that will not result in an error, but may result in a model construction we had not anticipated. Safe to say, close inspection of the Scikit-Learn documentation is important.\n",
    "  \n",
    "**Silly Hyperparameter Values**\n",
    "  \n",
    "There are also values for different hyperparameters that may be valid but are very unlikely to yield good results. Some examples of this are: Having a random forest algorithm with a very low number of trees. Would you consider it a forest if it had 2 trees? How about 5 or 10? Still probably not. But at 300, 500, 1000 or more that is definitely getting there! Having only 1 neighbor in a K-nearest neighbor algorithm. This algorithm averages votes of 'neighbors' to your sample. Safe to say averaging the vote of 1 person doesn't sound robust! Finally, incrementing some hyperparameters by a small amount is unlikely to greatly improve the model. One more tree in a forest for example, isn't likely to have a large impact. Researching and documenting sensible values for different hyperparameters and algorithms will be a very useful activity.\n",
    "  \n",
    "**Automating Hyperparameter Choice**\n",
    "  \n",
    "In the previous exercise you built several different models to test a single hyperparameter like so. This was quite an inefficient way of writing code, I think we can do better to test different values for the number of neighbors hyperparameter.\n",
    "  \n",
    "**Automating Hyperparameter Tuning**\n",
    "  \n",
    "One thing we could try is using a for loop. We create a list of values to test. Then loop through the list, creating an estimator, fitting and predicting each time. We append the accuracy to a list of accuracy scores to analyze after. This method easily allows us to test more values than our previous work.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-selection-overview.png' alt='img' width='740'></center>\n",
    "  \n",
    "We can store the results in a DataFrame to view the effect of this hyperparameter on the accuracy of the model. It appears that adding any more neighbors doesn't help beyond 20.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-selection-overview1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Learning Curves**\n",
    "  \n",
    "A common tool that is used to assist with analyzing the impact of a singular hyperparameter on an end result is called a 'learning curve'. Firstly let's create a list of many more values to test using Python's `range()` function. The rest of the code is the same as before.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-selection-overview2.png' alt='img' width='740'></center>\n",
    "  \n",
    "Since we tested so many values, we will use a graph rather than a table to analyze the results. We plot the accuracy score on the Y axis and our hyperparameter value on our X axis.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-selection-overview3.png' alt='img' width='740'></center>\n",
    "  \n",
    "We can see our suspicions confirmed, that accuracy does not increase at all beyond where we tested before.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-selection-overview4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**A handy trick for generating values**\n",
    "  \n",
    "One thing to be aware of is that python's `range()` function does not work for decimal steps which is important for hyperparameters that work on that scale. A handy trick uses NumPy's `linspace()` function that will create a number of values, evenly spread between a start and end value that you specify. Here is a quick example you can see. 5 values, evenly spaced between 1 and 2 inclusive.\n",
    "  \n",
    "<center><img src='../_images/hyperparameter-selection-overview5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's practice trying different hyperparameters and plotting some learning curves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating Hyperparameter Choice\n",
    "  \n",
    "Finding the best hyperparameter of interest without writing hundreds of lines of code for hundreds of models is an important efficiency gain that will greatly assist your future machine learning model building.\n",
    "  \n",
    "An important hyperparameter for the GBM algorithm is the learning rate. But which learning rate is best for this problem? By writing a loop to search through a number of possibilities, collating these and viewing them you can find the best one.\n",
    "  \n",
    "Possible learning rates to try include 0.001, 0.01, 0.05, 0.1, 0.2 and 0.5\n",
    "  \n",
    "You will have available `X_train`, `X_test`, `y_train` & `y_test` datasets, and `GradientBoostingClassifier` has been imported for you.\n",
    "  \n",
    "1. Create a `learning_rates` list for the learning rates, and a `results_list` to hold the accuracy score of your predictions.\n",
    "2. Write a loop to create a GBM model for each learning rate mentioned and create predictions for each model.\n",
    "3. Save the learning rate and accuracy score to a `results_list`.\n",
    "4. Turn the results list into a DataFrame and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate  accuracy\n",
      "0          0.001  0.776667\n",
      "1          0.010  0.819111\n",
      "2          0.050  0.820889\n",
      "3          0.100  0.820333\n",
      "4          0.200  0.820667\n",
      "5          0.500  0.813111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Set the learning rates & results storage\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results_list = []\n",
    "\n",
    "# Create the for loop to evaluate model predictions for each learning rate\n",
    "for learning_rate in learning_rates:\n",
    "    model = GradientBoostingClassifier(learning_rate=learning_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    # Save the learning rate and accuracy score\n",
    "    results_list.append([learning_rate, accuracy_score(y_test, predictions)])\n",
    "\n",
    "# Gather everything into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You efficiently tested a few different values for a single hyperparameter and can easily see which learning rate value was the best. Here, it seems that a learning rate of 0.05 yields the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Learning Curves\n",
    "  \n",
    "If we want to test many different values for a single hyperparameter it can be difficult to easily view that in the form of a DataFrame. Previously you learned about a nice trick to analyze this. A graph called a 'learning curve' can nicely demonstrate the effect of increasing or decreasing a particular hyperparameter on the final result.\n",
    "  \n",
    "Instead of testing only a few values for the learning rate, you will test many to easily see the effect of this hyperparameter across a large range of values. A useful function from NumPy is `numpy.linspace(start, end, num)` which allows you to create a number of values (`num`) evenly spread within an interval (start, end) that you specify.\n",
    "  \n",
    "You will have available `X_train`, `X_test`, `y_train` & `y_test` datasets.\n",
    "  \n",
    "1. Create a list of 30 learning rates evenly spread between 0.01 and 2.\n",
    "2. Create a similar loop to last exercise but just save out accuracy scores to a list.\n",
    "3. Plot the learning rates against the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1           1.0506           32.11s\n",
      "         2           1.0468           58.30s\n",
      "         3           1.0431           51.14s\n",
      "         4           1.0395           43.13s\n",
      "         5           1.0360           38.94s\n",
      "         6           1.0327           35.45s\n",
      "         7           1.0294           33.13s\n",
      "         8           1.0262           31.05s\n",
      "         9           1.0231           30.01s\n",
      "        10           1.0201           29.49s\n",
      "        20           0.9939           30.94s\n",
      "        30           0.9733           23.84s\n",
      "        40           0.9564           19.29s\n",
      "        50           0.9426           15.57s\n",
      "        60           0.9311           12.08s\n",
      "        70           0.9214            8.87s\n",
      "        80           0.9129            5.81s\n",
      "        90           0.9057            2.89s\n",
      "       100           0.8996            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9747           34.98s\n",
      "         2           0.9370           30.05s\n",
      "         3           0.9129           30.66s\n",
      "         4           0.8965           30.13s\n",
      "         5           0.8861           30.73s\n",
      "         6           0.8784           30.12s\n",
      "         7           0.8727           29.66s\n",
      "         8           0.8690           30.05s\n",
      "         9           0.8655           29.32s\n",
      "        10           0.8626           28.79s\n",
      "        20           0.8445           22.99s\n",
      "        30           0.8347           19.19s\n",
      "        40           0.8272           15.91s\n",
      "        50           0.8202           13.06s\n",
      "        60           0.8141           10.68s\n",
      "        70           0.8087            7.89s\n",
      "        80           0.8023            5.20s\n",
      "        90           0.7971            2.57s\n",
      "       100           0.7922            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9224           20.93s\n",
      "         2           0.8903           21.74s\n",
      "         3           0.8761           22.61s\n",
      "         4           0.8675           26.37s\n",
      "         5           0.8627           25.01s\n",
      "         6           0.8593           24.34s\n",
      "         7           0.8553           23.44s\n",
      "         8           0.8512           23.64s\n",
      "         9           0.8480           23.29s\n",
      "        10           0.8466           22.85s\n",
      "        20           0.8307           19.43s\n",
      "        30           0.8182           16.74s\n",
      "        40           0.8071           15.42s\n",
      "        50           0.7970           15.13s\n",
      "        60           0.7878           12.67s\n",
      "        70           0.7797            9.78s\n",
      "        80           0.7697            6.64s\n",
      "        90           0.7597            3.25s\n",
      "       100           0.7513            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8953           27.13s\n",
      "         2           0.8742           32.69s\n",
      "         3           0.8634           29.39s\n",
      "         4           0.8584           27.72s\n",
      "         5           0.8539           26.30s\n",
      "         6           0.8501           27.43s\n",
      "         7           0.8480           26.25s\n",
      "         8           0.8450           25.37s\n",
      "         9           0.8424           24.78s\n",
      "        10           0.8387           25.72s\n",
      "        20           0.8236           21.17s\n",
      "        30           0.8102           19.27s\n",
      "        40           0.7954           15.61s\n",
      "        50           0.7805           12.42s\n",
      "        60           0.7699            9.63s\n",
      "        70           0.7575            7.77s\n",
      "        80           0.7476            5.22s\n",
      "        90           0.7367            2.59s\n",
      "       100           0.7251            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8913           46.28s\n",
      "         2           0.8722           52.13s\n",
      "         3           0.8623           46.68s\n",
      "         4           0.8576           40.99s\n",
      "         5           0.8535           37.41s\n",
      "         6           0.8498           36.60s\n",
      "         7           0.8466           33.64s\n",
      "         8           0.8452           31.57s\n",
      "         9           0.8425           30.00s\n",
      "        10           0.8396           28.48s\n",
      "        20           0.8198           30.78s\n",
      "        30           0.8017           25.01s\n",
      "        40           0.7859           19.78s\n",
      "        50           0.7728           16.01s\n",
      "        60           0.7573           12.26s\n",
      "        70           0.7429            8.87s\n",
      "        80           0.7307            5.75s\n",
      "        90           0.7173            2.82s\n",
      "       100           0.7034            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9060           22.13s\n",
      "         2           0.8836           25.04s\n",
      "         3           0.8678           23.31s\n",
      "         4           0.8627           23.53s\n",
      "         5           0.8582           22.60s\n",
      "         6           0.8550           22.85s\n",
      "         7           0.8518           22.57s\n",
      "         8           0.8494           22.37s\n",
      "         9           0.8456           21.86s\n",
      "        10           0.8414           21.56s\n",
      "        20           0.8221           19.42s\n",
      "        30           0.8058           17.01s\n",
      "        40           0.7914           14.44s\n",
      "        50           0.7745           11.99s\n",
      "        60           0.7585            9.54s\n",
      "        70           0.7428            7.18s\n",
      "        80           0.7301            4.80s\n",
      "        90           0.7165            2.41s\n",
      "       100           0.7032            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9345           27.98s\n",
      "         2           1.0320           27.68s\n",
      "         3           1.0856           30.21s\n",
      "         4          24.2111           29.72s\n",
      "         5 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           28.56s\n",
      "         6 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           26.99s\n",
      "         7 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           26.16s\n",
      "         8 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           25.30s\n",
      "         9 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           25.21s\n",
      "        10 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           24.48s\n",
      "        20 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           21.80s\n",
      "        30 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           18.55s\n",
      "        40 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           16.85s\n",
      "        50 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           15.17s\n",
      "        60 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000           12.23s\n",
      "        70 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000            9.39s\n",
      "        80 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000            6.16s\n",
      "        90 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000            3.07s\n",
      "       100 130176908572929626173493085737396324371288476272244693282104239497361687710801230281528151517231606700720920668653306017245803841286207742029070336.0000            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9727           24.99s\n",
      "         2           1.0797           23.27s\n",
      "         3           1.0674           23.54s\n",
      "         4           1.0766           23.12s\n",
      "         5           2.6454           23.99s\n",
      "         6 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           23.02s\n",
      "         7 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           22.56s\n",
      "         8 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           23.17s\n",
      "         9 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           22.72s\n",
      "        10 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           22.32s\n",
      "        20 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           19.38s\n",
      "        30 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           17.53s\n",
      "        40 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           14.65s\n",
      "        50 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000           12.08s\n",
      "        60 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000            9.62s\n",
      "        70 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000            7.17s\n",
      "        80 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000            4.75s\n",
      "        90 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000            2.37s\n",
      "       100 21831625616952207061630837011225434326376542390215096368046056238692048200972112920300984228311585945799278342300981660611865680543744.0000            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0180           43.12s\n",
      "         2           1.3629           33.47s\n",
      "         3           2.8905           28.93s\n",
      "         4           5.7869           28.76s\n",
      "         5 310467415517441536.0000           27.98s\n",
      "         6 310467415517441984.0000           26.67s\n",
      "         7 310467415517415616.0000           25.42s\n",
      "         8 310467415517415680.0000           25.08s\n",
      "         9 310467415517427072.0000           24.98s\n",
      "        10 310467415526180992.0000           24.30s\n",
      "        20 310467415818733952.0000           22.00s\n",
      "        30 310467415818733952.0000           18.56s\n",
      "        40 310467415818733952.0000           16.81s\n",
      "        50 310467415818733952.0000           13.72s\n",
      "        60 310467415818733952.0000           10.84s\n",
      "        70 310467415818733952.0000            8.09s\n",
      "        80 310467415818733952.0000            5.48s\n",
      "        90 310467415818733952.0000            2.73s\n",
      "       100 310467415818733952.0000            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0684           33.52s\n",
      "         2           1.6446           31.89s\n",
      "         3        1357.1936           34.41s\n",
      "         4       13011.9721           31.50s\n",
      "         5 9293820601426958336.0000           30.06s\n",
      "         6 9293820601426958336.0000           36.14s\n",
      "         7 22392284943338899844622188544.0000           37.32s\n",
      "         8 22392284943338899844622188544.0000           36.11s\n",
      "         9 22392284943338899844622188544.0000           34.24s\n",
      "        10 22392284943338899844622188544.0000           32.75s\n",
      "        20 22392284943338899844622188544.0000           23.92s\n",
      "        30 22392284943338899844622188544.0000           19.45s\n",
      "        40 22392284943338899844622188544.0000           16.21s\n",
      "        50 22392284943338899844622188544.0000           13.36s\n",
      "        60 22392284943338899844622188544.0000           10.49s\n",
      "        70 22392284943338899844622188544.0000            7.76s\n",
      "        80 22392284943338899844622188544.0000            5.14s\n",
      "        90 22392284943338899844622188544.0000            2.61s\n",
      "       100 22392284943338899844622188544.0000            0.00s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAK9CAYAAADG5r/mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACTWUlEQVR4nOzdd3hUddrG8Xtmkkx6SAgkBEJCky4qCoKAqBQVUbBXsGEDG6674qq4usquq6yrolhQsIIFy2tBESmiKAo2ektCCQmEkl5nzvtHMgORBFImOVO+n+vKtcvJmZlnDqPe+eU5z89iGIYhAAAAwE9ZzS4AAAAAaEoEXgAAAPg1Ai8AAAD8GoEXAAAAfo3ACwAAAL9G4AUAAIBfI/ACAADArxF4AQAA4NcIvAAAAPBrBF4Afmnz5s0aMWKEYmJiZLFY9NFHHzV7DUuWLJHFYtGSJUvcx6699lqlpqZWO6+goEA33nijEhMTZbFYdNddd0mSsrOzdfHFF6tly5ayWCx6+umnm632pmaxWPTwww+bXcYxpaam6tprrzW7DACNFGR2AQBq9/zzz2vixInq16+ffvzxR7PL8Snjx49XWlqaHnvsMbVo0UInn3yy2SXV6vHHH9fs2bP14IMPqlOnTurevbsk6e6779aXX36pqVOnKjEx0avfw/PPP6/w8HDCoRd7/PHH1aNHD40ZM8bsUoBmZzEMwzC7CAA1O+2005SZman09HRt3rxZnTt3Nrskn1BcXKzw8HD9/e9/1z//+U/T6liyZInOOOMMLV68WEOHDpUklZeXy+l0ym63u8879dRTFRQUpOXLl1d7fGJiooYNG6Y333yzOctukF69eik+Pr7aavbRWCwWTZ061etXeUtLS2W1WhUcHGx2KY0WGRmpiy++WLNnzza7FKDZ0dIAeKm0tDR9//33mj59ulq1aqW33nrL7JJqVVhYaHYJ1ezdu1eS1KJFC489p6feY3BwcLWwK0l79uypsdbajjdURUWFysrKPPZ8vqYh799ut3tl2HU6nSopKTG7DMBnEHgBL/XWW28pNjZWo0aN0sUXX1xr4D148KDuvvtupaamym63q127dho3bpxycnLc55SUlOjhhx/Wcccdp9DQULVp00YXXnihtm7dKqnmXlNJSk9Pl8ViqbYidO211yoyMlJbt27Vueeeq6ioKF111VWSpG+//VaXXHKJ2rdvL7vdruTkZN19990qLi4+ou4NGzbo0ksvVatWrRQWFqauXbvq73//uyRp8eLFslgs+vDDD4943Ntvvy2LxaIVK1bUeD0efvhhpaSkSJLuvfdeWSyWaj2zv/zyi8455xxFR0crMjJSZ511ln744YdqzzF79mxZLBYtXbpUt912m1q3bq127drV+HouO3fu1JgxYxQREaHWrVvr7rvvVmlp6RHnHd7D67ruaWlp+uyzz2SxWNzX22KxyDAMzZgxw33c5eDBg7rrrruUnJwsu92uzp0769///recTqf7HNff3ZNPPqmnn35anTp1kt1u17p169zX/+KLL1ZcXJxCQ0N18skn65NPPqnxOnz33XeaPHmyWrVqpYiICI0dO9b9Q4VU2ee6du1aLV261F2ra0W7Pnbt2qXrr79eCQkJstvt6tmzp1599dVq55SVlemhhx5S3759FRMTo4iICA0ePFiLFy+udt7R3v/DDz8si8WiLVu26Nprr1WLFi0UExOj6667TkVFRdWe5889vHW9JlJlKH344YeVlJSk8PBwnXHGGVq3bl2D+oItFosmTZqkt956Sz179pTdbteCBQskSU8++aQGDhyoli1bKiwsTH379tX7779/xOMLCws1Z84c99/R4TXU5dpL0rPPPquePXsqPDxcsbGxOvnkk/X222/X670AZqCHF/BSb731li688EKFhIToiiuu0AsvvKCffvpJp5xyivucgoICDR48WOvXr9f111+vk046STk5Ofrkk0+0c+dOxcfHy+Fw6LzzztOiRYt0+eWX684771R+fr4WLlyoNWvWqFOnTvWuraKiQiNHjtSgQYP05JNPKjw8XJL03nvvqaioSLfeeqtatmyplStX6tlnn9XOnTv13nvvuR//+++/a/DgwQoODtZNN92k1NRUbd26Vf/3f/+nxx57TEOHDlVycrLeeustjR079ojr0qlTJw0YMKDG2i688EK1aNFCd999t6644gqde+65ioyMlCStXbtWgwcPVnR0tP76178qODhYL774ooYOHaqlS5eqf//+1Z7rtttuU6tWrfTQQw8ddYW3uLhYZ511lrZv36477rhDSUlJeuONN/TNN98c9Tp2795db7zxhu6++261a9dO99xzjyTpxBNP1BtvvKFrrrlGw4cP17hx49yPKSoq0umnn65du3bp5ptvVvv27fX9999rypQp2r179xE3tr322msqKSnRTTfdJLvdrri4OK1du1annXaa2rZtq/vuu08RERF69913NWbMGH3wwQdHXPPbb79dsbGxmjp1qtLT0/X0009r0qRJmjdvniTp6aef1u23367IyEj3Dy0JCQlHfe9/lp2drVNPPdUd7Fq1aqUvvvhCN9xwg/Ly8tw38uXl5emVV17RFVdcoQkTJig/P1+zZs3SyJEjtXLlSp1wwgnHfP8ul156qTp06KBp06Zp9erVeuWVV9S6dWv9+9//Pma9x7omkjRlyhQ98cQTGj16tEaOHKnffvtNI0eObPDK7DfffKN3331XkyZNUnx8vPsHp//97386//zzddVVV6msrExz587VJZdcok8//VSjRo2SJL3xxhu68cYb1a9fP910002S5P5nv67X/uWXX9Ydd9yhiy++WHfeeadKSkr0+++/68cff9SVV17ZoPcENBsDgNf5+eefDUnGwoULDcMwDKfTabRr18648847q5330EMPGZKM+fPnH/EcTqfTMAzDePXVVw1JxvTp02s9Z/HixYYkY/HixdW+n5aWZkgyXnvtNfex8ePHG5KM++6774jnKyoqOuLYtGnTDIvFYmRkZLiPDRkyxIiKiqp27PB6DMMwpkyZYtjtduPgwYPuY3v27DGCgoKMqVOnHvE6NdX9n//8p9rxMWPGGCEhIcbWrVvdxzIzM42oqChjyJAh7mOvvfaaIckYNGiQUVFRcdTXMgzDePrppw1Jxrvvvus+VlhYaHTu3PmI6zp+/HgjJSWl2uNTUlKMUaNGHfG8koyJEydWO/boo48aERERxqZNm6odv++++wybzWZs37692jWIjo429uzZU+3cs846y+jdu7dRUlLiPuZ0Oo2BAwcaXbp0OeI6DBs2rNrfzd13323YbLZqfzc9e/Y0Tj/99Fqu0JEkVft7vOGGG4w2bdoYOTk51c67/PLLjZiYGPdnq6KiwigtLa12zoEDB4yEhATj+uuvdx872vufOnWqIana+YZhGGPHjjVatmxZ7VhKSooxfvx495/rek2ysrKMoKAgY8yYMdWe7+GHHzYkVXvOupBkWK1WY+3atUd878//3JWVlRm9evUyzjzzzGrHIyIianzdul77Cy64wOjZs2e96ga8BS0NgBd66623lJCQoDPOOENS5a8jL7vsMs2dO1cOh8N93gcffKA+ffocsSLneozrnPj4eN1+++21ntMQt9566xHHwsLC3P+/sLBQOTk5GjhwoAzD0C+//CKpsr922bJluv7669W+ffta6xk3bpxKS0ur/Wp23rx5qqio0NVXX13veh0Oh7766iuNGTNGHTt2dB9v06aNrrzySi1fvlx5eXnVHjNhwgTZbLZjPvfnn3+uNm3a6OKLL3YfCw8Pd6+kedJ7772nwYMHKzY2Vjk5Oe6vYcOGyeFwaNmyZdXOv+iii9SqVSv3n/fv369vvvlGl156qfLz892P37dvn0aOHKnNmzdr165d1Z7jpptuqvZ3M3jwYDkcDmVkZHjkPRmGoQ8++ECjR4+WYRjV3tfIkSOVm5ur1atXS5JsNptCQkIkVbYM7N+/XxUVFTr55JPd5xzt/R/ulltuqfbnwYMHa9++fUd8DmpyrGuyaNEiVVRU6Lbbbqv2uJr+Oayr008/XT169Dji+OH/3B04cEC5ubkaPHhwjdfjz+pz7Vu0aKGdO3fqp59+avB7AMxCSwPgZRwOh+bOnaszzjhDaWlp7uP9+/fXU089pUWLFmnEiBGSpK1bt+qiiy466vNt3bpVXbt2VVCQ5/5xDwoKqrGndfv27XrooYf0ySef6MCBA9W+l5ubK0natm2bpMq7+o+mW7duOuWUU/TWW2/phhtukFT5g8Cpp57aoGkVe/fuVVFRkbp27XrE97p37y6n06kdO3aoZ8+e7uMdOnSo03NnZGSoc+fOR/wAUdNrNdbmzZv1+++/1xri9uzZU+3Pf34PW7ZskWEYevDBB/Xggw/W+hxt27Z1//nPP5jExsZK0hF/xw21d+9eHTx4UC+99JJeeumlWmtymTNnjp566ilt2LBB5eXl7uM1/X0d7e/waO8rOjr6qDUf65q4gu+fP6txcXHuc+urtvfy6aef6p///Kd+/fXXan3jdfmBtj7X/m9/+5u+/vpr9evXT507d9aIESN05ZVX6rTTTmvAuwGaF4EX8DLffPONdu/erblz52ru3LlHfP+tt95yB15Pqe0/jIevJh/ObrfLarUece7w4cO1f/9+/e1vf1O3bt0UERGhXbt26dprr612Q1VdjRs3Tnfeead27typ0tJS/fDDD3ruuefq/TwNdfjKmbdwOp0aPny4/vrXv9b4/eOOO67an//8Hlx/D3/5y180cuTIGp/jzyGttlVuw0NTLV01XX311Ro/fnyN5xx//PGSpDfffFPXXnutxowZo3vvvVetW7eWzWbTtGnT3DdhHu5of4eNeV9NfU1qUtN7+fbbb3X++edryJAhev7559WmTRsFBwfrtddeq9PNZPW59t27d9fGjRv16aefasGCBfrggw/0/PPP66GHHtI//vGPRrwzoOkReAEv89Zbb6l169aaMWPGEd+bP3++PvzwQ82cOVNhYWHq1KmT1qxZc9Tn69Spk3788UeVl5fXOl7JteJ08ODBasfr8yvrP/74Q5s2bdKcOXOq3WS1cOHCaue52gmOVbckXX755Zo8ebLeeecdFRcXKzg4WJdddlmdazpcq1atFB4ero0bNx7xvQ0bNshqtSo5OblBz52SkqI1a9bIMIxqPzzU9FqN1alTJxUUFGjYsGENerzr+gcHBzf4OWrSmPaYVq1aKSoqSg6H45g1vf/+++rYsaPmz59f7TWnTp3a4NdvCq5JIVu2bKm2Mrtv3z6PrYxLlS1LoaGh+vLLL6uNu3vttdeOOLemv6P6XHtJioiI0GWXXabLLrtMZWVluvDCC/XYY49pypQpCg0NbdybAZoQPbyAFykuLtb8+fN13nnn6eKLLz7ia9KkScrPz3ePj7rooov022+/1Ti+y7XSdNFFFyknJ6fGlVHXOSkpKbLZbEf0fz7//PN1rt214nX4CpdhGPrf//5X7bxWrVppyJAhevXVV7V9+/Ya63GJj4/XOeecozfffFNvvfWWzj77bMXHx9e5pj/XN2LECH388cdKT093H8/Oztbbb7+tQYMGHfPX2LU599xzlZmZWa3fuKioqNZfETfGpZdeqhUrVujLL7884nsHDx5URUXFUR/funVrDR06VC+++KJ27959xPf/PFqrriIiIo74gamubDabLrroIn3wwQc1/iB0eE01fc5+/PHHWsfUmeWss85SUFCQXnjhhWrHPf0bCpvNJovFUu23Menp6TVupV3T31F9rv2+ffuqfS8kJEQ9evSQYRjVWksAb8QKL+BFPvnkE+Xn5+v888+v8funnnqqexOKyy67TPfee6/ef/99XXLJJbr++uvVt29f7d+/X5988olmzpypPn36aNy4cXr99dc1efJkrVy5UoMHD1ZhYaG+/vpr3XbbbbrgggsUExOjSy65RM8++6wsFos6deqkTz/99Ih+0KPp1q2bOnXqpL/85S/atWuXoqOj9cEHH9S4mvXMM89o0KBBOumkk3TTTTepQ4cOSk9P12effaZff/212rnjxo1z3wz26KOP1v1i1uCf//ynFi5cqEGDBum2225TUFCQXnzxRZWWluqJJ55o8PNOmDBBzz33nMaNG6dVq1apTZs2euONN9zj2jzp3nvv1SeffKLzzjtP1157rfr27avCwkL98ccfev/995Wenn7MHwpmzJihQYMGqXfv3powYYI6duyo7OxsrVixQjt37tRvv/1W77r69u2rF154Qf/85z/VuXNntW7dWmeeeWadH/+vf/1LixcvVv/+/TVhwgT16NFD+/fv1+rVq/X1119r//79kqTzzjtP8+fP19ixYzVq1CilpaVp5syZ6tGjhwoKCupdd1NJSEjQnXfeqaeeekrnn3++zj77bP3222/64osvFB8f36gV8cONGjVK06dP19lnn60rr7xSe/bs0YwZM9S5c2f9/vvv1c7t27evvv76a02fPl1JSUnq0KGD+vfvX+drP2LECCUmJuq0005TQkKC1q9fr+eee06jRo1SVFSUR94P0GSafS4EgFqNHj3aCA0NNQoLC2s959prrzWCg4PdI4T27dtnTJo0yWjbtq0REhJitGvXzhg/fny1EUNFRUXG3//+d6NDhw5GcHCwkZiYaFx88cXVxnPt3bvXuOiii4zw8HAjNjbWuPnmm401a9bUOJYsIiKixtrWrVtnDBs2zIiMjDTi4+ONCRMmGL/99tsRz2EYhrFmzRpj7NixRosWLYzQ0FCja9euxoMPPnjEc5aWlhqxsbFGTEyMUVxcXJfLWOtYMsMwjNWrVxsjR440IiMjjfDwcOOMM84wvv/++2rnuEZP/fTTT3V6PcMwjIyMDOP88883wsPDjfj4eOPOO+80FixY4PGxZIZhGPn5+caUKVOMzp07GyEhIUZ8fLwxcOBA48knnzTKysqOeQ0MwzC2bt1qjBs3zkhMTDSCg4ONtm3bGuedd57x/vvvH/M61DTGLisryxg1apQRFRVlSDrmiDL9aSyZYRhGdna2MXHiRCM5Odn9OT3rrLOMl156yX2O0+k0Hn/8cSMlJcWw2+3GiSeeaHz66adHXNejvX/XWLK9e/dWO+56v2lpae5jtY0lq8s1qaioMB588EEjMTHRCAsLM84880xj/fr1RsuWLY1bbrnlqNfnz2r7LBiGYcyaNcvo0qWLYbfbjW7duhmvvfaa+z0ebsOGDcaQIUOMsLCwI0aj1eXav/jii8aQIUOMli1bGna73ejUqZNx7733Grm5ufV6L4AZLIbRhB32ANBIFRUVSkpK0ujRozVr1iyzywEa5eDBg4qNjdU///lP9yYdAJoePbwAvNpHH32kvXv3VrsRDvAFNW2p7doJryFbLwNoOFZ4AXilH3/8Ub///rseffRRxcfH12mIPuBNZs+erdmzZ7u3t16+fLneeecdjRgxwn3TYVZW1lGfIywsTDExMc1RLuDXuGkNgFd64YUX9Oabb+qEE07Q7NmzzS4HqLfjjz9eQUFBeuKJJ5SXl+e+ke2f//yn+5w2bdoc9TnGjx/P5x/wAFZ4AQAwyddff33U7yclJdW4nTCA+iHwAgAAwK9x0xoAAAD8Gj28NXA6ncrMzFRUVJTHhoMDAADAcwzDUH5+vpKSkmS1Hn0Nl8Bbg8zMTCUnJ5tdBgAAAI5hx44dateu3VHPIfDWwLVF4o4dOxQdHW1yNQAAAPizvLw8JScn12lrawJvDVxtDNHR0QReAAAAL1aX9lNuWgMAAIBfI/ACAADArxF4AQAA4NcIvAAAAPBrBF4AAAD4NQIvAAAA/BqBFwAAAH6NwAsAAAC/RuAFAACAXyPwAgAAwK8ReAEAAODXvCLwzpgxQ6mpqQoNDVX//v21cuXKo57/9NNPq2vXrgoLC1NycrLuvvtulZSUuL8/bdo0nXLKKYqKilLr1q01ZswYbdy4sanfBgAAALyQ6YF33rx5mjx5sqZOnarVq1erT58+GjlypPbs2VPj+W+//bbuu+8+TZ06VevXr9esWbM0b9483X///e5zli5dqokTJ+qHH37QwoULVV5erhEjRqiwsLC53hYAAAC8hMUwDMPMAvr3769TTjlFzz33nCTJ6XQqOTlZt99+u+67774jzp80aZLWr1+vRYsWuY/dc889+vHHH7V8+fIaX2Pv3r1q3bq1li5dqiFDhhyzpry8PMXExCg3N1fR0dENfGcAAABoKvXJa6au8JaVlWnVqlUaNmyY+5jVatWwYcO0YsWKGh8zcOBArVq1yt32sG3bNn3++ec699xza32d3NxcSVJcXFyN3y8tLVVeXl61LwAAAPiHIDNfPCcnRw6HQwkJCdWOJyQkaMOGDTU+5sorr1ROTo4GDRokwzBUUVGhW265pVpLw+GcTqfuuusunXbaaerVq1eN50ybNk3/+Mc/GvdmAAAA4JVM7+GtryVLlujxxx/X888/r9WrV2v+/Pn67LPP9Oijj9Z4/sSJE7VmzRrNnTu31uecMmWKcnNz3V87duxoqvIBAADQzExd4Y2Pj5fNZlN2dna149nZ2UpMTKzxMQ8++KCuueYa3XjjjZKk3r17q7CwUDfddJP+/ve/y2o9lOEnTZqkTz/9VMuWLVO7du1qrcNut8tut3vgHQEAAMDbmLrCGxISor59+1a7Ac3pdGrRokUaMGBAjY8pKiqqFmolyWazSZJc998ZhqFJkybpww8/1DfffKMOHTo00TsAAACAtzN1hVeSJk+erPHjx+vkk09Wv3799PTTT6uwsFDXXXedJGncuHFq27atpk2bJkkaPXq0pk+frhNPPFH9+/fXli1b9OCDD2r06NHu4Dtx4kS9/fbb+vjjjxUVFaWsrCxJUkxMjMLCwsx5owAAADCF6YH3sssu0969e/XQQw8pKytLJ5xwghYsWOC+kW379u3VVnQfeOABWSwWPfDAA9q1a5datWql0aNH67HHHnOf88ILL0iShg4dWu21XnvtNV177bVN/p4AAADgPUyfw+uNmMMLAADg3XxmDi8AAADQ1ExvaYD02e+7tfNAkcJCbAoNtims6is02KawEOuhYyGHjtuDrLJYLGaXDgAA4PUIvF7g/VU7tHjj3no9xmJRtWAcGmytFoj/HJDDQmwKDaoM0Icfcz/Hnx8bbFNoiFUhNoI1AADwbQReL3Ba53jFRoSopNyh4jKHissdKil3Vv652jGHyh2u0WtSUZlDRWWOJq3N6grWIdWDdGiQKyRbj/x+1Z/t1f585Eq1K2hH2YMI1QAAoMkQeL3AjYM71vnccsehIFxa7qwWiIvLHSo57P8Xl1WG5BLXebV8v/iwc0rKHCoqd8jhrAzWTkMqLHOosAmDdcf4CD06ppdO6xzfZK8BAAACF1MaasCUhspg/eeAXFLurBaSD195Pnxl+s/Hjjyn8ryisgo5D/v0XdK3nf4+qrtahIeY98YBAIBPqE9eY4UXNQq2WRVssyo6NLjJXsMwDOUVV+iphRv1xg8Zem/VTi3euEcPje6p0ce3oc0BAAB4BGPJYBqLxaKY8GA9ckEvvX/LAHVpHamcgjLd8c4vun72T9p1sNjsEgEAgB8g8MIr9E2J06d3DNLdw45TiM2qxRv3avj0pXp1eZq7nxgAAKAhCLzwGvYgm+4c1kWf3zlIp6TGqqjMoUc+XacLX/heG7LyzC4PAAD4KAIvvE7n1lGad9MAPTa2l6LsQfptx0Gd98xy/efLDSopb9oxbAAAwP8QeOGVrFaLruqfooWTT9fIngmqcBqasXirzvnft1qxdZ/Z5QEAAB9C4IVXS4wJ1YvXnKyZV/dV6yi70nIKdcXLP+hv7/+u3KJys8sDAAA+gMALn3B2r0R9fc/puqp/e0nSvJ936KzpS/Xp75lilDQAADgaAi98RnRosB4b21vv3jxAnVpFKKegVJPe/kUTXv9ZmYwwAwAAtSDwwuf06xCnz+8crDvO6qJgm0Vfr9+j4dOXas736YwwAwAARyDwwifZg2yaPPw4fXbHYJ3UvoUKyxya+slaXTzze23Myje7PAAA4EUIvPBpxyVE6f1bBurRC3oq0h6kX7Yf1HnPfqvpX21khBkAAJBE4IUfsFotumZAqhZOHqLhPRJU7jD0zDdbdO4z32pl2n6zywMAACYj8MJvtIkJ00vX9NULV52kVlF2bdtbqEtfXKEp8/9QbjEjzAAACFQEXvgVi8Wic3q30dd3n64r+iVLkt5ZuV3Dpy/VgjW7Ta4OAACYgcALvxQTHqxpFx6vuTedqo7xEdqTX6pb3lytm17/WVm5JWaXBwAAmhGBF37t1I4t9fmdg3X7mZ0VZLXoq3XZGj59qd74IUNORpgBABAQCLzwe6HBNt0zoqs+vWOQTkhuofzSCj340Rpd+uIKbc5mhBkAAP6OwIuA0S0xWh/cOlAPj+6hiBCbfs44oHOf+Vb/XbhJpRWMMAMAwF8ReBFQbFaLrj2tg76afLrO7NZa5Q5D/1u0WaOeWa6f0xlhBgCAPyLwIiC1bRGmWeNP1rNXnKj4yBBt2VOgi2eu0AMf/aG8EkaYAQDgTwi8CFgWi0Wj+yTp68mn69KT20mS3vyhcoTZl2uzTK4OAAB4CoEXAa9FeIieuLiP3p7QX6ktw5WdV6qb31ilW95Ypew8RpgBAODrCLxAlYGd4rXgriG6bWgnBVktWrA2S8OmL9XbP25nhBkAAD6MwAscJjTYpr+e3U2fTBqkPu1ilF9Sofs//EOXv/SDtuwpMLs8AADQAAReoAY9kqI1/7bT9OB5PRQeYtPK9P0693/f6plFm1VW4TS7PAAAUA8EXqAWNqtFNwzqoK/uHqKhXVupzOHU9IWbdN6z32pVxgGzywMAAHVE4AWOoV1suF679hT97/IT1DIiRJuyC3TxzO819eM1KiitMLs8AABwDAReoA4sFosuOKGtvp58ui46qZ0MQ5qzIkPDpy/V1+uyzS4PAAAcBYEXqIfYiBA9dWkfvXlDf7WPC9fu3BLd+PrPmvjWau3JZ4QZAADeiMALNMCgLvH68q4huvn0jrJZLfrsj90a9tRSzV25XYbBCDMAALwJgRdooLAQm6ac010fTzxNvdpGK6+kQvfNrxxhtm0vI8wAAPAWBF6gkXq1jdFHt52mv5/bXWHBNv2Ytl9n/+9bzVi8ReUORpgBAGA2Ai/gAUE2qyYM6aiv7h6iwV3iVVbh1H++3KjRzy7XL9sZYQYAgJkIvIAHJceF6/Xr++m/l/VRbHiwNmTl68IXvtfDn6xlhBkAACYh8AIeZrFYNPbEdlp0z1BdeGJbGYY0+/t0jZi+VN9sYIQZAADNjcALNJG4iBBNv+wEvX59P7WLDVNmbomun/2zbn/nF+3NLzW7PAAAAgaBF2hiQ45rpa/uHqIJgzvIapH+77dMDZu+VO/+vIMRZgAANAMCL9AMwkOC9PdRPfTxxEHqmRSt3OJy/fX93zX+tZ+UnceGFQAANCUCL9CMereL0ccTT9OUc7rJHmTVsk17NfLpZfrs991mlwYAgN8i8ALNLMhm1c2nd9Kntw9Sr7bROlhUrolvr9bd835VbnG52eUBAOB3CLyASbokRGn+radp0hmdZbVIH/6yS+c8vUwrtu4zuzQAAPwKgRcwUUiQVX8Z2VXv3TJAKS3DlZlboitf+UGPfbZOJeUOs8sDAMAvEHgBL9A3JU6f3zFYV/RrL8OQXv42TRc8953WZeaZXRoAAD6PwAt4iQh7kKZd2Fuzxp+s+MgQbczO1wUzluuFJVvlcDK+DACAhiLwAl7mrO4J+vKuIRreI0HlDkP/XrBBl7+0Qjv2F5ldGgAAPonAC3ihlpF2vXRNXz1x0fGKCLHpp/QDOvvpZWxWAQBAAxB4AS9lsVh06SnJ+uLOITolNVaFZQ799f3fdcubq7SvgK2JAQCoKwIv4OXatwzX3JsG6G9nd1OwzaIv12Zr5NPf6psN2WaXBgCATyDwAj7AZrXo1qGd9NHE03RcQqRyCkp1/eyfNWX+HyosrTC7PAAAvBqBF/AhPZNi9MmkQbpxUAdJ0jsrt+vcZ77VqowDJlcGAID3IvACPiY02KYHzuuht2/sr6SYUGXsK9IlM7/XU19tVLnDaXZ5AAB4HQIv4KMGdo7XF3cN0dgT28ppSM9+s0UXPv+9tuwpMLs0AAC8CoEX8GExYcH672Un6LkrT1RMWLD+2JWrUc98q9nfpcnJZhUAAEgi8AJ+4bzjk/TlXUM0uEu8Siucevj/1mn8ayuVlVtidmkAAJiOwAv4icSYUL1+fT89ckFP2YOs+nZzjkY+vUz/91um2aUBAGAqAi/gRywWi8YNSNVndwzW8e1ilFtcrtvf+UV3zv1FuUXlZpcHAIApCLyAH+rcOlIf3DpQd5zVRTarRR//mqmz/7dM323JMbs0AACaHYEX8FPBNqsmDz9O790yQKktw7U7t0RXvfKjHvm/dSopd5hdHgAAzYbAC/i5k9rH6vM7B+uq/u0lSa9+l6bRzy7Xml25JlcGAEDzIPACASA8JEiPje2tV689WfGRdm3eU6Cxz3+nGYu3yMH4MgCAnyPwAgHkzG4J+vKuwRrZM0HlDkP/+XKjLntxhbbvKzK7NAAAmgyBFwgwLSPtmnl1Xz15SR9F2oP0c8YBnfO/ZZr303YZBqu9AAD/Q+AFApDFYtHFfdvpizsHq19qnArLHPrbB39owuurlFNQanZ5AAB4FIEXCGDJceF656ZTNeWcbgq2WfT1+myN/O8yLVyXbXZpAAB4DIEXCHA2q0U3n95JH08cpK4JUdpXWKYJr/+s+z74XQWlFWaXBwBAoxF4AUiSeiRF6+NJp+mmIR1lsUhzf9qhc//3rVZl7De7NAAAGoXAC8AtNNim+8/trrdvPFVtW4Rp+/4iXTJzhf7z5QaVVTjNLg8AgAYh8AI4woBOLfXFXYN14Ult5TSkGYu3auzz32lzdr7ZpQEAUG8EXgA1ig4N1vRLT9DzV52kFuHBWpuZp1HPLtery9PkZLMKAIAPIfACOKpze7fRV3cN0enHtVJZhVOPfLpO17z6o3bnFptdGgAAdULgBXBMraNDNfu6U/TomF4KDbbquy37NPK/y/Txr7vMLg0AgGMi8AKoE4vFomtOTdHndwxWn3Yxyiup0J1zf9Xt7/yi3KJys8sDAKBWBF4A9dKxVaTev3Wg7hrWRTarRf/3W6ZGPr1M327ea3ZpAADUiMALoN6CbVbdNew4fXDrQHWIj1BWXomumbVSD3+yViXlDrPLAwCgGgIvgAY7IbmFPrtjkK45NUWSNPv7dI165lv9sTPX5MoAADiEwAugUcJDgvTomF567bpT1CrKrq17CzX2+e/03DebVeFgswoAgPkIvAA84oyurfXVXUN0Tq9EVTgNPfnVJl364gpl7Cs0uzQAQIAj8ALwmNiIED1/1UmafmkfRdmDtHr7QZ3zv2/1zsrtMgw2qwAAmIPAC8CjLBaLLjypnb64a7D6d4hTUZlDU+b/oRvn/Ky9+aVmlwcACEAEXgBNol1suN6ZcKr+fm53hdisWrRhj0Y+vUxfrs0yuzQAQIAh8AJoMlarRROGdNQnt5+mbolR2l9YppvfWKW/vv8b48sAAM2GwAugyXVLjNbHk07Tzad3lMUivfvzTl358g/aV0CLAwCg6RF4ATQLe5BNU87prrdu6K/o0Mob2i584Xul5TDFAQDQtAi8AJrVwM7xmn/bQLWLDVPGviJd+Px3+jl9v9llAQD8GIEXQLPr3DpKH952mvq0i9GBonJd+cqP+vT3TLPLAgD4KQIvAFO0irJr7k0DNLxHgsoqnJr09i+auXQr83oBAB5H4AVgmrAQm2Ze3VfXDkyVJP3riw164KM1bEkMAPAoAi8AU9msFj18fk89dF4PWSzSWz9u142v/6yC0gqzSwMA+AkCLwCvcP2gDpp5dV+FBlu1ZONeXfbiCmXnlZhdFgDADxB4AXiNkT0TNfemAYqPDNHazDyNmfGdNmTlmV0WAMDHEXgBeJUTklto/q2nqWOrCO3OLdElL6zQ8s05ZpcFAPBhpgfeGTNmKDU1VaGhoerfv79Wrlx51POffvppde3aVWFhYUpOTtbdd9+tkpJDv/ZctmyZRo8eraSkJFksFn300UdN/A4AeFr7luGaf+tA9esQp/zSCl372kq9+/MOs8sCAPgoUwPvvHnzNHnyZE2dOlWrV69Wnz59NHLkSO3Zs6fG899++23dd999mjp1qtavX69Zs2Zp3rx5uv/++93nFBYWqk+fPpoxY0ZzvQ0ATaBFeIjeuKGfLjghSRVOQ399/3dN/2ojY8sAAPVmMUz8r0f//v11yimn6LnnnpMkOZ1OJScn6/bbb9d99913xPmTJk3S+vXrtWjRIvexe+65Rz/++KOWL19+xPkWi0UffvihxowZU6+68vLyFBMTo9zcXEVHR9fvTQHwKMMw9NRXm/Tc4i2SpLEnttW/Luote5DN5MoAAGaqT14zbYW3rKxMq1at0rBhww4VY7Vq2LBhWrFiRY2PGThwoFatWuVue9i2bZs+//xznXvuuY2qpbS0VHl5edW+AHgHi8Wiv4zsqn9d2Fs2q0Uf/rJL419dqdyicrNLAwD4CNMCb05OjhwOhxISEqodT0hIUFZWVo2PufLKK/XII49o0KBBCg4OVqdOnTR06NBqLQ0NMW3aNMXExLi/kpOTG/V8ADzv8n7t9dq1pyjSHqQftu3XRTO/1479RWaXBQDwAabftFYfS5Ys0eOPP67nn39eq1ev1vz58/XZZ5/p0UcfbdTzTpkyRbm5ue6vHTu4OQbwRkOOa6X3bhmgxOhQbdlToLHPf6ffdhw0uywAgJczLfDGx8fLZrMpOzu72vHs7GwlJibW+JgHH3xQ11xzjW688Ub17t1bY8eO1eOPP65p06bJ6Wz4VqR2u13R0dHVvgB4p+5tovXRxNPUvU20cgrKdPlLP2jhuuxjPxAAELBMC7whISHq27dvtRvQnE6nFi1apAEDBtT4mKKiIlmt1Uu22SpvXOHObSBwJMaE6r1bBuj041qpuNyhm974WbO/SzO7LACAlzK1pWHy5Ml6+eWXNWfOHK1fv1633nqrCgsLdd1110mSxo0bpylTprjPHz16tF544QXNnTtXaWlpWrhwoR588EGNHj3aHXwLCgr066+/6tdff5UkpaWl6ddff9X27dub/f0BaDqR9iC9Mv5kXdEvWYYhPfx/6/Top+vkcPLDLwCguiAzX/yyyy7T3r179dBDDykrK0snnHCCFixY4L6Rbfv27dVWdB944AFZLBY98MAD2rVrl1q1aqXRo0frsccec5/z888/64wzznD/efLkyZKk8ePHa/bs2c3zxgA0i2CbVY+P7a3kuHA9sWCjZi1P084DRXr6shMVFsLYMgBAJVPn8Hor5vACvueT3zL1l3d/U5nDqROSW+iV8ScrPtJudlkAgCbiE3N4AcCTzu+TpDdv7K8W4cH6dcdBjX3+O23dW2B2WQAAL0DgBeA3+nWI0we3DlT7uHDt2F+sC5//XivT9ptdFgDAZAReAH6lU6tIzb9toE5IbqHc4nJd/cqP+vjXXWaXBQAwEYEXgN+Jj7Rr7k2n6uyeiSpzOHXn3F81Y/EWxhcCQIAi8ALwS6HBNs246iTdOKiDJOk/X27U/R/+oXJHwzepAQD4JgIvAL9ls1r0wHk99I/ze8pqkd5ZuUM3zPlZBaUVZpcGAGhGBF4Afm/8wFS9eM3JCgu2admmvbpk5grtzi02uywAQDMh8AIICMN7JGjezacqPtKu9bvzNHbG91qXmWd2WQCAZkDgBRAwjm/XQh/eNlCdW0cqK69El764Qks37TW7LABAEyPwAggoyXHh+uCWgTq1Y5wKSit0/eyfNHfldrPLgp8qq3CquMxhdhlAs/hjZ64OFJZ55UQcAi+AgBMTHqzXr++vC09sK4fT0H3z/9B/vtzglf+Shu9yOA1d9ML3GvzEYu3JKzG7HKBJlZQ7NPq55Trx0YU6UFRudjlHIPACCEghQVY9dWkf3XFWF0nSjMVbdde8X1VawWocPGPBmiz9sStXOQWlevabLWaXAzSp7fuLJElRoUGKDQ82uZojEXgBBCyLxaLJw4/TExcfryCrRR//mqlrZq3UwaIys0uDjzMMQzOXbnX/+Z2V27V9X5GJFQFNKz2nUJKU2jJCFovF5GqOROAFEPAuPTlZs6/rpyh7kFam7deFL3xPOEGjrNi6T3/sylVosFWnpMaqwmlo+sKNZpcFNJmMqn9nprQMN7mSmhF4AUDSoC7xeu/WAUqKCdW2vYUa+/x3+mX7AbPLgo+auWybpMofpqaO7ilJ+vi3TG3IYhQe/FP6vsoV3g7xESZXUjMCLwBU6ZYYrQ8nnqaeSdHaV1imK17+QQvWZJldFnzM2sxcLdu0V1aLNGFwR/VqG6NRx7eRYUhPfskqL/yTK/CmtCTwAoDXS4gO1bs3D9AZXVuppNypW99apVnL08wuCz7kparV3VHHJyk5rvLXu/cMP042q0Vfr9+jn9P3m1ke0CTScypbGlJpaQAA3xBhD9LL407WVf3byzCkRz9dp4c/WSuHk7FlOLod+4v06e+7JUk3D+noPt6xVaQuPbmdJOmJLzcyAg9+pbTCocyq7dpZ4QUAHxJks+qfY3ppyjndJEmzv0/XLW+uUlFZhcmVwZvNWp4mh9PQ4C7x6tU2ptr37jiri0KCrFqZtp8d/uBXduwvlmFIkfYgxUeGmF1OjQi8AFALi8Wim0/vpOeuPFEhQVYtXJetK176QXvzS80uDV5of2GZ5v5UuWvfzUM6HfH9NjFhGj8gRZL0ny83yslvDOAnXCPJUlqGe+VIMonACwDHdN7xSXr7xv6KDQ/WbztzNfb577RlT77ZZcHLvLEiQyXlTvVMitZpnVvWeM5tQzsryh6ktZl5+uyP3c1cIdA0XDespXppO4NE4AWAOjk5NU7zbztNKS3DtfNAsS58/nut2LrP7LLgJYrLHJqzIl2SdMvpnWpd5YqNCNGEqt7e6Qs3qdzhbK4SgSbj7TN4JQIvANRZh/gIzb91oE5q30J5JRUa9+qP+vCXnWaXBS/w3qod2l9YpuS4MJ3TK/Go514/qINaRoQoLadQ76/i8wPfxwovAPiZlpF2vT3hVI3q3UblDkN3z/tNzy7azF33AazC4dTL31aOIpswuKOCbEf/T2ukPUgTz+gsSfrf15tVUu5o8hqBpuRa4U310k0nJAIvANRbaLBNz15xonvs1FMLN+lvH/zOr6cD1OdrsrRjf7HiIkJ0Sd/kOj3mqlPbq22LMGXllej1qlYIwBeVVTi184B3z+CVCLwA0CBWq0VTzu2uRy/oKatFevfnnbp+9k/KKyk3uzQ0I8Mw9OLSrZKk8QNSFRZiq9Pj7EE23TWsiyTp+SVb+dzAZ+08UCSnIYUF29Qqym52ObUi8AJAI1wzIFWvjD9Z4SE2fbs5R5fOXKHMg8Vml4VmsnxLjtZm5iks2KZxVSPH6urCk9qpc+tIHSwq1ytVu7MBvubwG9a8dSSZROAFgEY7s1uC5t00QK2i7NqQla+xz3+ntZm5ZpeFZvDi0sqgetkpyYqNqN/AfZvVor+M6CpJemV5mnIKmO8M3+MLN6xJBF4A8Ije7WL00cTTdFxCpLLzSnXpzBVavHGP2WWhCa3ZlavlW3Jks1p0w6AODXqOkT0T1Ce5hYrKHHrumy0erhBoeq5NJ7z5hjWJwAsAHtO2RZjeu2WgBnZqqcIyh26c87Pe/nG72WWhicys6t0dfXwbJcc17GYdi8Wiv46sXOV9+8ft7pt/AF+Rvs/7b1iTCLwA4FExYcGafV0/Xdy3nRxOQ/d/+If+9cUGtpH1M9v3Fenzqp3SbqphG+H6OK1zvE7r3FJlDqee/nqzJ8oDmk3GPte2wqzwAkBACQmy6j8XH6+7hx0nqXIl8I65vzBv1Y+8snybnIY05LhW6pEU3ejn++vIbpKk+at3anM221bDN5Q7nNp5oPIm3dR4VngBIOBYLBbdOayLnrqkj4JtFn36+25d/cqPOlBYZnZpaKR9BaV69+cdkqRbTu/okefsk9xCZ/dMlNOQnvxqo0eeE2hqmQeLVeE0FBpsVUJUqNnlHBWBFwCa0EV922nOdf0UFRqknzMOaPxrK+WgvcGnzVmRoZJyp45vF6MBHVt67Hn/MvI4WS3Sl2uz9euOgx57XqCppFXdsJYSFyGr1XtHkkkEXgBocgM7x2v+rQMVFRqk33fmav7qnWaXhAYqKqtw74x285BOHp072rl1lC48qZ0k6T9fbvDY8wJN5fAZvN6OwAsAzaBLQpRuP7OzpMpfWReX0c/ri+b9tEMHi8qV0jJcZ/dK9Pjz3zWsi0JsVn23ZZ+Wb87x+PMDnuSewevlI8kkAi8ANJtxA1LVLjZM2XmlevlbdtbyNeUOp175Nk2SNGFwR9ma4Fe47WLDddWp7SVVrvIaBu0v8F6s8AIAjhAabNPfzq68G3/m0q3ak19ickWoj8//2K1dB4sVHxmii/u2a7LXmXhGZ4WH2PTbzlx9uTaryV4HaCzXCm8HLx9JJhF4AaBZnXd8G51QtbPWfxduMrsc1JFhGJpZtY3wtQNTFRpsa7LXio+068aqndue/GqTKhzOJnstoKEqHE7t2F+1wktLAwDgcBaLRQ+M6i6psh90YxYzV33Bss05Wr87T+EhNl19akqTv96NQzoqNjxYW/YUaP4vu5r89YD62p1bonKHoZAgq9pEe/dIMonACwDN7uTUOJ3Tq3Lm6uOfrze7HNTBzCWV2whf0a+9WoSHNPnrRYcG67ahlTc5/u/rzSqt4CZHeBdXO0P7uHCvH0kmEXgBwBR/O7ubgm0WLd20V8s27TW7HBzFbzsOasW2fQqyWnR9VatBc7hmQIoSo0O162Cx3vphe7O9LlAX6VU3rKX6QP+uROAFAFOkxkfomlNTJVWu8rIZhfd6aVll7+75fZLUtkVYs71uaLBNdw7rIkmasXiLCkormu21gWNJr9p0ItUHJjRIBF4AMM0dZ3VWdGiQNmTl64NVbEbhjdJzCvXFmt2SpJs8tI1wfVzSt506xkdoX2GZZlWNRAO8QUZVS4Mv3LAmEXgBwDQtwkN0x1mVK3hPfrVRhazgeZ2Xv90mpyGd0bWVuiVGN/vrB9msmjziOHct+wvLmr0GoCaHWhpY4QUAHMM1A1LUPi5ce/LZjMLb7M0v1XtVK+83n97JtDrO7dVGPZOiVVBaoReWbDGtDsDF4TS0nR5eAEBd2YMObUbx4tJtys5jMwpvMef7dJVVOHVCcgv17xBnWh1Wq0X3juxaWdOKDO3OLTatFkCSsvJKVOZwKthmUVIz9rU3BoEXAEx2bu9EndS+hYrLHZr+FZtReIPC0gq9viJdknTL6R1lsZg7dun041qpf4c4lVU49b+vN5taC+C6YS05LrxJtthuCgReADCZxWLR30f1kCS9u2qH1u/OM7kizP1ph/JKKtQhPkLDeySaXY4sFov+WvWbgPdW7dS2vQUmV4RA5prB6yvtDBKBFwC8Qt+UWI06vo2Mqs0oDIMxZWYpdzg1q6qf+qYhHb1mBatvSqyGdW8th9PQU2xLDRNlVPXvpvjIDWsSgRcAvMbfRnZTiM2qbzfnaCmbUZjm/37LVGZuieIj7Rp7Yluzy6nmLyO7ymKRPvt9t9bsyjW7HASoQzN4WeEFANRT+5bhGj8wRVLlKm+Fw2lyRYHHMAy9uLRydfe601IVGmwzuaLquiVGa8wJlSH8iS83mlwNApVrhTfVR2bwSgReAPAqk87oohbhwdqUXeAeiYXms2TjXm3MzldEiE1Xn5pidjk1unvYcQqyWrRs0179sG2f2eUgwDidxmE9vLQ0AAAaICY8WHecWbkZxVNfbWI72WY2c+lWSdKV/dsrJizY5Gpq1r5luK7o116S9MSCDfR7o1ll55eotMKpIKulWbfabiwCLwB4matPTVFqy3DlFJTqpaoAhqb3y/YD+jFtv4JtFl0/qIPZ5RzV7Wd2VmiwVau3H9TX6/eYXQ4CSHpOZTtDu9gwBdl8J0b6TqUAECBCgqy675zKEVQvfbuNjQaaiat394IT2qpNjHevXLWODtX1p1WG8ie/3CiHk1VeNI8MVzuDD/XvSgReAPBKI3sm6pTUWJWUO/UUm1E0uW17C/TluixJ0s1DOppcTd3cPKSTokODtDE7X5/8tsvschAg0nxwBq9E4AUAr3T4ZhQfrN6ptZmMoGpKL3+7TYYhDeveWl0Soswup05iwoN1y9BOkqTpCzeprIKpHmh6GTm+N4NXIvACgNc6IbmFzu+TJMOQHvuMzSiayp78En2wqnKF9ObTO5lcTf1cN7CDWkXZtWN/seb+tN3schAAfHGXNYnACwBe7d6RXRUSZNX3W/dp8UZuTmoKs79LV5nDqZPat9DJKbFml1MvYSE23XFW5VSPZxZtUVEZUz3QdAzD8Mld1iQCLwB4teS4cF13Wqok6fHPN7AZhYfll5TrjR8yJEm3nN5JFot3bCNcH5ednKz2cZVTPV77Lt3scuDH9uaXqrjcIZvVonaxBF4AgAfdNrSzYsODtWVPgeb+tMPscvzK3JU7lF9SoY6tIjSse4LZ5TRISJBVk4cfJ0l6celW5RaVm1wR/FVa1ZbCbVuEKSTItyKkb1ULAAEoJixYdw2rDDRPf71J+SUEGk8oq3Bq1vI0SZWTGaxW31vddTm/T5K6JUYpr6RCLzC7GU3EV9sZJAIvAPiEK/u3V8f4COUUlLl3A0PjfPzrLmXllah1lF1jTmxrdjmNYrVadO/IrpKk2d+naU9eickVwR/56g1rEoEXAHxCsO3QZhSvfJumzINsRtEYTqehl5ZVbjRx/aAOsgfZTK6o8c7s1lp9UypnNz/zzWazy4Efcq3w+tqmExKBFwB8xvAeCerXIU6lFU49+eVGs8vxaYs37tHmPQWKsgfpyv7tzS7HIywWi/5atco7d+UO945YgKccWuGlpQEA0EQsFoseGNVdkjT/l136YyebUTSUqy3kylPbKzo02ORqPKd/x5Y6/bhWqnAamr6QHfrgOYZhKL3qprUUWhoAAE3p+HYtNOaEJEnSY5+vYzOKBliVsV8/pR9QsM2i60/rYHY5Hufq5f3kt0yt351ncjXwFzkFZSosc8hikZLjwswup94IvADgY+49u5tCgqz6Ydt+fb2ezSjq68Wllb27Y09sq4ToUJOr8bxebWN03vFtZBii9QUe42qRSYoJ88medwIvAPiYti3CdMOgypXJaV+sVzmbUdTZlj0FWrg+W5J00xDf2ka4Pu4Z0VU2q0WLNuzRz+n7zS4HfiC96oa1Dj54w5pE4AUAn3Tb0E5qGRGibXsL9c7K7WaX4zNeXrZNhlF5A2Dn1pFml9NkOsRH6NKT20mSnliwkdYXNNqh/l3fu2FNIvACgE+KCg3WXcO6SJKe/nqz8tiM4piy80r04S+7JFVuI+zv7jiri+xBVq1M368lm/aaXQ58nC/P4JUIvADgsy7v116dWkVof2GZnl/MZhTH8up3aSpzOHVKaqz6psSaXU6TaxMTpvEDUyVJ/1mwUU4nq7xoOF/eZU0i8AKAzwq2WTXlnMoxZa9+l6adB4pMrsh75ZWU6+0fKls/bvbj3t0/u/X0ToqyB2nd7jx9+sdus8uBjzIMw73CSw8vAKDZndW9tQZ0bKmyCqf+wx35tXr7x+3KL61Ql9aROrNba7PLaTaxESG6aUhHSdL0rzZygyMa5EBRufJLKqpGkrHCCwBoZhaLRX8f1V0Wi/Txr5n6bcdBs0vyOqUVDr26PE2SdNOQjrJaLSZX1LyuH9RB8ZEhSt9XpPd+3ml2OfBBaVU3rLWJDlVosO+NJJMIvADg83q1jdHYE9tKkh77bD135P/Jx79kak9+qRKjQ3XBCW3NLqfZRdiDNPGMzpKk/y3apJJyh8kVwde4ZvD64g5rLgReAPAD947s6r4j/6t12WaX4zWcTkMzl1Xe0HfDoA4KCQrM/+xd2b+92rYIU3ZeqeZ8n252OfAxrhm8qfG+2c4gEXgBwC+0iQnThMGVvZr/+mKDyiro1ZSkr9dna9veQkWFBunyfslml2Mae5BNdw8/TpL0wtKtjLFDvWT4+EgyicALAH7jlqGdFB8ZorScQr39Y4bZ5XiFF5dVbiN89akpigoNNrkac409sa26tI7UwaJyvVx1XYC6OLTpBIEXAGCySHuQexXvf4s2K7c4sFfxfkrfr1UZBxRis+q601LNLsd0NqtF94zoKkmatTxNe/NLTa4IvoKWBgCAV7ns5GR1aR2pA0XlmrF4i9nlmOrFpZW9uxf1bavWUaEmV+MdRvZMUJ/kFioqcwT85wN1c7CozP3Dc3sfHUkmEXgBwK8E2ay6/9zKzShmf5euHfsDczOKzdn5+nr9Hlkscvc2o3KM3d9GVq7yvvVjRsB+PlB3rtXdhGi7wkOCTK6m4Qi8AOBnhnZtpUGd41XmcOrfCzaYXY4pXL27I3skqmOrSJOr8S4DO8drUOd4lTsMPf31ZrPLgZfzhxvWJAIvAPgdi8Wi+8+t3Izi0993a/X2A2aX1Kx25xbr4193SZJuPp3V3ZrcW7XK++EvO7UpO9/kauDNXJtOEHgBAF6nR1K0Lj6pnaTA24zi1eVpKncY6t8hTie2jzW7HK/UJ7mFzu6ZKKchPcmW1DiKjKqWhhQfvmFNIvACgN+6Z0RXhQXbtCrjgBasyTK7nGaRW1yut3/cLkm65fROJlfj3f4y8jhZLdJX67L1S4D9FgB1l05LAwDAmyXGhGrCkKrNKBYExmYUb/2YocIyh7omRGlo11Zml+PVOreO0kVVvwV4YsHGgPotAOrOtcJL4AUAeK2bh3RUqyi7MvYV6Y0f/HszipJyh15dni6psnfXYrGYW5APuGv4cQqxWbVi2z4t35JjdjnwMrnF5dpfWCZJSmlJSwMAwEtF2IN0T9VmFM8s2qyDRWUmV9R0Pvxll3IKSpUUE6rRfZLMLscntG0RpqtObS9J+s+XrPKiOteEhlZRdkXYfXckmUTgBQC/d8nJyeqaEKXc4nI9941/bjbgcBru7XKvH9RBwTb+81ZXE8/orIgQm37fmRswvd6oG/cOaz6+uisReAHA79msFt0/qnIzijkr0t2rNv5k4bosbcspVExYsK7o197scnxKfKRdN1RtzvHkVxtV4fD/Xm/UTUbVSLIUH+/flQi8ABAQTj+ulQZ3qdxs4IkF/jWGyjAMvbC0cnX3mlNTfP5Xr2aYMLiDYsODtXVvoeav3mV2OfASrhXeDvEEXgCAj/j7qO6yWqTP/titVRn7zS7HY1am7ddvOw4qJMiq8QNTzS7HJ0WFBuu2oZ0lSU9/vUkl5Q6TK4I3cI0k8/Ub1iQCLwAEjG6J0bqkb7Ik6Z9+tBnFzKVbJUmX9G2nVlF2k6vxXdcMSFGbmFBl5pborapZxghs/rKtsETgBYCAcs+I4xQeYtMv2w/qsz92m11Oo23IytPijXtlsUgTBrONcGOEBtt051ldJEkzFm9RQWmFyRXBTPkl5cop8I+RZBKBFwACSuvoUN08pHIHsn8v2KDSCt/+1fVLVZMZzumVqFQ/6DM028V926ljfIT2F5bplW+3mV0OTOTacCI+MkRRocEmV9N4BF4ACDAThnRQQrRdO/YX6/XvfXczil0Hi/XJr5mS5A7xaJwgm1WTR1TObX7l2zT3pgMIPK7A6w8TGiQvCbwzZsxQamqqQkND1b9/f61cufKo5z/99NPq2rWrwsLClJycrLvvvlslJSWNek4ACBThIUG6Z0RXSdKz32zWAR8NNa8uT1OF09CAji3VJ7mF2eX4jXN7tVGvttEqKK3Q84v9c24zjs2fbliTvCDwzps3T5MnT9bUqVO1evVq9enTRyNHjtSePXtqPP/tt9/Wfffdp6lTp2r9+vWaNWuW5s2bp/vvv7/BzwkAgeaik9qpW2KU8koq9Mw3m80up94OFpXpnZWVN1bdMpTVXU+yWi26d2Q3SdLrP2Qo82CxyRXBDOk5/nPDmuQFgXf69OmaMGGCrrvuOvXo0UMzZ85UeHi4Xn311RrP//7773XaaafpyiuvVGpqqkaMGKErrrii2gpufZ8TAAKNzWrRA6N6SJLeWJGhtBzf2ozizR8yVFTmULfEKA3pEm92OX5nSJd49e8Qp7IKp/73te/9QITGO9TSwApvo5WVlWnVqlUaNmyY+5jVatWwYcO0YsWKGh8zcOBArVq1yh1wt23bps8//1znnntug5+ztLRUeXl51b4AwN8N6hKvoV1bqcJp6N9fbDC7nDorKXdo9vfpkqRbTu8ki8VibkF+yGKx6K9nV67yvrdqh7buLTC5IjQ3V0uDP2w6IZkceHNycuRwOJSQkFDteEJCgrKyat7P+8orr9QjjzyiQYMGKTg4WJ06ddLQoUPdLQ0Nec5p06YpJibG/ZWcnOyBdwcA3u/+cys3o1iwNksr03xjM4r3V+1UTkGZ2rYI06jj25hdjt/qmxKrYd0T5DSk6V9tMrscNKOisgrtyS+VJKXEEXhNsWTJEj3++ON6/vnntXr1as2fP1+fffaZHn300QY/55QpU5Sbm+v+2rFjhwcrBgDvdVxClC47pb0k6bHP1snp9O7NKBxOQy9Xjcu6cXAHBdt87j9jPuXekV1lqdqd74+duWaXg2aSnlPZzhAbHqyYcN8fSSaZHHjj4+Nls9mUnZ1d7Xh2drYSExNrfMyDDz6oa665RjfeeKN69+6tsWPH6vHHH9e0adPkdDob9Jx2u13R0dHVvgAgUNw9vIsiQmz6bWeu/u/3TLPLOaov12YpY1+RWoQH67JT+G1cU+uaGKUxJ7SVJD3xpe+0vaBxMtwTGvxjdVcyOfCGhISob9++WrRokfuY0+nUokWLNGDAgBofU1RUJKu1etk2m02SZBhGg54TAAJZ66hQ3XJ65aSDJxZsVEm5d25GYRiGexvhcQNSFR4SZHJFgeHuYccpyGrRt5tztGLrPrPLQTNIr7phLdVPbliTvKClYfLkyXr55Zc1Z84crV+/XrfeeqsKCwt13XXXSZLGjRunKVOmuM8fPXq0XnjhBc2dO1dpaWlauHChHnzwQY0ePdodfI/1nACA6m4c3FGJ0aHadbDYfUOYt1mxbZ9+35kre5BV4wekmF1OwGjfMlxX9q9se3niyw0yDO9ue0HjuVZ4/Wn3QtN/PL7sssu0d+9ePfTQQ8rKytIJJ5ygBQsWuG862759e7UV3QceeEAWi0UPPPCAdu3apVatWmn06NF67LHH6vycAIDqwkJs+svIrvrLe79pxjdbdOnJyYqLCDG7rGpeXFrZu3vpyclqGWk3uZrAMunMznrv5536ZftBLVyXrRE9a24RhH9I87MZvJJkMfhR7Qh5eXmKiYlRbm4u/bwAAobTaei8Z5dr3e48jR+Qon9c0MvsktzWZebp3Ge+ldUiLfnLGWrvR79q9RVPLNig55ds1XEJkfriziGyWRkH569OfXyRsvJK9OFtA3Vi+1izy6lVffKa6S0NAADvYLVa9MCo7pKkt37c7lWzV19aVtm7e27vNoRdk9w8pJOiQ4O0KbtAH/+6y+xy0ESKyxzKyiuR5F8rvAReAIDbwM7xOqtba1U4Df3LSzaj2HmgSP/3+25Jct9ch+YXEx6sW4d2liRNX7hJZRVOkytCU9i+v/KGtZiwYMV6WVtTYxB4AQDVTDm3m2xWixauy9YP28y/K/+Vb9PkcBoa1DlevdrGmF1OQLt2YKpaR9m180Cx3lm53exy0ARcO6z504QGicALAPiTzq2jdEW/yhm3j3223tTNKA4UlmneT5WbAd18ekfT6kClsBCbbj+riyTp2W+2qKiswuSK4GnpOf43g1ci8AIAanDXsOMUaQ/SH7ty9fFv5vVrvr4iQ8XlDvVMitagzvGm1YFDLjs5We3jwpVTUKrXvks3uxx4mD/O4JUIvACAGsRH2nXr0Mp+2f+YtBlFcZlDc1akS5JuPr2TLBamAniDkCCr7hlxnCRp5tKtOlhUZnJF8CR/3GVNIvACAGpxw6AOSooJVWZuiWYtT2v2139/1Q7tLyxTclyYzu3F3FdvMvr4JHVLjFJ+SYVeqNr9Dv4hw7XC60ebTkgEXgBALUKDbbr37K6SpBeWbFVOQWmzvXaFw6mXvq3caGLC4I4KsvGfK29itVp078jKz8bs79KVXTXGCr6tpNyhzNxiSbQ0AAACyAV92qp32xgVlFbo6a83NdvrfrEmSzv2Fys2PFiX9E1uttdF3Z3ZrbX6psSqtMKpZxZtNrsceMCO/UUyDCnKHuR1Oy02FoEXAFArq9Wiv1dtRvHOyh3asie/yV/TMAy9WLXRxPiBqQoLsTX5a6L+LBaL/nZ2N0nSvJ92uO/uh+9y3bCWEh/udz3zBF4AwFGd2rGlhvdIkMNpaNrnTb8ZxXdb9mnNrjyFBds0fkBqk78eGq5fhzgN7dpKFU5D0xc2328A0DQy3DN4/at/VyLwAgDq4L5zuinIatGiDXv0/ZacJn0t1+ruZack+9VOT/7qLyMqe3k/+S1T6zLzTK4GjZFO4AUABLJOrSJ1Vf/2kqTHPm+6zSjW7MrVt5tzZLNadMOgDk3yGvCsXm1jNLpPkiTpya82mlwNGiM9p6qlwc9uWJMIvACAOrrjrC6KsgdpbWaePvylaTajeHFZ5WSG845vo+Q4//uPrr+aPPw42awWfbNhj35K3292OWgg9wqvn40kkwi8AIA6ahlp18QzO0uS/vPlRhWXeXYzih37i/TZ75mSpJuGsI2wL+kQH6FLT66cpvHEgg0yDPO2o0bDlFY4lHmwciQZK7wAgIB27cBUtW0Rpqy8Es1avs2jz/3Kt9vkNKTBXeLVMynGo8+NpnfnWV1kD7Lqp/QDWrJxr9nloJ52HiiW05AiQmxqFWk3uxyPI/ACAOosNNimvx62GcWefM9sOLCvoFTzft4hSbr19E4eeU40r8SYUF07MFWS9MSXG5uszxtN4/Athf1tJJlE4AUA1NPo45PUp12MCsscevprz2w48PqKDJWUO9W7bYwGdGrpkedE87vl9E6Ksgdp/e48/V9Vewp8Q1qOa0th/2tnkAi8AIB6qtyMoockae7K7dqU3bjNKIrKKvT6inRJ0s2nd/TL1aVAERsR4u6/nr5wk8odTpMrQl0dvsLrjwi8AIB669chTiN7JshpSI9/vr5Rz/XuTzt0oKhc7ePCdU6vNh6qEGa5flAHxUeGKGNfkd6talOB93PtspbqhzesSQReAEAD3XdOdwVZLVqyca++3dywm5QqHE69/G2aJGnCkI6yWVnd9XUR9iDdNrRymse7PxF4fYU/77ImEXgBAA3UIT5CV5+aIkl67LP1cjTgJqXP/titXQeL1TIiRJf0befpEmGSs7q3liStz8qnrcEHlDuc2nmgciSZP87glQi8AIBGuPOsLooKDdKGrHx9sHpnvR5rGIZmLq0cbXbtwFSFBtuaokSYIDk2XFH2IJVVOLVlT4HZ5eAYdh4olsNpKDTYqtZR/jeSTCLwAgAaITYiRLdXbUbx5JcbVVRWUefHfrs5R+t35yk8xKZrBqQ0VYkwgdVqUfekaEnS2sw8k6vBsaQf1s7grzeNEngBAI0yfmCq2sWGaU9+qV5ellbnx81culWSdPkp7dUiPKSpyoNJelVtHrJmV67JleBYMnL8u39XIvACABrJHmTT387uJkl6cdlW7ck79mYUv+88qO+37pPNatENgzs0dYkwQc+qFd51rPB6PdeEhhQ/ncErEXgBAB5w3vFtdGL7Fioqc2j6wk3HPP/FZZW9u+f3SVLbFmFNXR5M0Ktt5Qrvut157Lrm5dL9fEKDROAFAHiAxWLRA6O6S5Le/XmHNmTVvqqXsa9QX/yxW1LlRhPwT51aRcgeZFVBaYUy9heZXQ6OIsO1wuunM3glAi8AwEP6psTp3N6JVZtRbKj1vJe/3SanIQ3t2krdEqObsUI0pyCbVd3auG5co4/XW1U4nNqx37XpBCu8AAAc09/O7qZgm0XLNu3V0k1HbkaRU1Cq936uHF9285BOzV0empmrj3fNLvp4vVXmwRJVOA3Zg6xKjA41u5wmQ+AFAHhMSssIjRuQKkl6vIbNKOZ8n67SCqf6JLfQqR3jTKgQzck1qYEVXu/l6t9NaRkuqx/vdEjgBQB41O1ndlZMWLA2ZufrvZ8PbS1bWFqh11dkSJJuGdLRb+d94pCeh83iNQxuXPNGhwKv/7YzSAReAICHtQg/tBnFUws3qbC0cjOKeT/tUG5xuTrER2hEz0QzS0Qz6ZoYJZvVov2FZcqqw7g6NL/0HFf/rv/esCYReAEATWDcgFSltAzX3vxSvbhsm8odTs1aXrkpxYTBHWXz41+d4pDQYJu6tI6URB+vt8pwjSSLZ4UXAIB6CQmyujejeGnZVr3ybZp2HSxWfKRdF57U1uTq0Jx60sfr1QJhBq9E4AUANJFzeiWqb0qsSsqd+veCyjFl152WqtBgm8mVoTkxqcF7OZyGduwvluTfM3ilBgTe1NRUPfLII9q+fXtT1AMA8BMWi0V/r9qMQpIiQmy6un+KiRXBDIe2GGaF19tkHixWmcOpEJtVbWL8e8fDegfeu+66S/Pnz1fHjh01fPhwzZ07V6WlpU1RGwDAx53UPlaj+yRJkq46NUUx4cEmV4Tm1qMq8Gbmlmh/YZnJ1eBwrh3WkuPC/L6vvkGB99dff9XKlSvVvXt33X777WrTpo0mTZqk1atXN0WNAAAf9sRFx+uFq07SX0Z0NbsUmCAqNNg9AYA+Xu/i6t/t4Oc3rEmN6OE96aST9MwzzygzM1NTp07VK6+8olNOOUUnnHCCXn31VebtAQAkSWEhNp3Tu41CgrhtJFD1bOu6cY0+Xm+SESAzeKVGBN7y8nK9++67Ov/883XPPffo5JNP1iuvvKKLLrpI999/v6666ipP1gkAAHzUoRvXWOH1JmkBMoNXkoLq+4DVq1frtdde0zvvvCOr1apx48bpv//9r7p16+Y+Z+zYsTrllFM8WigAAPBNri2G17HC61UCaYW33oH3lFNO0fDhw/XCCy9ozJgxCg4+8gaEDh066PLLL/dIgQAAwLe5Vni35RSqoLRCkfZ6xw94mNNpKGO/a4WXwHuEbdu2KSXl6GNlIiIi9NprrzW4KAAA4D9aRtrVJiZUu3NLtH53nk5JjTO7pICXlVeisgqngm0WJbUINbucJlfvHt49e/boxx9/POL4jz/+qJ9//tkjRQEAAP9CH693cU1oSI4NV5DN/28orfc7nDhxonbs2HHE8V27dmnixIkeKQoAAPiXQ1sM08frDdKrbljz9x3WXOodeNetW6eTTjrpiOMnnnii1q1b55GiAACAf2GF17sE0g1rUgMCr91uV3Z29hHHd+/eraAgmtABAMCRXLN4t+wpUEm5w+RqEEibTkgNCLwjRozQlClTlJt76Ce0gwcP6v7779fw4cM9WhwAAPAPSTGhig0PVoXT0KbsfLPLCXiubYVpaajFk08+qR07diglJUVnnHGGzjjjDHXo0EFZWVl66qmnmqJGAADg4ywWC328XsLpNNwrvIEwkkxqwFiytm3b6vfff9dbb72l3377TWFhYbruuut0xRVX1DiTFwAAQJJ6to3W8i059PGabE9+qUrKnbJZLWobG2Z2Oc2iQU23ERERuummmzxdCwAA8GOs8HoH1+puu9gwBQfASDKpgYFXqpzWsH37dpWVlVU7fv755ze6KAAA4H96VU1q2JCVpwqHMyDmv3qjjABrZ5AauNPa2LFj9ccff8hiscgwDEmVvTmS5HBw5yUAADhSassIRYTYVFjm0LacQh2XEGV2SQEpfZ9rS+HAuGFNasBNa3feeac6dOigPXv2KDw8XGvXrtWyZct08skna8mSJU1QIgAA8AdWq0U9qlZ512bSx2uW9JzAmsErNSDwrlixQo888oji4+NltVpltVo1aNAgTZs2TXfccUdT1AgAAPyEq493zS76eM3iXuGNZ4W3Vg6HQ1FRlb+CiI+PV2ZmpiQpJSVFGzdu9Gx1AADAr/RkhddUhmHQw1sXvXr10m+//aYOHTqof//+euKJJxQSEqKXXnpJHTt2bIoaAQCAnzh8UoNhGO57gNA89haUqqjMIatFahcbOCu89Q68DzzwgAoLK38yeOSRR3Teeedp8ODBatmypebNm+fxAgEAgP/okhCpEJtV+SUV2rG/WO0D6MYpb5CeU9nO0DY2TCFBgTMlo96Bd+TIke7/37lzZ23YsEH79+9XbGwsP6UBAICjCrZZ1TUxSn/sytWazFwCbzMLtB3WXOoV7cvLyxUUFKQ1a9ZUOx4XF0fYBQAAdUIfr3lc/bspAfaDRr0Cb3BwsNq3b8+sXQAA0GA92zKpwSyHZvCywntUf//733X//fdr//79TVEPAADwc4ev8Lo2sELzCMQJDVIDenife+45bdmyRUlJSUpJSVFERPULtnr1ao8VBwAA/E/3xGhZLVJOQZn25JcqITrU7JICgmEY7pvWAmkGr9SAwDtmzJgmKAMAAASKsBCbOrWK1OY9BVqbmUvgbSb7CstUUFohS4CNJJMaEHinTp3aFHUAAIAA0qttTGXg3ZWnM7slmF1OQHC1MyTFhCk02GZyNc0rcAawAQAAr+Hq413DpIZmE6jtDFIDVnitVutRR5AxwQEAABzL4TuuoXkcGkkWWDesSQ0IvB9++GG1P5eXl+uXX37RnDlz9I9//MNjhQEAAP/Vo2qFd+eBYh0sKlOL8BCTK/J/ae6RZKzwHtMFF1xwxLGLL75YPXv21Lx583TDDTd4pDAAAOC/YsKC1T4uXNv3F2ldZp4Gdo43uyS/F8grvB7r4T311FO1aNEiTz0dAADwc/TxNh/DMJSWUxl4O8QTeBukuLhYzzzzjNq2beuJpwMAAAHg0AYU9PE2tYNF5covqZAktY+jpeGYYmNjq920ZhiG8vPzFR4erjfffNOjxQEAAP91aIthVnibWlpVO0ObmNCAG0kmNSDw/ve//60WeK1Wq1q1aqX+/fsrNjbWo8UBAAD/5Vrh3ZZTqKKyCoWH1DuWoI4O9e8G3uqu1IDAe+211zZBGQAAINC0jgpV6yi79uSXav3uPPVNiTO7JL/lnsEbgDesSQ3o4X3ttdf03nvvHXH8vffe05w5czxSFAAACAz08TYP1wpvagDesCY1IPBOmzZN8fFHjg5p3bq1Hn/8cY8UBQAAAkMv+nibRXoAz+CVGhB4t2/frg4dOhxxPCUlRdu3b/dIUQAAIDCwwts80gN4Bq/UgMDbunVr/f7770cc/+2339SyZUuPFAUAAAKDa4vhTdn5KqtwmlyNfzpYVKaDReWSAvemtXoH3iuuuEJ33HGHFi9eLIfDIYfDoW+++UZ33nmnLr/88qaoEQAA+Kl2sWGKCQtWucPQpux8s8vxSxlV7QwJ0faAnYRR73f96KOPKj09XWeddZaCgiof7nQ6NW7cOHp4AQBAvVgsFvVMitb3W/dpXWaeu6cXnhPo7QxSAwJvSEiI5s2bp3/+85/69ddfFRYWpt69eyslJaUp6gMAAH7OFXjXZObqUiWbXY7fyQjwG9akBgRely5duqhLly6erAUAAAQg16ouN641jfQcVnjr3cN70UUX6d///vcRx5944gldcsklHikKAAAEDtekhnWZeXI4DZOr8T+uloZA3XRCakDgXbZsmc4999wjjp9zzjlatmyZR4oCAACBo0N8pMKCbSoudyitajUSnuNuaYgP3JaGegfegoIChYSEHHE8ODhYeXn8KgIAANSPzWpR9zZRkqS1mWxA4Ul5JeXaV1gmiZaGeundu7fmzZt3xPG5c+eqR48eHikKAAAEFtc8Xvp4PSsjp3J1Nz7Srkh7YI4kkxpw09qDDz6oCy+8UFu3btWZZ54pSVq0aJHefvttvf/++x4vEAAA+L9ebSv7eNli2LMO9e8GbjuD1IDAO3r0aH300Ud6/PHH9f777yssLEx9+vTRN998o7i4uKaoEQAA+LnDV3gNw5DFYjG5Iv+QwQxeSQ1oaZCkUaNG6bvvvlNhYaG2bdumSy+9VH/5y1/Up08fT9cHAAACQJeESAXbLMotLteug8Vml+M30qtuWOsQwDesSQ0MvFLltIbx48crKSlJTz31lM4880z98MMPnqwNAAAECHuQTV1aV964tmYXfbyewgpvpXq1NGRlZWn27NmaNWuW8vLydOmll6q0tFQfffQRN6wBAIBG6dU2Wut252ldZq7O7pVodjl+IS3HtctaYAfeOq/wjh49Wl27dtXvv/+up59+WpmZmXr22WebsjYAABBAXH28a5jU4BEFpRXKKSiVJKUEeEtDnVd4v/jiC91xxx269dZb2VIYAAB4nGtSA7N4PcPVztAyIkTRocEmV2OuOq/wLl++XPn5+erbt6/69++v5557Tjk5OU1ZGwAACCDdEqNlsUjZeaXam19qdjk+z7XDWkqAjyST6hF4Tz31VL388svavXu3br75Zs2dO1dJSUlyOp1auHCh8vPzm7JOAADg5yLsQeoQX9lryipv4x2awRvY/btSA6Y0RERE6Prrr9fy5cv1xx9/6J577tG//vUvtW7dWueff35T1AgAAAJEL3Zc85j0HCY0uDR4LJkkde3aVU888YR27typd955x1M1AQCAANUziT5eT3HN4E0N8BvWpEYGXhebzaYxY8bok08+adDjZ8yYodTUVIWGhqp///5auXJlrecOHTpUFovliK9Ro0a5z8nOzta1116rpKQkhYeH6+yzz9bmzZsbVBsAAGg+vdpWTWpgFm+jZdDS4OaRwNsY8+bN0+TJkzV16lStXr1affr00ciRI7Vnz54az58/f752797t/lqzZo1sNpsuueQSSZJhGBozZoy2bdumjz/+WL/88otSUlI0bNgwFRYWNudbAwAA9eRa4d2+v0i5xeUmV+O7isoqlJ1XeeMfgdcLAu/06dM1YcIEXXfdderRo4dmzpyp8PBwvfrqqzWeHxcXp8TERPfXwoULFR4e7g68mzdv1g8//KAXXnhBp5xyirp27aoXXnhBxcXFtbZdlJaWKi8vr9oXAABofi3CQ9S2RZgkaR19vA3mmtDQIjxYMeGBPZJMMjnwlpWVadWqVRo2bJj7mNVq1bBhw7RixYo6PcesWbN0+eWXKyKi8qeX0tLKn2ZCQ0OrPafdbtfy5ctrfI5p06YpJibG/ZWcnNzQtwQAABqJPt7GY0vh6kwNvDk5OXI4HEpISKh2PCEhQVlZWcd8/MqVK7VmzRrdeOON7mPdunVT+/btNWXKFB04cEBlZWX697//rZ07d2r37t01Ps+UKVOUm5vr/tqxY0fj3hgAAGgwVx8vK7wN57phrQMzeCV5QUtDY8yaNUu9e/dWv3793MeCg4M1f/58bdq0SXFxcQoPD9fixYt1zjnnyGqt+e3a7XZFR0dX+wIAAOZwrfCuYYW3wVjhrc7UwBsfHy+bzabs7Oxqx7Ozs5WYmHjUxxYWFmru3Lm64YYbjvhe37599euvv+rgwYPavXu3FixYoH379qljx44erR8AAHiea4V3y54CFZc5TK7GN6XnMJLscKYG3pCQEPXt21eLFi1yH3M6nVq0aJEGDBhw1Me+9957Ki0t1dVXX13rOTExMWrVqpU2b96sn3/+WRdccIHHagcAAE2jdZRd8ZEhchrShizaGhoinRXeakxvaZg8ebJefvllzZkzR+vXr9ett96qwsJCXXfddZKkcePGacqUKUc8btasWRozZoxatmx5xPfee+89LVmyxD2abPjw4RozZoxGjBjR5O8HAAA0jsViUU92XGuwknKHdueWSGIkmUuQ2QVcdtll2rt3rx566CFlZWXphBNO0IIFC9w3sm3fvv2I3tuNGzdq+fLl+uqrr2p8zt27d2vy5MnKzs5WmzZtNG7cOD344INN/l4AAIBn9EyK1tJNe5nU0ADb91e2M0SHBimWkWSSJIthGIbZRXibvLw8xcTEKDc3lxvYAAAwwWe/79bEt1fr+HYx+mTSILPL8Slfrc3STW+s8vtrV5+8ZnpLAwAAwJ/1alsZYDbszle5w2lyNb6F/t0jEXgBAIDXSY4NV5Q9SGUOp7bsKTC7HJ/imsGbygxeNwIvAADwOlarRT1c83h30cdbH8zgPRKBFwAAeCUmNTSMawZvB2bwuhF4AQCAV3L18TKpoe5KKxzKzC2WxArv4Qi8AADAK7lWeNdl5snpZKhUXezYXyTDkCLtQWoZEWJ2OV6DwAsAALxSp1YRsgdZVVjmUEbVbFkcnaudIaVluCwWi8nVeA8CLwAA8EpBNqu6teHGtfpwjSRLjaed4XAEXgAA4LV6Jbn6eLlxrS4yGElWIwIvAADwWocmNbDCWxdsOlEzAi8AAPBaPQ9b4TUMblw7FndLA4G3GgIvAADwWl0To2SzWrS/sEy7c0vMLserlVU4tetA5UgyWhqqI/ACAACvFRpsU5fWkZLo4z2WnQeK5DSk8BCbWkXZzS7HqxB4AQCAV3P18TKp4ehcN6yltIxgJNmfEHgBAIBX68mkhjpJy3H179LO8GcEXgAA4NV6tWVSQ11kMKGhVgReAADg1bq3iZIk7c4t0b6CUpOr8V7pVS0NHeJZ4f0zAi8AAPBqUaHB6lC1cxhtDbVjhbd2BF4AAOD1etDHe1TlDqd2ukeSEXj/jMALAAC8Xi92XDuqXQeKVeE0FBpsVWtGkh2BwAsAALwekxqOzr2lcFyErFZGkv0ZgRcAAHg9V+BNyylUfkm5ydV4H9cM3lRuWKsRgRcAAHi9lpF2tYkJlSSt351vcjXex7XCS/9uzQi8AADAJxxqa6CP988O32UNRyLwAgAAn3Boi2H6eP8snV3WjorACwAAfAIrvDWrcDi140DVCm88K7w1IfACAACf4NpiePOeApWUO0yuxnvszi1RucNQSJBVbaJDzS7HKxF4AQCAT2gTE6rY8GA5nIY2ZXPjmsuhkWThjCSrBYEXAAD4BIvF4l7lpY/3EFf/Ljes1Y7ACwAAfEYP+niPkO6awcsNa7Ui8AIAAJ/h2mJ4DTuuuWW4ZvByw1qtCLwAAMBnuCY1bNidpwqH0+RqvMOhFV4Cb20IvAAAwGektoxQRIhNpRVObavqXQ1kDqeh7e5NJ2hpqA2BFwAA+Ayr1eLu412ziz7e3bnFKnM4FWyzKKlFmNnleC0CLwAA8CmuHdfW0sfr3lI4OS5cNkaS1YrACwAAfEpPVnjdXDN4O9C/e1QEXgAA4FNcK7zrMvPkdBomV2OuDHf/LoH3aAi8AADAp3RJiFSIzar80grtOFBkdjmmSstxjSTjhrWjIfACAACfEmyzqmtilCT6eF0zeFnhPToCLwAA8Dm92tLH63Qa7pYGeniPjsALAAB8Tg8mNSg7v0SlFU4FWS1KahFqdjlejcALAAB8Tq+qSQ1rM3NlGIF541p6zqGRZEE2It3RcHUAAIDP6ZYYLatFyiko0578UrPLMUW6u3+XG9aOhcALAAB8TliITZ1bR0oK3D5eV+BNpX/3mAi8AADAJwX6jmsZVS0NqazwHhOBFwAA+KSeh/XxBiJ3S0M8K7zHQuAFAAA+ybXCu2ZX4K3wGsahkWS0NBwbgRcAAPikHlUrvLsOFutgUZnJ1TSvPfmlKi53yGa1qG2LMLPL8XoEXgAA4JNiwoLVPq6yfzXQ+njTq7YUbtsiTCFBxLlj4QoBAACfFah9vO52Bvp364TACwAAfFavtoHZx3toJBkTGuqCwAsAAHxWjwBd4T206QQrvHVB4AUAAD6rV9Wkhm05hSosrTC5muaTzgzeeiHwAgAAn9Uqyq7WUXYZhrQhKzDaGipHklW1NNDDWycEXgAA4NMCrY83p6BMhWUOWS1Su1hGktUFgRcAAPi0QJvU4FrdTWoRJnuQzeRqfAOBFwAA+DTXjmuBMos3Lcc1oYF2hroi8AIAAJ/mWuHdlJ2vsgqnydU0PdcM3hRuWKszAi8AAPBp7WLDFBMWrHKHoU3Z+WaX0+RcI8k6cMNanRF4AQCAT7NYLAHVx3tohZfAW1cEXgAA4PMOBV7/7uM1DINd1hqAwAsAAHzeodFk/r3Cu7+wTPklFbJYpOQ4Am9dEXgBAIDPc63wrt+dL4fTMLmappNe1c6QFBOm0GBGktUVgRcAAPi8DvGRCgu2qbjcobScArPLaTKuGbxMaKgfAi8AAPB5NqtF3dtESfLvPt50blhrEAIvAADwC4HQx5ueww1rDUHgBQAAfiEQJjUcamlghbc+CLwAAMAvuLYYXrMrV4bhnzeuuVoa2HSifgi8AADALxyXEKVgm0V5JRXaeaDY7HI87mBRmXKLyyVJ7RlJVi8EXgAA4BdCgqzq0tp/b1xzre4mRocqLISRZPVB4AUAAH6jV1v/3WLYdcMaI8nqj8ALAAD8hquP1z9XeCsDL/279UfgBQAAfsO1wuuPo8kymMHbYAReAADgN7olRstikfbkl2pPfonZ5XiUa4WXGbz1R+AFAAB+I8IepI5Vv/L3t7YGVngbjsALAAD8iquPd50fBd7c4nLtLyyTxE1rDUHgBQAAfsUf+3hdO6y1jrIrwh5kcjW+h8ALAAD8ij9OanDN4E2lnaFBCLwAAMCv9EyqXOHdvr/IvTOZr8tgBm+jEHgBAIBfaREeorYtwiT5Tx9vmmtCAzN4G4TACwAA/I6/7bh2aEIDK7wNQeAFAAB+x9/6eDPcM3hZ4W0IAi8AAPA7rj5ef1jhzS8pV04BI8kag8ALAAD8Tq+2lSu8W/YUqLjMYXI1jeNqZ4iPDFFUaLDJ1fgmAi8AAPA7raPsio8MkdOQNmT5dluDa0thdlhrOAIvAADwOxaLxd3Hu8bH+3gzmMHbaAReAADgl1x9vOt8vI83Pcd1wxr9uw1F4AUAAH7J1ce7Zpd/rPCmMIO3wQi8AADAL7lWeDdm5avc4TS5moZzbzrBCm+DEXgBAIBfah8XrqjQIJU5nNqcXWB2OQ1SWFqhvfmlkrhprTEIvAAAwC9ZLBb1aOPb83hd7QxxESGKCWMkWUMReAEAgN9y9fH66o5rGe6RZLQzNAaBFwAA+C1f33EtnZFkHkHgBQAAfss1i3ddZp6cTsPkaurPNZKMFd7GIfACAAC/1alVhOxBVhWWOdw7lvkSV80dGEnWKAReAADgt4JsVnVz37jme3287hm8tDQ0CoEXAAD4tV5VfbxrfKyPt7jMoay8EknM4G0sAi8AAPBrh/fx+pLt+ytXd2PCgtUiPMTkanybVwTeGTNmKDU1VaGhoerfv79WrlxZ67lDhw6VxWI54mvUqFHucwoKCjRp0iS1a9dOYWFh6tGjh2bOnNkcbwUAAHiZXm2rVnh35cowfOfGtbQcdljzFNMD77x58zR58mRNnTpVq1evVp8+fTRy5Ejt2bOnxvPnz5+v3bt3u7/WrFkjm82mSy65xH3O5MmTtWDBAr355ptav3697rrrLk2aNEmffPJJc70tAADgJY5LiJLNatGBonLtzi0xu5w6c83gTeWGtUYzPfBOnz5dEyZM0HXXXedeiQ0PD9err75a4/lxcXFKTEx0fy1cuFDh4eHVAu/333+v8ePHa+jQoUpNTdVNN92kPn361LpyXFpaqry8vGpfAADAP4QG29SldaSkylVeX5HODWseY2rgLSsr06pVqzRs2DD3MavVqmHDhmnFihV1eo5Zs2bp8ssvV0TEoQ/DwIED9cknn2jXrl0yDEOLFy/Wpk2bNGLEiBqfY9q0aYqJiXF/JScnN+6NAQAAr+Lq4/WlSQ3uFV5aGhrN1MCbk5Mjh8OhhISEascTEhKUlZV1zMevXLlSa9as0Y033ljt+LPPPqsePXqoXbt2CgkJ0dlnn60ZM2ZoyJAhNT7PlClTlJub6/7asWNHw98UAADwOq4+Xl/ace3QphOs8DZWkNkFNMasWbPUu3dv9evXr9rxZ599Vj/88IM++eQTpaSkaNmyZZo4caKSkpKqrSa72O122e325iobAAA0M19b4S0pdygzl5FknmJq4I2Pj5fNZlN2dna149nZ2UpMTDzqYwsLCzV37lw98sgj1Y4XFxfr/vvv14cffuie3HD88cfr119/1ZNPPllj4AUAAP6tR9Us3t25JdpXUKqWkd690LWjaiRZVGiQ4iIYSdZYprY0hISEqG/fvlq0aJH7mNPp1KJFizRgwICjPva9995TaWmprr766mrHy8vLVV5eLqu1+luz2WxyOp2eKx4AAPiMSHuQe3teX1jldd2wltoyQhaLxeRqfJ/pUxomT56sl19+WXPmzNH69et16623qrCwUNddd50kady4cZoyZcoRj5s1a5bGjBmjli1bVjseHR2t008/Xffee6+WLFmitLQ0zZ49W6+//rrGjh3bLO8JAAB4nx4+tOOa64a1FNoZPML0Ht7LLrtMe/fu1UMPPaSsrCydcMIJWrBggftGtu3btx+xWrtx40YtX75cX331VY3POXfuXE2ZMkVXXXWV9u/fr5SUFD322GO65ZZbmvz9AAAA79QrKUaf/b7bJ1Z4D206wQ1rnmB64JWkSZMmadKkSTV+b8mSJUcc69q161F3SklMTNRrr73mqfIAAIAf6Fm1wusLWwxnuFoa2HTCI0xvaQAAAGgOrsCbllOo/JJyk6s5unRm8HoUgRcAAASElpF2tYkJlSSt351vcjW1K61wKPNgsSRm8HoKgRcAAAQM1zxeb95ieOeBYjkNKSLEpvhIRpJ5AoEXAAAEDFdbgzffuHb4DmuMJPMMAi8AAAgYvdq6dlzz3hVe1wzeDtyw5jEEXgAAEDBcK7yb9xSopNxhcjU1Ywav5xF4AQBAwGgTE6q4iBA5nIY2ZnnnjWuH77IGzyDwAgCAgGGxWLy+j/dQDy8rvJ5C4AUAAAHFm7cYLqtwaucBeng9jcALAAACSq8k141r3rfCu+tg5UiysGCbWkXZzS7HbxB4AQBAQHG1NGzYnacKh9PkaqpLP+yGNUaSeQ6BFwAABJTUlhGKCLGptMKprXsLzS6nmowc15bCtDN4EoEXAAAEFKvV4u7j9bZ5vK4JDSnx3LDmSQReAAAQcA5tMexdfbyuloYOrPB6FIEXAAAEnJ5eusKb4VrhJfB6FIEXAAAEHNcWw+sy8+R0GiZXU6nC4dSO/VWbTtDS4FEEXgAAEHA6t45USJBV+aUV2lE199Zsuw4Wq8JpyB5kVUJUqNnl+BUCLwAACDjBNqu6JUZJ8p4+XvcNay3DZbUyksyTCLwAACAgeVsfb8Y+RpI1FQIvAAAISO5JDV6y41p6jqt/l8DraQReAAAQkNwrvLtyZRjm37iWcdgua/AsAi8AAAhI3RKjZbVI+wrLlJ1XanY5SqOlockQeAEAQEAKC7Gpc+tISeb38TqcxmEjyQi8nkbgBQAAAcvVx7vW5D7ezIPFKncYCgmyqk00I8k8jcALAAAClquPd80uc1d4XTustY9jJFlTIPACAICA5S0rvOnu/l1uWGsKBF4AABCwelSt8O46WKwDhWWm1ZGe45rQQP9uUyDwAgCAgBUTFqz2cZWrqut2m7fK69pljRvWmgaBFwAABLRebc3v482gpaFJEXgBAEBAM7uP1+k0lOEaSUZLQ5Mg8AIAgIDmntRg0ize3XklKqtwKthmUZsYRpI1BQIvAAAIaK4V3rScQhWWVjT762dU3bCWHBeuIBvRrClwVQEAQEBrFWVX6yi7DENab8KNa+4b1mhnaDIEXgAAEPB6tTWvj9d1w1oKN6w1GQIvAAAIeGbuuHZo0wlWeJsKgRcAAAQ8Myc1pOdUtjSwwtt0CLwAACDguVZ4N+/JV2mFo9let3IkWeUKbwc2nWgyBF4AABDw2sWGKSYsWOUOQ5uzC5rtdffkl6qk3Kkgq0VtW4Q12+sGGgIvAAAIeBaLxb3Ku7YZ5/G6+nfbxYYxkqwJcWUBAAB0aFLDml3N18d7aEID7QxNicALAAAgmbLCm5bjmsHLDWtNicALAACgQ5Ma1u/Ol8NpNMtrulZ4U7lhrUkReAEAAFQ5JSEs2KbicofScprnxjV2WWseBF4AAABJNqtF3dtESWqePl7DMNhlrZkQeAEAAKoc2mK46ft49+aXqqjMIatFahdL4G1KBF4AAIAqh7YYbvoVXlc7Q7vYcIUEEcmaElcXAACgyqEthnNlGE1741o67QzNhsALAABQ5biEKAXbLMorqdDOA8VN+lruCQ3csNbkCLwAAABVQoKsOi6h8sa1pu7jdbU0sMLb9Ai8AAAAhzm0AUXT9vGm57DC21wIvAAAAIc5tMVw063wVo4kq5rBy6YTTY7ACwAAcJjmWOHdV1imgtIKWSxSclxYk70OKhF4AQAADtO9TbQsFmlPfqn25Jc0yWu4blhLigmTPcjWJK+BQwi8AAAAhwkPCVLHqjaDplrlTc9xtTNww1pzIPACAAD8iXvHtSbq401nJFmzIvACAAD8SVP38bpGkhF4mweBFwAA4E9cO66taaJZvBnsstasCLwAAAB/4lrh3bG/WLlF5R59bsMwlOaawctIsmZB4AUAAPiTFuEhatuiclzY2t2eXeU9UFSu/JIKSVL7OFZ4mwOBFwAAoAa92lau8q7zcB9vunskWahCgxlJ1hwIvAAAADVw9/F6eFLDof5d2hmaC4EXAACgBq4VXk9PamAGb/Mj8AIAANTAtcK7dW+BisscHnteVnibH4EXAACgBq2j7IqPtMtpSOuzPLfKm+aewcsKb3Mh8AIAANTAYrE0yQYUrhVeRpI1HwIvAABALdx9vB66ce1gUZkOVs31ZSRZ8yHwAgAA1MLVx+upFd6MqnaGhGi7wkOCPPKcODYCLwAAQC1cLQ0bs/JV7nA2+vnSuWHNFAReAACAWrSPC1dUaJDKHE5tzi5o9PO5RpJ1IPA2KwIvAABALSwWi3q0qVzlXZPZ+D5e90gyZvA2KwIvAADAUfRqW9nH64kthl0tDams8DYrAi8AAMBRuPp4PbHFsOumtRRm8DYrAi8AAMBRuFd4d+fJ6TQa/Dx5JeXaV1gmiZvWmhuBFwAA4Cg6xkfIHmRVUZlDaVUtCQ2RUXXDWqsouyLtjCRrTgReAACAowiyWdW9TeN3XDvUv0s7Q3Mj8AIAABzDoS2GG97Hm8EMXtMQeAEAAI7B1ce7dldjVngrWxpY4W1+BF4AAIBjOHyF1zAaduNaek5VS0M8K7zNjcALAABwDMclRCnIatGBonJl5pY06DkOrfASeJsbgRcAAOAYQoNt6tw6UpK0tgHzeAtKK5RTUCpJak9LQ7Mj8AIAANRBz6TKPt41DZjU4LphrWVEiKJDgz1aF46NwAsAAFAHvdpW9vGua8CkhvQcdlgzE4EXAACgDtwrvA2Y1OCewcsNa6Yg8AIAANRBj6pJDVl5Je5+3LrKcG86QeA1A4EXAACgDiLtQepQtUJb3x3XXBMaaGkwB4EXAACgjhq64xorvOYi8AIAANSRq4+3PjuuFZVVKDuvsgWCwGsOAi8AAEAduSY11GeFN6OqnSE2PFgx4YwkMwOBFwAAoI5cK7zp+4qUX1Jep8e42hlSWN01DYEXAACgjuIiQpQUEypJWlfHG9cObSnMDWtmIfACAADUQw9XH28dAy8rvOYj8AIAANSDa1LDmjr28ablVAbeDmw6YRoCLwAAQD30alu5wlvXloYMZvCajsALAABQD64V3s17ClRS7jjquSXlDu3OLZHESDIzEXgBAADqoU1MqOIiQuRwGtqYlX/Uc7fvr1zdjQ4NUgtGkpmGwAsAAFAPFoulzn28rv7d1PgIWSyWJq8NNSPwAgAA1FPPOk5qYEth7+AVgXfGjBlKTU1VaGio+vfvr5UrV9Z67tChQ2WxWI74GjVqlPucmr5vsVj0n//8pzneDgAA8HOuFd61u46+wssMXu9geuCdN2+eJk+erKlTp2r16tXq06ePRo4cqT179tR4/vz587V7927315o1a2Sz2XTJJZe4zzn8+7t379arr74qi8Wiiy66qLneFgAA8GOuSQ0bsvJV4XDWeh4zeL2D6YF3+vTpmjBhgq677jr16NFDM2fOVHh4uF599dUaz4+Li1NiYqL7a+HChQoPD68WeA//fmJioj7++GOdccYZ6tixY3O9LQAA4MdS4sIVaQ9SaYVTW/cW1npeek7VCm88K7xmMjXwlpWVadWqVRo2bJj7mNVq1bBhw7RixYo6PcesWbN0+eWXKyKi5p+csrOz9dlnn+mGG26o9TlKS0uVl5dX7QsAAKA2VqtFPdpU3bhWS1tDSblDmbnFkujhNZupgTcnJ0cOh0MJCQnVjickJCgrK+uYj1+5cqXWrFmjG2+8sdZz5syZo6ioKF144YW1njNt2jTFxMS4v5KTk+v+JgAAQEDq4erjreXGtZ0HimQYUpQ9SHERIc1ZGv7E9JaGxpg1a5Z69+6tfv361XrOq6++qquuukqhoaG1njNlyhTl5ua6v3bs2NEU5QIAAD/i6uOtbTSZq50hJT6ckWQmCzLzxePj42Wz2ZSdnV3teHZ2thITE4/62MLCQs2dO1ePPPJIred8++232rhxo+bNm3fU57Lb7bLb7XUvHAAABDzXpIb1mXlyOg1ZrdVDbTo3rHkNU1d4Q0JC1LdvXy1atMh9zOl0atGiRRowYMBRH/vee++ptLRUV199da3nzJo1S3379lWfPn08VjMAAIAkdW4dqZAgq/JLK9w7qh0u3T2DlxvWzGZ6S8PkyZP18ssva86cOVq/fr1uvfVWFRYW6rrrrpMkjRs3TlOmTDnicbNmzdKYMWPUsmXLGp83Ly9P77333lH7ewEAABoq2GZVt8QoSTX38Wa4Z/Cywms2U1saJOmyyy7T3r179dBDDykrK0snnHCCFixY4L6Rbfv27bJaq+fyjRs3avny5frqq69qfd65c+fKMAxdccUVTVo/AAAIXD2TovX7zlytyczVqOPbVPuee4U3nsBrNothGIbZRXibvLw8xcTEKDc3V9HR0WaXAwAAvNSbP2TogY/WaMhxrfT69Yduoi+rcKrbg1/IaUgr/36WWkfVfvM8GqY+ec30lgYAAABfdfgWw4evIe48UCSnIYWH2NQqkhvjzUbgBQAAaKDubaJls1q0r7BM2Xml7uOHT2hgJJn5CLwAAAANFBpsU6dWlT26h++45prB24Ethb0CgRcAAKAReiVVbkBx+KSGDGbwehUCLwAAQCMc2mL4sBVe90gyVni9AYEXAACgEVxbDLPC670IvAAAAI3gWuHddbBYBwrLVO5waseBYklSB2bwegUCLwAAQCNEhwarfVxl68LazDztOlAsh9NQaLBVraMYSeYNCLwAAACN1KvtoT5e9w5rjCTzGgReAACARupZNalhTWaeMqpuWEvhhjWvEWR2AQAAAL6u52GTGlpGhEiqXOGFdyDwAgAANJJrhTctp1DxVVsJp3LDmtegpQEAAKCRWkXZlRBtl2FIP6fvl0RLgzch8AIAAHiAa5XXaVT+mZYG70HgBQAA8IBeVX28kmQPsioxOtTEanA4Ai8AAIAH9Kha4ZUq2xmsVkaSeQsCLwAAgAe4ZvFKbCnsbQi8AAAAHtC2RZhiwoIlSancsOZVCLwAAAAeYLFYdHy7yraGjq0iTa4Gh2MOLwAAgIfcd043Hd9ut0b3STK7FByGwAsAAOAhPZNi3OPJ4D1oaQAAAIBfI/ACAADArxF4AQAA4NcIvAAAAPBrBF4AAAD4NQIvAAAA/BqBFwAAAH6NwAsAAAC/RuAFAACAXyPwAgAAwK8ReAEAAODXCLwAAADwawReAAAA+DUCLwAAAPwagRcAAAB+jcALAAAAv0bgBQAAgF8j8AIAAMCvEXgBAADg1wi8AAAA8GsEXgAAAPg1Ai8AAAD8GoEXAAAAfo3ACwAAAL9G4AUAAIBfCzK7AG9kGIYkKS8vz+RKAAAAUBNXTnPltqMh8NYgPz9fkpScnGxyJQAAADia/Px8xcTEHPUci1GXWBxgnE6nMjMzFRUVJYvF0mSvk5eXp+TkZO3YsUPR0dFN9jq+hutSO65N7bg2tePa1I5rUzOuS+24NrVr7mtjGIby8/OVlJQkq/XoXbqs8NbAarWqXbt2zfZ60dHR/ENTA65L7bg2tePa1I5rUzuuTc24LrXj2tSuOa/NsVZ2XbhpDQAAAH6NwAsAAAC/RuA1kd1u19SpU2W3280uxatwXWrHtakd16Z2XJvacW1qxnWpHdemdt58bbhpDQAAAH6NFV4AAAD4NQIvAAAA/BqBFwAAAH6NwAsAAAC/RuD1oBkzZig1NVWhoaHq37+/Vq5cedTz33vvPXXr1k2hoaHq3bu3Pv/882rfNwxDDz30kNq0aaOwsDANGzZMmzdvbsq30GTqc21efvllDR48WLGxsYqNjdWwYcOOOP/aa6+VxWKp9nX22Wc39dtoEvW5NrNnzz7ifYeGhlY7x18+N/W5LkOHDj3iulgsFo0aNcp9jr98ZpYtW6bRo0crKSlJFotFH3300TEfs2TJEp100kmy2+3q3LmzZs+efcQ59f33lzeq77WZP3++hg8frlatWik6OloDBgzQl19+We2chx9++IjPTbdu3ZrwXTSN+l6bJUuW1PjPVFZWVrXzfP1zU9/rUtO/RywWi3r27Ok+x18+M9OmTdMpp5yiqKgotW7dWmPGjNHGjRuP+ThvzTYEXg+ZN2+eJk+erKlTp2r16tXq06ePRo4cqT179tR4/vfff68rrrhCN9xwg3755ReNGTNGY8aM0Zo1a9znPPHEE3rmmWc0c+ZM/fjjj4qIiNDIkSNVUlLSXG/LI+p7bZYsWaIrrrhCixcv1ooVK5ScnKwRI0Zo165d1c47++yztXv3bvfXO++80xxvx6Pqe22kyh1sDn/fGRkZ1b7vD5+b+l6X+fPnV7sma9askc1m0yWXXFLtPH/4zBQWFqpPnz6aMWNGnc5PS0vTqFGjdMYZZ+jXX3/VXXfdpRtvvLFasGvI59Ab1ffaLFu2TMOHD9fnn3+uVatW6YwzztDo0aP1yy+/VDuvZ8+e1T43y5cvb4rym1R9r43Lxo0bq7331q1bu7/nD5+b+l6X//3vf9Wux44dOxQXF3fEv2v84TOzdOlSTZw4UT/88IMWLlyo8vJyjRgxQoWFhbU+xquzjQGP6NevnzFx4kT3nx0Oh5GUlGRMmzatxvMvvfRSY9SoUdWO9e/f37j55psNwzAMp9NpJCYmGv/5z3/c3z948KBht9uNd955pwneQdOp77X5s4qKCiMqKsqYM2eO+9j48eONCy64wNOlNrv6XpvXXnvNiImJqfX5/OVz09jPzH//+18jKirKKCgocB/zl8/M4SQZH3744VHP+etf/2r07Nmz2rHLLrvMGDlypPvPjb3e3qgu16YmPXr0MP7xj3+4/zx16lSjT58+nivMC9Tl2ixevNiQZBw4cKDWc/ztc9OQz8yHH35oWCwWIz093X3MHz8zhmEYe/bsMSQZS5curfUcb842rPB6QFlZmVatWqVhw4a5j1mtVg0bNkwrVqyo8TErVqyodr4kjRw50n1+WlqasrKyqp0TExOj/v371/qc3qgh1+bPioqKVF5erri4uGrHlyxZotatW6tr16669dZbtW/fPo/W3tQaem0KCgqUkpKi5ORkXXDBBVq7dq37e/7wufHEZ2bWrFm6/PLLFRERUe24r39mGuJY/67xxPX2F06nU/n5+Uf8u2bz5s1KSkpSx44dddVVV2n79u0mVdj8TjjhBLVp00bDhw/Xd9995z7O56bSrFmzNGzYMKWkpFQ77o+fmdzcXEk64p+Pw3lztiHwekBOTo4cDocSEhKqHU9ISDii38klKyvrqOe7/rc+z+mNGnJt/uxvf/ubkpKSqv0DcvbZZ+v111/XokWL9O9//1tLly7VOeecI4fD4dH6m1JDrk3Xrl316quv6uOPP9abb74pp9OpgQMHaufOnZL843PT2M/MypUrtWbNGt14443VjvvDZ6Yhavt3TV5enoqLiz3yz6i/ePLJJ1VQUKBLL73Ufax///6aPXu2FixYoBdeeEFpaWkaPHiw8vPzTay06bVp00YzZ87UBx98oA8++EDJyckaOnSoVq9eLckz/273dZmZmfriiy+O+HeNP35mnE6n7rrrLp122mnq1atXred5c7YJatJnBxrpX//6l+bOnaslS5ZUuznr8ssvd///3r176/jjj1enTp20ZMkSnXXWWWaU2iwGDBigAQMGuP88cOBAde/eXS+++KIeffRREyvzHrNmzVLv3r3Vr1+/ascD9TODunn77bf1j3/8Qx9//HG1PtVzzjnH/f+PP/549e/fXykpKXr33Xd1ww03mFFqs+jatau6du3q/vPAgQO1detW/fe//9Ubb7xhYmXeY86cOWrRooXGjBlT7bg/fmYmTpyoNWvW+GQvsgsrvB4QHx8vm82m7Ozsasezs7OVmJhY42MSExOPer7rf+vznN6oIdfG5cknn9S//vUvffXVVzr++OOPem7Hjh0VHx+vLVu2NLrm5vL/7d1/TNT1Hwfw5wV3cgcYKYQMA68CKkbJj8lkFWy4YLl1YlNq8qM0cCXL26BmlmFrlU1NXWM1/1CoERf2yy02SX5cucsE7YoriV+RBVEslgsSDrx7ff/w62ee/OZI5Hw+ttvY5/P6vD+v92vvO15++HxOd2pzmVqtRmxsrDJvT1g37tTl33//hclkmtIvlfm4ZmZivM+ahQsXQqvVzso6nO9MJhOeeuopVFZWjvpz7NUCAgIQGRnp8etmLCtWrFDmfaOvGxHBoUOHkJ2dDY1GM2HsfF8zBQUF+Pzzz1FfX4+lS5dOGHs99zZseGeBRqNBfHw8amtrlW1OpxO1tbUuV+OutHLlSpd4ADh+/LgSr9frsWTJEpeYf/75B6dOnRp3zOvRTGoDXHqK89VXX8WxY8eQkJAw6Xm6urrQ19eHkJCQWcn7Wphpba7kcDhgs9mUeXvCunGnLkeOHIHdbkdWVtak55mPa2YmJvusmY11OJ9VVFTgySefREVFhcvX2I1nYGAAHR0dHr9uxvLdd98p877R182XX36J9vb2Kf3jer6uGRFBQUEBPv30U9TV1UGv1096zHXd2/ynj8TdQEwmkyxYsEBKS0vl7Nmzkp+fLwEBAfLHH3+IiEh2drZs27ZNibdYLOLt7S179uyR5uZmKS4uFrVaLTabTYnZtWuXBAQEyNGjR6WpqUkMBoPo9XoZHBy85vNzx3Rrs2vXLtFoNPLRRx9JT0+P8urv7xcRkf7+fikqKpKTJ09KZ2en1NTUSFxcnERERMjQ0NCczHGmplubV155Raqrq6Wjo0POnDkjjz32mPj4+MiPP/6oxHjCupluXS67//77JTMzc9R2T1oz/f39YrVaxWq1CgB56623xGq1yrlz50REZNu2bZKdna3E//zzz6LT6eS5556T5uZmKSkpES8vLzl27JgSM1m954vp1qa8vFy8vb2lpKTE5bPm/PnzSkxhYaGYzWbp7OwUi8Uiq1atksDAQOnt7b3m83PHdGuzb98++eyzz6StrU1sNpts3bpVbrrpJqmpqVFiPGHdTLcul2VlZUliYuKYY3rKmnn66afl5ptvFrPZ7PL+uHDhghIzn3obNryz6O2335awsDDRaDSyYsUK+eabb5R9ycnJkpub6xJfWVkpkZGRotFoJDo6Wqqqqlz2O51O2bFjhwQHB8uCBQskNTVVWlparsVUZt10ahMeHi4ARr2Ki4tFROTChQvy0EMPSVBQkKjVagkPD5e8vLx59SF7penUxmg0KrHBwcHy8MMPy7fffusynqesm+m+n3766ScBIF988cWosTxpzVz+uqirX5frkZubK8nJyaOOWb58uWg0Grn99tvl8OHDo8adqN7zxXRrk5ycPGG8yKWvcAsJCRGNRiOhoaGSmZkp7e3t13Zis2C6tXnzzTfljjvuEB8fH1m0aJGkpKRIXV3dqHHn+7qZyfvp/PnzotVq5eDBg2OO6SlrZqy6AHD5/JhPvY3q/5MiIiIiIvJIvIeXiIiIiDwaG14iIiIi8mhseImIiIjIo7HhJSIiIiKPxoaXiIiIiDwaG14iIiIi8mhseImIiIjIo7HhJSIiIiKPxoaXiGgGUlJSYDQa5zoN7Ny5E8uXL5/rNIiIrmtseImI5rGioiLU1tbOdRpT8sQTT2DNmjVznQYR3YDY8BIRXYeGh4enFOfn54fFixf/x9lMbGRkZE7PT0Q0GTa8RERustvtKCoqQmhoKHx9fZGYmAiz2azs7+vrw+OPP47Q0FDodDrExMSgoqLCZYyUlBQUFBTAaDQiMDAQaWlpMJvNUKlUqK2tRUJCAnQ6HZKSktDS0qIcd/UtDZevou7ZswchISFYvHgxtmzZ4tKU9vT0YPXq1dBqtdDr9fjggw+wbNky7N+/f0rzValUeOedd/DII4/A19cXr732GhwOBzZt2gS9Xg+tVouoqCgcOHDAJc+ysjIcPXoUKpUKKpVKqdFvv/2G9evXIyAgAIsWLYLBYMAvv/wy5foTEU2GDS8RkZsKCgpw8uRJmEwmNDU1Yd26dUhPT0dbWxsAYGhoCPHx8aiqqsIPP/yA/Px8ZGdno6GhwWWcsrIyaDQaWCwWvPvuu8r2F198EXv37sXp06fh7e2NjRs3TphPfX09Ojo6UF9fj7KyMpSWlqK0tFTZn5OTg99//x1msxkff/wxDh48iN7e3mnNeefOncjIyIDNZsPGjRvhdDqxdOlSHDlyBGfPnsXLL7+M7du3o7KyEsClWy/Wr1+P9PR09PT0oKenB0lJSRgZGUFaWhr8/f1x4sQJWCwW+Pn5IT09fcpXuYmIJiVERDRtycnJsnXrVjl37px4eXlJd3e3y/7U1FR54YUXxj1+9erVUlhY6DJebGysS0x9fb0AkJqaGmVbVVWVAJDBwUERESkuLpb77rtP2Z+bmyvh4eFy8eJFZdu6deskMzNTRESam5sFgDQ2Nir729raBIDs27dvSnMHIEajcdK4LVu2yKOPPuqSm8FgcIl5//33JSoqSpxOp7LNbreLVquV6urqKeVDRDQZ7znttomI5jmbzQaHw4HIyEiX7Xa7Xbm31uFw4PXXX0dlZSW6u7sxPDwMu90OnU7nckx8fPyY57j33nuVn0NCQgAAvb29CAsLGzM+OjoaXl5eLsfYbDYAQEtLC7y9vREXF6fsv/POO3HLLbdMdcoAgISEhFHbSkpKcOjQIfz6668YHBzE8PDwpN8g8f3336O9vR3+/v4u24eGhtDR0TGtnIiIxsOGl4jIDQMDA/Dy8sKZM2dcmkzg0gNlALB7924cOHAA+/fvR0xMDHx9fWE0Gkf9yd7X13fMc6jVauVnlUoFAHA6nePmdGX85WMmip+Jq3M1mUwoKirC3r17sXLlSvj7+2P37t04derUhOMMDAwgPj4e5eXlo/YFBQXNas5EdONiw0tE5IbY2Fg4HA709vbigQceGDPGYrHAYDAgKysLwKVmtbW1Fffcc8+1TBUAEBUVhYsXL8JqtSpXlNvb2/H333+7Na7FYkFSUhKeeeYZZdvVV2g1Gg0cDofLtri4OHz44Ye49dZbsXDhQrdyICIaDx9aIyJyQ2RkJDZs2ICcnBx88skn6OzsRENDA9544w1UVVUBACIiInD8+HF8/fXXaG5uxubNm/Hnn3/OSb533XUXVq1ahfz8fDQ0NMBqtSI/Px9arVa5ejwTEREROH36NKqrq9Ha2oodO3agsbHRJWbZsmVoampCS0sL/vrrL4yMjGDDhg0IDAyEwWDAiRMn0NnZCbPZjGeffRZdXV3uTpeICAAbXiIitx0+fBg5OTkoLCxEVFQU1qxZg8bGRuUe25deeglxcXFIS0tDSkoKlixZMqf/AcN7772H4OBgPPjgg8jIyEBeXh78/f3h4+Mz4zE3b96MtWvXIjMzE4mJiejr63O52gsAeXl5iIqKQkJCAoKCgmCxWKDT6fDVV18hLCwMa9euxd13341NmzZhaGiIV3yJaNaoRETmOgkiIpo7XV1duO2221BTU4PU1NS5ToeIaNax4SUiusHU1dVhYGAAMTEx6OnpwfPPP4/u7m60traOeuCNiMgT8JYGIqIbzMjICLZv347o6GhkZGQgKCgIZrMZarUa5eXl8PPzG/MVHR0916kTEc0Ir/ASEZGiv79/3Afq1Go1wsPDr3FGRETuY8NLRERERB6NtzQQERERkUdjw0tEREREHo0NLxERERF5NDa8REREROTR2PASERERkUdjw0tEREREHo0NLxERERF5tP8Bl543ig0qyUQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2, num=10)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "  \t# Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, verbose=1)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You can see that for low values, you get a pretty good accuracy. However once the learning rate pushes much above 1.5, the accuracy starts to drop. You have learned and practiced a useful skill for visualizing large amounts of results for a single hyperparameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
