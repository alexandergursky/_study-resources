{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "  \n",
    "This chapter introduces you to a popular automated hyperparameter tuning methodology called Grid Search. You will learn what it is, how it works and practice undertaking a Grid Search using Scikit-Learn. You will then learn how to analyze the output of a Grid Search & gain practical experience doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Scikit Learn Documentation](https://scikit-learn.org/stable/)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pandas.get_dummies</td>\n",
    "    <td>A function from the Pandas library used to perform one-hot encoding on categorical data, converting categorical variables into a binary matrix representation for machine learning.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>sklearn.model_selection.train_test_split</td>\n",
    "    <td>A function from scikit-learn used to split a dataset into training and testing subsets, enabling the assessment of machine learning models' performance on unseen data.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>sklearn.linear_model.LogisticRegression</td>\n",
    "    <td>A class from scikit-learn representing a logistic regression classifier used for binary classification tasks, modeling the probability of a binary outcome.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>sklearn.ensemble.RandomForestClassifier</td>\n",
    "    <td>A class from scikit-learn representing a random forest classifier, an ensemble learning method that combines multiple decision trees for improved predictive accuracy and generalization.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>sklearn.metrics.confusion_matrix</td>\n",
    "    <td>A function from scikit-learn used to compute a confusion matrix, providing insights into the performance of a classification model by detailing true positive, true negative, false positive, and false negative predictions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>sklearn.metrics.accuracy_score</td>\n",
    "    <td>A function from scikit-learn used to calculate the accuracy score, a common metric to evaluate the correctness of classification predictions by comparing them to the actual labels.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>sklearn.neighbors.KNeighborsClassifier</td>\n",
    "    <td>A class from scikit-learn representing a k-nearest neighbors classifier, a machine learning algorithm that assigns a label to a data point based on the majority class among its k-nearest neighbors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>sklearn.ensemble.GradientBoostingClassifier</td>\n",
    "    <td>A class from scikit-learn representing a gradient boosting classifier, an ensemble method that builds a predictive model through the iterative combination of weak learners, usually decision trees.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>numpy.linspace</td>\n",
    "    <td>A function from NumPy used to create an array of evenly spaced values over a specified range, facilitating the generation of input data for mathematical operations and visualization.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>plt.gca()</td>\n",
    "    <td>A function from Matplotlib used to get the current Axes instance within a plot, allowing for customization and fine-tuning of plot elements.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>sklearn.model_selection.GridSearchCV</td>\n",
    "    <td>A function from scikit-learn used for hyperparameter tuning through exhaustive search over a specified parameter grid, optimizing model performance.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>sklearn.metrics.roc_auc_score</td>\n",
    "    <td>A function from scikit-learn used to compute the area under the Receiver Operating Characteristic (ROC) curve, providing a measure of a classification model's ability to distinguish between classes.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: scikit-learn  \n",
    "Version: 1.3.0  \n",
    "Summary: A set of python modules for machine learning and data mining  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "from pprint import pprint           # Pretty print              Advanced print options\n",
    "\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Grid Search\n",
    "  \n",
    "In this section we will look at extending our work on automatic hyperparameter tuning and learn what a Grid Search is. Let's get started!\n",
    "  \n",
    "**Automating 2 Hyperparameters**\n",
    "  \n",
    "Let's remind ourselves of your previous work using a for loop to test different values of the number of neighbors in a KNN algorithm. We then collated those into a DataFrame to analyze. For this section we are working with a reduced dataset so you may see slightly different results.\n",
    "  \n",
    "But what if we want to test different values of 2 hyperparameters? Let us take the example of a GBM algorithm, which has a few more hyperparameters to tune than KNN or Random Forest algorithms. Let's' say we want to tune the two hyperparameters and values as follows. How would you do that? One suggestion could be a nested loop.\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search1.png' alt='img' width='740'></center>\n",
    "  \n",
    "We can first write nicer code by having the model creation component as a function. We feed in the two hyperparameter values as arguments and use these to create a model Then we fit to our data and generate predictions. Finally we return the hyperparameter values used and the score in a list for analysis.\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search2.png' alt='img' width='740'></center>\n",
    "  \n",
    "Now we can loop through and call our function, appending our results to a list as we go. We have a nested loop so we test all values of our first hyperparameter for all values of our second hyperparameter.\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search3.png' alt='img' width='740'></center>\n",
    "  \n",
    "We can save these results into a DataFrame as well And then print it out to view.\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**How many models?**\n",
    "  \n",
    "You will notice that many more models are built when adding more hyperparameters and values to test. Importantly, this relationship between models created and hyperparameters or values to test is not a linear relationship. For each of the values tested for the first hyperparameter, you test every value of the second hyperparameter. This means to test 5 values for the first hyperparameter and 10 values for the second hyperparameter, we have 50 models to run. And what if we k-fold cross-validated each model 10 times? That would be 500 models to run!\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**From 2 to N hyperparameters**\n",
    "  \n",
    "That was just for 2 hyperparameters. What if we wanted to test a third or fourth hyperparameter? We could nest again (and again) We first list the extra things to test.\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search6.png' alt='img' width='740'></center>\n",
    "  \n",
    "Then we adjust our function to take in more inputs. Notice how our function has a more complex model build but is very similar to what we did before?\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search7.png' alt='img' width='740'></center>\n",
    "  \n",
    "Finally, we can adjust our for loop to add the extra level of nesting. This code will also look familiar, we are just adding more levels of nesting but still saving out our results for analysis.\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search8.png' alt='img' width='740'></center>\n",
    "  \n",
    "So how many models did we just create? Testing 7 values for our first hyperparameter, and the listed number for the other hyperparameters, we can see this number has greatly increased. Safe to say we cannot keep nesting forever as our code becomes complex and inefficient. Plus, what if we also wanted some extra information on training and testing times and scores. Our code will get quite complex.\n",
    "  \n",
    "**Introducing Grid Search**\n",
    "  \n",
    "Let's review our work in an alternate way. If we created a grid with each value of `max_depth=` that we want to test down the left and each value of `learning_rate=` across the top. The intersection square of each of these is a model that we need to run.\n",
    "  \n",
    "Running a model for every cell in the grid with the hyperparameters specified is known as a Grid Search. For example, the mentioned cell here is equivalent to creating a gradient boosting estimator with these inputs.\n",
    "  \n",
    "<center><img src='../_images/introducing-grid-search9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Grid Search Pros & Cons**\n",
    "  \n",
    "Grid search has a number of advantages. It's programmatic, and saves many lines of code. It is guaranteed to find the best model within the grid you specify. But if you specify a poor grid with silly or conflicting values you won't get a good score! Finally, it is an easy methodology to explain compared to some of the more complex ones we will cover later in the course.\n",
    "  \n",
    "**Advantages of Grid Search**\n",
    "  \n",
    "- You don't have to write thousands of lines of code\n",
    "- Finds the best model within the grid (only within the defined grid)\n",
    "- Easy to explain\n",
    "\n",
    "However there are some disadvantages to this approach. It is computationally expensive. It is also 'uninformed' because it doesn't learn as it creates models the next model it creates could be better or worse. There are 'informed' methods that get better as they build more and more models and we will see those later in the course.\n",
    "  \n",
    "**Disadvantages of Grid Search**\n",
    "  \n",
    "- Computationally expensive\n",
    "- It is 'uninformed'. Results of one model don't help create the next model.\n",
    "- User defined\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Grid Search functions\n",
    "  \n",
    "In data science it is a great idea to try building algorithms, models and processes 'from scratch' so you can really understand what is happening at a deeper level. Of course there are great packages and libraries for this work (and we will get to that very soon!) but building from scratch will give you a great edge in your data science work.\n",
    "  \n",
    "In this exercise, you will create a function to take in 2 hyperparameters, build models and return results. You will use this function in a future exercise.\n",
    "  \n",
    "You will have available the `X_train`, `X_test`, `y_train` and `y_test` datasets available.\n",
    "  \n",
    "1. Build a function that takes two parameters called `learning_rate=` and `max_depth=` for the learning rate and maximum depth.\n",
    "2. Add capability in the function to build a GBM model and fit it to the data with the input hyperparameters.\n",
    "3. Have the function return the results of that model and the chosen hyperparameters (`learning_rate=` and `max_depth=`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 25)\n",
      "(30000, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "      <th>SEX_2</th>\n",
       "      <th>EDUCATION_1</th>\n",
       "      <th>EDUCATION_2</th>\n",
       "      <th>EDUCATION_3</th>\n",
       "      <th>EDUCATION_4</th>\n",
       "      <th>EDUCATION_5</th>\n",
       "      <th>EDUCATION_6</th>\n",
       "      <th>MARRIAGE_1</th>\n",
       "      <th>MARRIAGE_2</th>\n",
       "      <th>MARRIAGE_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>3913</td>\n",
       "      <td>3102</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2682</td>\n",
       "      <td>1725</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29239</td>\n",
       "      <td>14027</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46990</td>\n",
       "      <td>48233</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8617</td>\n",
       "      <td>5670</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  AGE  PAY_0  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6  BILL_AMT1  \\\n",
       "0   1      20000   24      2      2     -1     -1     -2     -2       3913   \n",
       "1   2     120000   26     -1      2      0      0      0      2       2682   \n",
       "2   3      90000   34      0      0      0      0      0      0      29239   \n",
       "3   4      50000   37      0      0      0      0      0      0      46990   \n",
       "4   5      50000   57     -1      0     -1      0      0      0       8617   \n",
       "\n",
       "   BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0       3102        689          0          0          0         0       689   \n",
       "1       1725       2682       3272       3455       3261         0      1000   \n",
       "2      14027      13559      14331      14948      15549      1518      1500   \n",
       "3      48233      49291      28314      28959      29547      2000      2019   \n",
       "4       5670      35835      20940      19146      19131      2000     36681   \n",
       "\n",
       "   PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  SEX_2  \\\n",
       "0         0         0         0         0                           1      1   \n",
       "1      1000      1000         0      2000                           1      1   \n",
       "2      1000      1000      1000      5000                           0      1   \n",
       "3      1200      1100      1069      1000                           0      1   \n",
       "4     10000      9000       689       679                           0      0   \n",
       "\n",
       "   EDUCATION_1  EDUCATION_2  EDUCATION_3  EDUCATION_4  EDUCATION_5  \\\n",
       "0            0            1            0            0            0   \n",
       "1            0            1            0            0            0   \n",
       "2            0            1            0            0            0   \n",
       "3            0            1            0            0            0   \n",
       "4            0            1            0            0            0   \n",
       "\n",
       "   EDUCATION_6  MARRIAGE_1  MARRIAGE_2  MARRIAGE_3  \n",
       "0            0           1           0           0  \n",
       "1            0           0           1           0  \n",
       "2            0           0           1           0  \n",
       "3            0           1           0           0  \n",
       "4            0           1           0           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import\n",
    "credit_card = pd.read_csv('../_datasets/credit-card-full.csv')\n",
    "print(credit_card.shape)\n",
    "\n",
    "# To change categorical variable with dummy variables\n",
    "credit_card = pd.get_dummies(credit_card, columns=['SEX', 'EDUCATION', 'MARRIAGE'], drop_first=True, dtype=int)\n",
    "print(credit_card.shape)\n",
    "\n",
    "# X/y split\n",
    "X = credit_card.drop(['ID', 'default payment next month'], axis=1)\n",
    "y = credit_card['default payment next month']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "\n",
    "credit_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create the function\n",
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "    # Create the model\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth)\n",
    "    \n",
    "    # Use the model to make predictions\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Return the hyperparameters and score\n",
    "    return ([learn_rate, max_depth, accuracy_score(y_test, predictions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You now have a function you can call to test different combinations of two hyperparameters for the GBM algorithm. In the next exercise we will use it to test some values and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively tune multiple hyperparameters\n",
    "  \n",
    "In this exercise, you will build on the function you previously created to take in 2 hyperparameters, build a model and return the results. You will now use that to loop through some values and then extend this function and loop with another hyperparameter.\n",
    "  \n",
    "The function `gbm_grid_search(learn_rate, max_depth)` is available in this exercise.\n",
    "  \n",
    "If you need to remind yourself of the function you can run the function `print_func()` that has been created for you.\n",
    "\n",
    "```python\n",
    "def print_func():\n",
    "    lines = inspect.getsource(gbm_grid_search)\n",
    "    print(lines)\n",
    "```\n",
    "  \n",
    "1. Write a for-loop to test the values (0.01, 0.1, 0.5) for the `learning_rate=` and (2, 4, 6) for the `max_depth=` using the function you created `gbm_grid_search` and print the results.\n",
    "2. Extend the `gbm_grid_search` function to include the hyperparameter `subsample`. Name this new function `gbm_grid_search_extended`.\n",
    "3. Extend your loop to call `gbm_grid_search` (available in your console), then test the values [0.4 , 0.6] for the `subsample` hyperparameter and print the results. `max_depth_list` & `learn_rate_list` are available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01, 2, 0.8203333333333334],\n",
      " [0.01, 4, 0.8206666666666667],\n",
      " [0.01, 6, 0.8166666666666667],\n",
      " [0.1, 2, 0.8237777777777778],\n",
      " [0.1, 4, 0.822],\n",
      " [0.1, 6, 0.8205555555555556],\n",
      " [0.5, 2, 0.82],\n",
      " [0.5, 4, 0.8072222222222222],\n",
      " [0.5, 6, 0.7882222222222223]]\n"
     ]
    }
   ],
   "source": [
    "# Create the relevant lists\n",
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2, 4, 6]\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate, max_depth))\n",
    "        \n",
    "# Print the results\n",
    "pprint(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the function input\n",
    "def gbm_grid_search_extended(learn_rate, max_depth, subsample):\n",
    "    # Extend the model creation section\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=learn_rate, \n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample\n",
    "        )\n",
    "    \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Extend the return part\n",
    "    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01, 2, 0.8203333333333334],\n",
      " [0.01, 4, 0.8206666666666667],\n",
      " [0.01, 6, 0.8166666666666667],\n",
      " [0.1, 2, 0.8237777777777778],\n",
      " [0.1, 4, 0.822],\n",
      " [0.1, 6, 0.8205555555555556],\n",
      " [0.5, 2, 0.82],\n",
      " [0.5, 4, 0.8072222222222222],\n",
      " [0.5, 6, 0.7882222222222223],\n",
      " [0.01, 2, 0.4, 0.8111111111111111],\n",
      " [0.01, 2, 0.6, 0.8206666666666667],\n",
      " [0.01, 4, 0.4, 0.8176666666666667],\n",
      " [0.01, 4, 0.6, 0.818],\n",
      " [0.01, 6, 0.4, 0.8172222222222222],\n",
      " [0.01, 6, 0.6, 0.8174444444444444],\n",
      " [0.1, 2, 0.4, 0.8242222222222222],\n",
      " [0.1, 2, 0.6, 0.8236666666666667],\n",
      " [0.1, 4, 0.4, 0.8231111111111111],\n",
      " [0.1, 4, 0.6, 0.8213333333333334],\n",
      " [0.1, 6, 0.4, 0.8206666666666667],\n",
      " [0.1, 6, 0.6, 0.8193333333333334],\n",
      " [0.5, 2, 0.4, 0.8172222222222222],\n",
      " [0.5, 2, 0.6, 0.8208888888888889],\n",
      " [0.5, 4, 0.4, 0.7942222222222223],\n",
      " [0.5, 4, 0.6, 0.7992222222222222],\n",
      " [0.5, 6, 0.4, 0.7868888888888889],\n",
      " [0.5, 6, 0.6, 0.7842222222222223]]\n"
     ]
    }
   ],
   "source": [
    "# Create the new list to test\n",
    "subsample_list = [0.4, 0.6]\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        # Extend the for loop\n",
    "        for subsample in subsample_list:\n",
    "            # Extend the results to include the new hyperparameter\n",
    "            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))\n",
    "\n",
    "\n",
    "# Print the results\n",
    "pprint(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations. You have effectively built your own grid search! You went from 2 to 3 hyperparameters and can see how you could extend that to even more values and hyperparameters. That was a lot of effort though. Be warned - we are now entering a world that can get very computationally expensive very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Models?\n",
    "  \n",
    "Adding more hyperparameters or values, you increase the amount of models created but the increases is not linear it is proportional to how many values and hyperparameters you already have.\n",
    "  \n",
    "How many models would be created when running a grid search over the following hyperparameters and values for a GBM algorithm?\n",
    "  \n",
    "```python\n",
    "learning_rate = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2]\n",
    "max_depth = [4,6,8,10,12,14,16,18, 20]\n",
    "subsample = [0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "```\n",
    "  \n",
    "These lists are in your console so you can utilize properties of them to help you!\n",
    "  \n",
    "Possible answers\n",
    "  \n",
    "- [ ] 26\n",
    "- [ ] 9 of one model, 9 of another\n",
    "- [ ] 1 large model\n",
    "- [x] 1215\n",
    "  \n",
    "Excellent! For every value of one hyperparameter, we test EVERY value of EVERY other hyperparameter. So you correctly multiplied the number of values (the lengths of the lists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Scikit Learn\n",
    "  \n",
    "In this lesson we will move beyond our manual code and leverage Scikit Learn to assist our grid search.\n",
    "  \n",
    "**GridSearchCV Object**\n",
    "  \n",
    "In this lesson we will be introduced to Scikit Learn's `GridSearchCV`. It will help us create a grid search more efficiently and get some performance analytics. This is an example of a `GridSearchCV` object. Don't worry, we will break it down!\n",
    "  \n",
    "<center><img src='../_images/grid-search-with-scikit-learn.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Steps in a Grid Search**\n",
    "  \n",
    "Firstly, let us conceptualize the steps needed to do a proper grid search. Some of these will be familiar from our manual work before. One Select an algorithm (or '`estimator=`') to tune. Two Define which hyperparameters we will tune. Three Define a range of values for each hyperparameter. Four Decide a cross-validation scheme. Five Define a scoring function to determine which model was the best. Six Include extra useful information or functions. The only one of these we did not do much work with previously is step (4), but we will cover each now.\n",
    "  \n",
    "1. An algorithm to tune the hyperparameters (or estimator)\n",
    "2. Defining which hyperparameters to tune\n",
    "3. Defining a range of values for each hyperparameter\n",
    "4. Setting a cross-validatoin scheme\n",
    "5. Defining a score function so we can decide which square on our grid was 'the best'\n",
    "6. Include extra useful information or functions\n",
    "  \n",
    "**GridSearchCV Object Inputs**\n",
    "  \n",
    "A GridSearchCV object takes several important arguments, `estimator=`, `param_grid`, `cv=`, `scoring=`, `refit=`, `n_jobs=`, `return_train_score=`.\n",
    "  \n",
    "**GridSearchCV 'estimator'**\n",
    "  \n",
    "The estimator is our algorithm. Examples include KNN, Random Forest, GBM or Logistic Regression. We only pick one algorithm for each grid search.\n",
    "  \n",
    "**GridSearchCV 'param_grid'**\n",
    "  \n",
    "`param_grid=` is how we tell `GridSearchCV` which hyperparameters and which values to test. We were previously using lists, but `param_grid=` needs a dictionary. The dictionary keys must be the hyperparameter names, the values a list of values to test.\n",
    "  \n",
    "**GridSearchCV 'param_grid'**\n",
    "  \n",
    "The keys in the `param_grid=` dictionary must be valid hyperparameters else the Grid Search will fail. See the example here, '`best_choice=`' is not a hyperparameter of Scikit Learn's Logistic Regression estimator and so this will fail.\n",
    "  \n",
    "**GridSearchCV 'cv'**\n",
    "  \n",
    "The `cv=` input allows you to undertake cross-validation. You could specify different cross-validation types here. But simply providing an integer will create a k-fold. You are likely familiar with standard 5 and 10 k-fold cross validation.\n",
    "  \n",
    "<center><img src='../_images/grid-search-with-scikit-learn1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**GridSearchCV 'scoring'**\n",
    "  \n",
    "`scoring=` is a scoring function used to evaluate your model's performance. You did this manually previously using accuracy. You can use your own custom metric, or one from the available metrics from Scikit Learn's metrics module. You can check all available metrics using this command.\n",
    "  \n",
    "<center><img src='../_images/grid-search-with-scikit-learn2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**GridSearchCV 'refit'**\n",
    "  \n",
    "`refit=` set to true means the best hyperparameter combinations are used to undertake a fitting to the training data. The `GridSearchCV` object can be used as an estimator directly This is very handy as you don't need to save our the best hyperparameters and train another model.\n",
    "  \n",
    "**GridSearchCV 'n_jobs'**\n",
    "  \n",
    "`n_jobs=` assists with parallel execution. You can effectively 'split up' your work and have many models being created at the same time. This is possible because the results of one model do not affect the next one. You can check how many cores you have available, which determines how many models you can run in parallel using this handy code. Be careful using all cores for a task though as this may mean you can't do other work on your computer while your models run.\n",
    "  \n",
    "<center><img src='../_images/grid-search-with-scikit-learn3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**GridSearchCV 'return_train_score'**\n",
    "  \n",
    "Finally `return_train_score` logs statistics about the training runs that were undertaken. This can be useful for plotting and understanding test vs training set performance (and hence bias-variance tradeoff). While informative, this is computationally expensive and will not assist in finding the best model.\n",
    "  \n",
    "**Building a GridSearchCV object**\n",
    "  \n",
    "Now we have all the components to build a grid search object. Firstly we create our parameter grid for the hyperparameters and values we want to input. Then we create the base classifier, setting some default values at the time of creation.\n",
    "  \n",
    "<center><img src='../_images/grid-search-with-scikit-learn4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Building a GridSearchCV Object**\n",
    "  \n",
    "We can now put the pieces together to create the `GridSearchCV` object. You can see all the elements you learned about previously including the estimator and parameter grid we just created. If this seems like a lot of code, review the couple of previous slides to see what each element means.\n",
    "  \n",
    "<center><img src='../_images/grid-search-with-scikit-learn5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Using a GridSearchCV Object**\n",
    "  \n",
    "With '`refit=`' set to `True`, we can directly use the `GridSearchCV` object as an estimator. That means we can fit onto our data and make predictions, just like any other Scikit Learn estimator!\n",
    "  \n",
    "<center><img src='../_images/grid-search-with-scikit-learn6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's undertake our own Grid Search with Scikit Learn's `GridSearchCV` module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV inputs\n",
    "  \n",
    "Let's test your knowledge of `GridSeachCV` inputs by answering the question below.\n",
    "\n",
    "Three `GridSearchCV` objects are available in the console, named `model_1`, `model_2`, `model_3`. Note that there is no data available to fit these models. Instead, you must answer by looking at their construct.\n",
    "\n",
    "Which of these `GridSearchCV` objects would not work when we try to fit it?\n",
    "  \n",
    "```python\n",
    "Model #1:\n",
    " GridSearchCV(\n",
    "    estimator = RandomForestClassifier(),\n",
    "    param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']},\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True) \n",
    "\n",
    "\n",
    "Model #2:\n",
    " GridSearchCV(\n",
    "    estimator = KNeighborsClassifier(),\n",
    "    param_grid = {'n_neighbors': [5, 10, 20], 'algorithm': ['ball_tree', 'brute']},\n",
    "    scoring='accuracy',\n",
    "    n_jobs=8,\n",
    "    cv=10,\n",
    "    refit=False) \n",
    "\n",
    "\n",
    "Model #3:\n",
    " GridSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    param_grid = {'number_attempts': [2, 4, 6], 'max_depth': [3, 6, 9, 12]},\n",
    "    scoring='accuracy',\n",
    "    n_jobs=2,\n",
    "    cv=7,\n",
    "    refit=True)\n",
    "```\n",
    "  \n",
    "Possible answers\n",
    "  \n",
    "- [ ] model_1 would not work when we try to fit it.\n",
    "- [ ] model_2 would not work when we try to fit it.\n",
    "- [x] model_3 would not work when we try to fit it.\n",
    "- [ ] None - they will all work when we try to fit them.\n",
    "  \n",
    "Correct! By looking at the Scikit Learn documentation (or your excellent memory!) you know that number_attempts is not a valid hyperparameter. This GridSearchCV will not fit to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV with Scikit Learn\n",
    "  \n",
    "The `GridSearchCV` module from Scikit Learn provides many useful features to assist with efficiently undertaking a grid search. You will now put your learning into practice by creating a `GridSearchCV` object with certain parameters.\n",
    "  \n",
    "The desired options are:\n",
    "  \n",
    "- A Random Forest Estimator, with the split `criterion=` as `'entropy'`\n",
    "- 5-fold cross validation\n",
    "- The hyperparameters `max_depth=` (2, 4, 8, 15) and `max_features=` (`'auto'` vs `'sqrt'`)\n",
    "- Use `roc_auc` to score the models\n",
    "- Use 4 cores for processing in parallel\n",
    "- Ensure you refit the best model and return training scores\n",
    "  \n",
    "You will have available `X_train`, `X_test`, `y_train` & `y_test` datasets.\n",
    "  \n",
    "1. Create a Random Forest estimator as specified in the context above.\n",
    "2. Create a parameter grid as specified in the context above.\n",
    "3. Create a `GridSearchCV` object as outlined in the context above, using the two elements created in the previous two instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, estimator=RandomForestClassifier(criterion='entropy'),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'max_depth': [2, 4, 8, 15],\n",
      "                         'max_features': ['auto', 'sqrt']},\n",
      "             return_train_score=True, scoring='roc_auc')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth' : [2, 4, 8, 15],\n",
    "    'max_features' : ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    refit=True, \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(grid_rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work! You now understand all the inputs to a `GridSearchCV` object and can tune many different hyperparameters and many different values for each on a chosen algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding a grid search output\n",
    "  \n",
    "Now that you know how to run a grid search, let's focus on its output.\n",
    "  \n",
    "**Analyzing the output**\n",
    "  \n",
    "Let us now analyze each of the properties of the `GridSearchCV` output and learn how to access and use them. The properties of the object can be categorized into three different groups a results log the best results and 'Extra information'.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Accessing object properties**\n",
    "  \n",
    "Properties are accessed using the dot notation, that is `grid_search_object.property`. Where property is the actual property you want to retrieve Let's review each of the key properties now.\n",
    "  \n",
    "**The .cv_results_ property**\n",
    "  \n",
    "Firstly there is the `.cv_results_` property. This is a dictionary that we can read into a `pandas` DataFrame to explore. Notice there are 12 rows because there are 12 squares in our grid. Each row tells you about what happened when testing that square.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The .cv_results_ 'time' columns**\n",
    "  \n",
    "The `'time'` columns refer to the time it took to fit and score the model. We did a cross-validation so this ran 5 times and stored the average and standard deviation of the times it took in seconds.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The .cv_results_ 'param_' columns**\n",
    "  \n",
    "The `param_` columns contain information on the different parameters that were used in the model. Remember, each row in this DataFrame is about one model. So we can see row 3 for example tested the hyperparameter combination of max_depth 10 and `min_samples_leaf=` 2 and `n_estimators=` 100 for our random forest estimator.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The .cv_results_ 'param' column**\n",
    "  \n",
    "The `params` column is a dictionary of all the parameters from the previous `'param'` columns. We need to use `pd.set_option` here to ensure we don't truncate the results we are printing.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The .cv_results_ 'test_score' columns**\n",
    "  \n",
    "The next 5 columns are the testing scores for each of the 5 cross-folds, or splits, we made, followed by the the mean and standard deviation for those cross-folds.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The .cv_results_ 'rank_test_score' column**\n",
    "  \n",
    "The rank column conveniently ranks the rows by the `mean_test_score`. We can see that the model in our third row had the best `mean_test_score`.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Extracting the best row**\n",
    "  \n",
    "Using the `rank_test_score` column we can easily select the grid search square for analysis. This table is the row from the `cv_results` object that was the best model created.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The .cv_results_ 'train_score' columns**\n",
    "  \n",
    "The `test_score` columns are then repeated for the training scores. Note that if we had not set `return_train_score` to `True` this would not include the training scores. There is also no ranking column for the training scores, as we only care about performance on the test set in each fold.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The best grid square**\n",
    "  \n",
    "Information on the best grid square is found in three different properties `.best_params_` which is the dictionary of the parameters that gave the best score. `.best_score_`, the actual best score and `best_index`, the row in our `.cv_results_` that was the best. This is same as the index of the row with rank 1 in `.cv_results_` that we extracted just before.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**The best_estimator_ property**\n",
    "  \n",
    "`GridSearchCV` stores an estimator built with the best hyperparameters in the `best_estimator` property. Since it is an estimator, we can use this to predict on our test set. We can demonstrate this by using python's `type()` function and see it is a Random Forest Classification estimator. We can also use the `GridSearchCV` object itself directly as an estimator.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output10.png' alt='img' width='740'></center>\n",
    "  \n",
    "We can print out and see the estimator itself. This is why we set `refit=True` when creating the grid search, otherwise we would need to refit using the best parameters ourself before using the best estimator.\n",
    "  \n",
    "<center><img src='../_images/understanding-a-grid-search-output11.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Extra information**\n",
    "  \n",
    "Some extra information can be obtained with the following properties. These are not very useful properties but may be important if you construct you grid search differently. These include the `.scorer_` function that was used and the number of cross validation splits, `.n_splits_` (both of which we set ourselves), and the `.refit_time_` which is the number of seconds used for refitting the best model on the whole dataset. This may be of interest in analyzing efficiencies in your work, but not for our use case here.\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's practice analyzing the output of a Scikit Learn `GridSearchCV` object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best outputs\n",
    "  \n",
    "Which of the following parameters must be set in order to be able to directly use the `.best_estimator_` property for predictions?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] `return_train_score = True`\n",
    "- [x] `refit = True`\n",
    "- [ ] `refit = False`\n",
    "- [ ] `verbose = 1`\n",
    "  \n",
    "Correct! When we set this to true, the creation of the grid search object automatically refits the best parameters on the whole training set and creates the `.best_estimator_` property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the grid search results\n",
    "  \n",
    "You will now explore the `.cv_results_` property of the `GridSearchCV` object defined in the video. This is a dictionary that we can read into a `pandas` DataFrame and contains a lot of useful information about the grid search we just undertook.\n",
    "  \n",
    "A reminder of the different column types in this property:\n",
    "  \n",
    "- `time_` columns\n",
    "- `param_` columns (one for each hyperparameter) and the singular params column (with all hyperparameter settings)\n",
    "- a `train_score` column for each cv fold including the `mean_train_score` and `std_train_score` columns\n",
    "- a `test_score` column for each cv fold including the `mean_test_score` and `std_test_score` columns\n",
    "- a `rank_test_score` column with a number from 1 to n (number of iterations) ranking the rows based on their `mean_test_score`\n",
    "  \n",
    "1. Read the `.cv_results_` property of the `grid_rf_class` `GridSearchCV` object into a dataframe & print the whole thing out to `inspect`.\n",
    "2. Extract & print the singular column containing a dictionary of all hyperparameters used in each iteration of the grid search.\n",
    "3. Extract & print the row that had the best mean test score by indexing using the `rank_test_score` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.76478134        nan 0.77046671        nan 0.77580062\n",
      "        nan 0.77289362]\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the train scores are non-finite: [       nan 0.76751289        nan 0.77740269        nan 0.8277371\n",
      "        nan 0.97295882]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       0.069938      0.029345         0.000000        0.000000   \n",
      "1       8.035159      1.229820         0.320498        0.077827   \n",
      "2       0.039107      0.019978         0.000000        0.000000   \n",
      "3      12.897208      2.507468         0.188786        0.040932   \n",
      "4       0.016495      0.003481         0.000000        0.000000   \n",
      "5      14.115396      0.688304         0.235631        0.048647   \n",
      "6       0.014250      0.001280         0.000000        0.000000   \n",
      "7      20.595176      2.292307         0.267598        0.061484   \n",
      "\n",
      "  param_max_depth param_max_features  \\\n",
      "0               2               auto   \n",
      "1               2               sqrt   \n",
      "2               4               auto   \n",
      "3               4               sqrt   \n",
      "4               8               auto   \n",
      "5               8               sqrt   \n",
      "6              15               auto   \n",
      "7              15               sqrt   \n",
      "\n",
      "                                      params  split0_test_score  \\\n",
      "0   {'max_depth': 2, 'max_features': 'auto'}                NaN   \n",
      "1   {'max_depth': 2, 'max_features': 'sqrt'}           0.770253   \n",
      "2   {'max_depth': 4, 'max_features': 'auto'}                NaN   \n",
      "3   {'max_depth': 4, 'max_features': 'sqrt'}           0.777489   \n",
      "4   {'max_depth': 8, 'max_features': 'auto'}                NaN   \n",
      "5   {'max_depth': 8, 'max_features': 'sqrt'}           0.782502   \n",
      "6  {'max_depth': 15, 'max_features': 'auto'}                NaN   \n",
      "7  {'max_depth': 15, 'max_features': 'sqrt'}           0.778443   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0                NaN                NaN                NaN                NaN   \n",
      "1           0.764266           0.753591           0.776706           0.759091   \n",
      "2                NaN                NaN                NaN                NaN   \n",
      "3           0.769799           0.758693           0.781245           0.765108   \n",
      "4                NaN                NaN                NaN                NaN   \n",
      "5           0.776360           0.760826           0.785573           0.773743   \n",
      "6                NaN                NaN                NaN                NaN   \n",
      "7           0.777201           0.757441           0.780455           0.770928   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
      "0              NaN             NaN                5                 NaN   \n",
      "1         0.764781        0.008124                4            0.765254   \n",
      "2              NaN             NaN                5                 NaN   \n",
      "3         0.770467        0.008164                3            0.776025   \n",
      "4              NaN             NaN                5                 NaN   \n",
      "5         0.775801        0.008593                1            0.826753   \n",
      "6              NaN             NaN                5                 NaN   \n",
      "7         0.772894        0.008357                2            0.973976   \n",
      "\n",
      "   split1_train_score  split2_train_score  split3_train_score  \\\n",
      "0                 NaN                 NaN                 NaN   \n",
      "1            0.767306            0.771989            0.763691   \n",
      "2                 NaN                 NaN                 NaN   \n",
      "3            0.776943            0.781207            0.774924   \n",
      "4                 NaN                 NaN                 NaN   \n",
      "5            0.827581            0.830219            0.826636   \n",
      "6                 NaN                 NaN                 NaN   \n",
      "7            0.975310            0.973336            0.971862   \n",
      "\n",
      "   split4_train_score  mean_train_score  std_train_score  \n",
      "0                 NaN               NaN              NaN  \n",
      "1            0.769325          0.767513         0.002935  \n",
      "2                 NaN               NaN              NaN  \n",
      "3            0.777913          0.777403         0.002144  \n",
      "4                 NaN               NaN              NaN  \n",
      "5            0.827497          0.827737         0.001298  \n",
      "6                 NaN               NaN              NaN  \n",
      "7            0.970310          0.972959         0.001728  \n",
      "                                      params\n",
      "0   {'max_depth': 2, 'max_features': 'auto'}\n",
      "1   {'max_depth': 2, 'max_features': 'sqrt'}\n",
      "2   {'max_depth': 4, 'max_features': 'auto'}\n",
      "3   {'max_depth': 4, 'max_features': 'sqrt'}\n",
      "4   {'max_depth': 8, 'max_features': 'auto'}\n",
      "5   {'max_depth': 8, 'max_features': 'sqrt'}\n",
      "6  {'max_depth': 15, 'max_features': 'auto'}\n",
      "7  {'max_depth': 15, 'max_features': 'sqrt'}\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "5      14.115396      0.688304         0.235631        0.048647   \n",
      "\n",
      "  param_max_depth param_max_features  \\\n",
      "5               8               sqrt   \n",
      "\n",
      "                                     params  split0_test_score  \\\n",
      "5  {'max_depth': 8, 'max_features': 'sqrt'}           0.782502   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "5            0.77636           0.760826           0.785573           0.773743   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
      "5         0.775801        0.008593                1            0.826753   \n",
      "\n",
      "   split1_train_score  split2_train_score  split3_train_score  \\\n",
      "5            0.827581            0.830219            0.826636   \n",
      "\n",
      "   split4_train_score  mean_train_score  std_train_score  \n",
      "5            0.827497          0.827737         0.001298  \n"
     ]
    }
   ],
   "source": [
    "grid_rf_class.fit(X_train, y_train)\n",
    "\n",
    "# Read the cv_results property into adataframe & print it out\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "print(cv_results_df)\n",
    "\n",
    "# Extract and print the column with a dictionary of hyperparameters used\n",
    "column = cv_results_df.loc[:, [\"params\"]]\n",
    "print(column)\n",
    "\n",
    "# Extract and print the row that had the best mean test score\n",
    "best_row = cv_results_df[cv_results_df['rank_test_score'] == 1]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069938</td>\n",
       "      <td>0.029345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 'auto'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.035159</td>\n",
       "      <td>1.229820</td>\n",
       "      <td>0.320498</td>\n",
       "      <td>0.077827</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 'sqrt'}</td>\n",
       "      <td>0.770253</td>\n",
       "      <td>0.764266</td>\n",
       "      <td>0.753591</td>\n",
       "      <td>0.776706</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.764781</td>\n",
       "      <td>0.008124</td>\n",
       "      <td>4</td>\n",
       "      <td>0.765254</td>\n",
       "      <td>0.767306</td>\n",
       "      <td>0.771989</td>\n",
       "      <td>0.763691</td>\n",
       "      <td>0.769325</td>\n",
       "      <td>0.767513</td>\n",
       "      <td>0.002935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.019978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'auto'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.897208</td>\n",
       "      <td>2.507468</td>\n",
       "      <td>0.188786</td>\n",
       "      <td>0.040932</td>\n",
       "      <td>4</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 'sqrt'}</td>\n",
       "      <td>0.777489</td>\n",
       "      <td>0.769799</td>\n",
       "      <td>0.758693</td>\n",
       "      <td>0.781245</td>\n",
       "      <td>0.765108</td>\n",
       "      <td>0.770467</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>3</td>\n",
       "      <td>0.776025</td>\n",
       "      <td>0.776943</td>\n",
       "      <td>0.781207</td>\n",
       "      <td>0.774924</td>\n",
       "      <td>0.777913</td>\n",
       "      <td>0.777403</td>\n",
       "      <td>0.002144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016495</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'max_depth': 8, 'max_features': 'auto'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.115396</td>\n",
       "      <td>0.688304</td>\n",
       "      <td>0.235631</td>\n",
       "      <td>0.048647</td>\n",
       "      <td>8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>{'max_depth': 8, 'max_features': 'sqrt'}</td>\n",
       "      <td>0.782502</td>\n",
       "      <td>0.776360</td>\n",
       "      <td>0.760826</td>\n",
       "      <td>0.785573</td>\n",
       "      <td>0.773743</td>\n",
       "      <td>0.775801</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826753</td>\n",
       "      <td>0.827581</td>\n",
       "      <td>0.830219</td>\n",
       "      <td>0.826636</td>\n",
       "      <td>0.827497</td>\n",
       "      <td>0.827737</td>\n",
       "      <td>0.001298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.014250</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>auto</td>\n",
       "      <td>{'max_depth': 15, 'max_features': 'auto'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.595176</td>\n",
       "      <td>2.292307</td>\n",
       "      <td>0.267598</td>\n",
       "      <td>0.061484</td>\n",
       "      <td>15</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>{'max_depth': 15, 'max_features': 'sqrt'}</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>0.777201</td>\n",
       "      <td>0.757441</td>\n",
       "      <td>0.780455</td>\n",
       "      <td>0.770928</td>\n",
       "      <td>0.772894</td>\n",
       "      <td>0.008357</td>\n",
       "      <td>2</td>\n",
       "      <td>0.973976</td>\n",
       "      <td>0.975310</td>\n",
       "      <td>0.973336</td>\n",
       "      <td>0.971862</td>\n",
       "      <td>0.970310</td>\n",
       "      <td>0.972959</td>\n",
       "      <td>0.001728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.069938      0.029345         0.000000        0.000000   \n",
       "1       8.035159      1.229820         0.320498        0.077827   \n",
       "2       0.039107      0.019978         0.000000        0.000000   \n",
       "3      12.897208      2.507468         0.188786        0.040932   \n",
       "4       0.016495      0.003481         0.000000        0.000000   \n",
       "5      14.115396      0.688304         0.235631        0.048647   \n",
       "6       0.014250      0.001280         0.000000        0.000000   \n",
       "7      20.595176      2.292307         0.267598        0.061484   \n",
       "\n",
       "  param_max_depth param_max_features  \\\n",
       "0               2               auto   \n",
       "1               2               sqrt   \n",
       "2               4               auto   \n",
       "3               4               sqrt   \n",
       "4               8               auto   \n",
       "5               8               sqrt   \n",
       "6              15               auto   \n",
       "7              15               sqrt   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "0   {'max_depth': 2, 'max_features': 'auto'}                NaN   \n",
       "1   {'max_depth': 2, 'max_features': 'sqrt'}           0.770253   \n",
       "2   {'max_depth': 4, 'max_features': 'auto'}                NaN   \n",
       "3   {'max_depth': 4, 'max_features': 'sqrt'}           0.777489   \n",
       "4   {'max_depth': 8, 'max_features': 'auto'}                NaN   \n",
       "5   {'max_depth': 8, 'max_features': 'sqrt'}           0.782502   \n",
       "6  {'max_depth': 15, 'max_features': 'auto'}                NaN   \n",
       "7  {'max_depth': 15, 'max_features': 'sqrt'}           0.778443   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0                NaN                NaN                NaN                NaN   \n",
       "1           0.764266           0.753591           0.776706           0.759091   \n",
       "2                NaN                NaN                NaN                NaN   \n",
       "3           0.769799           0.758693           0.781245           0.765108   \n",
       "4                NaN                NaN                NaN                NaN   \n",
       "5           0.776360           0.760826           0.785573           0.773743   \n",
       "6                NaN                NaN                NaN                NaN   \n",
       "7           0.777201           0.757441           0.780455           0.770928   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0              NaN             NaN                5                 NaN   \n",
       "1         0.764781        0.008124                4            0.765254   \n",
       "2              NaN             NaN                5                 NaN   \n",
       "3         0.770467        0.008164                3            0.776025   \n",
       "4              NaN             NaN                5                 NaN   \n",
       "5         0.775801        0.008593                1            0.826753   \n",
       "6              NaN             NaN                5                 NaN   \n",
       "7         0.772894        0.008357                2            0.973976   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0                 NaN                 NaN                 NaN   \n",
       "1            0.767306            0.771989            0.763691   \n",
       "2                 NaN                 NaN                 NaN   \n",
       "3            0.776943            0.781207            0.774924   \n",
       "4                 NaN                 NaN                 NaN   \n",
       "5            0.827581            0.830219            0.826636   \n",
       "6                 NaN                 NaN                 NaN   \n",
       "7            0.975310            0.973336            0.971862   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0                 NaN               NaN              NaN  \n",
       "1            0.769325          0.767513         0.002935  \n",
       "2                 NaN               NaN              NaN  \n",
       "3            0.777913          0.777403         0.002144  \n",
       "4                 NaN               NaN              NaN  \n",
       "5            0.827497          0.827737         0.001298  \n",
       "6                 NaN               NaN              NaN  \n",
       "7            0.970310          0.972959         0.001728  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You have built invaluable skills in looking 'under the hood' at what your grid search is doing by extracting and analysing the `.cv_results_` property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the best results\n",
    "  \n",
    "At the end of the day, we primarily care about the best performing 'square' in a grid search. Luckily Scikit Learn's `GridSearchCV` objects have a number of parameters that provide key information on just the best square (or row in `.cv_results_`).\n",
    "  \n",
    "Three properties you will explore are:\n",
    "  \n",
    "- `.best_score_`  The score (here `ROC_AUC`) from the best-performing square.\n",
    "- `.best_index_`  The index of the row in `.cv_results_` containing information on the best-performing square.\n",
    "- `.best_params_`  A dictionary of the parameters that gave the best score, for example `'max_depth': 10`\n",
    "The grid search object `grid_rf_class` is available.\n",
    "  \n",
    "A dataframe (`cv_results_df`) has been created from the `.cv_results_` for you on line 6. This will help you index into the results.\n",
    "  \n",
    "1. Extract and print out the `ROC_AUC` score from the best performing square in `grid_rf_class`.\n",
    "2. Create a variable from the best-performing row by indexing into `cv_results_df`.\n",
    "3. Create a variable, `.best_n_estimators` by extracting the `n_estimators=` parameter from the best-performing square in `grid_rf_class` and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7758006241929944\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "5      14.115396      0.688304         0.235631        0.048647   \n",
      "\n",
      "  param_max_depth param_max_features  \\\n",
      "5               8               sqrt   \n",
      "\n",
      "                                     params  split0_test_score  \\\n",
      "5  {'max_depth': 8, 'max_features': 'sqrt'}           0.782502   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "5            0.77636           0.760826           0.785573           0.773743   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
      "5         0.775801        0.008593                1            0.826753   \n",
      "\n",
      "   split1_train_score  split2_train_score  split3_train_score  \\\n",
      "5            0.827581            0.830219            0.826636   \n",
      "\n",
      "   split4_train_score  mean_train_score  std_train_score  \n",
      "5            0.827497          0.827737         0.001298  \n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Print out the ROC_AUC score from the best-performing square\n",
    "best_score = grid_rf_class.best_score_\n",
    "print(best_score)\n",
    "\n",
    "# Create a variable from the row related to the best-performing square\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "print(best_row)\n",
    "\n",
    "# Get the max_depth parameter from the best-performing square and print\n",
    "best_max_depth = grid_rf_class.best_params_[\"max_depth\"]\n",
    "print(best_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice stuff! Being able to quickly find and prioritize the huge volume of information given back from machine learning modeling output is a great skill. Here you had great practice doing that with `.cv_results_` by quickly isolating the key information on the best performing square. This will be very important when your grids grow from 12 squares to many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best results\n",
    "  \n",
    "While it is interesting to analyze the results of our grid search, our final goal is practical in nature; we want to make predictions on our test set using our estimator object.\n",
    "  \n",
    "We can access this object through the `.best_estimator_` property of our grid search object.\n",
    "  \n",
    "Let's take a look inside the `.best_estimator_` property, make predictions, and generate evaluation scores. We will firstly use the default predict (giving class predictions), but then we will need to use `predict_proba` rather than predict to generate the roc-auc score as roc-auc needs probability scores for its calculation. We use a slice `[:,1]` to get probabilities of the positive class.\n",
    "  \n",
    "You have available the `X_test` and `y_test` datasets to use and the `grid_rf_class` object from previous exercises.\n",
    "  \n",
    "1. Check the type of the `.best_estimator_` property.\n",
    "2. Use the `.best_estimator_` property to make predictions on our test set.\n",
    "3. Generate a confusion matrix and `ROC_AUC` score from our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "[0 0 0 0 0 0 0 1 0 1]\n",
      "Confusion Matrix \n",
      " [[6687  334]\n",
      " [1275  704]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score \n",
      " 0.7831412641451952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# See what type of object the best_estimator_property is\n",
    "print(type(grid_rf_class.best_estimator_))\n",
    "\n",
    "# Create an array of predictions directly using the best_estimator_property\n",
    "predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
    "\n",
    "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "print(predictions[0:10])\n",
    "\n",
    "# Now create a confusion matrix\n",
    "print(\"Confusion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the ROC-AUC score\n",
    "predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice stuff! The `.best_estimator_` property is a really powerful property to understand for streamlining your machine learning model building process. You now can run a grid search and seamlessly use the best model from that search to make predictions. Piece of cake!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
