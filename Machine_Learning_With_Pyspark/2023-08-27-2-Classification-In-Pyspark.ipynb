{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification In PySpark\n",
    "  \n",
    "Now that you are familiar with getting data into Spark, you'll move onto building two types of classification model: Decision Trees and Logistic Regression. You'll also find out about a few approaches to data preparation.\n",
    "  \n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   \n",
    "      /_/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pyspark.sql.SparkSession</td>\n",
    "    <td>Main entry point for using Spark functionality</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>spark.version</td>\n",
    "    <td>Retrieves the version of Spark</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>spark.stop()</td>\n",
    "    <td>Terminates the Spark session and releases resources</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>SparkSession.builder.master('local[*]').appName('flights').getOrCreate()</td>\n",
    "    <td>Creates a SparkSession with specific configuration</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>spark.count()</td>\n",
    "    <td>Counts the number of rows in a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>spark.show()</td>\n",
    "    <td>Displays the contents of a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>pyspark.sql.types.StructType</td>\n",
    "    <td>Defines the structure for a DataFrame's schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>pyspark.sql.types.StructField</td>\n",
    "    <td>Defines a single field within a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>pyspark.sql.types.IntegerType</td>\n",
    "    <td>Represents the integer data type in a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>pyspark.sql.types.StringType</td>\n",
    "    <td>Represents the string data type in a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>spark.read.csv</td>\n",
    "    <td>Reads data from a CSV file into a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>spark.printSchema()</td>\n",
    "    <td>Prints the schema of a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>spark.filter</td>\n",
    "    <td>Filters rows from a DataFrame based on a condition</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>spark.select</td>\n",
    "    <td>Selects specific columns from a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>spark.dropna</td>\n",
    "    <td>Removes rows with missing values from a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>spark.drop</td>\n",
    "    <td>Removes specified columns from a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>pyspark.sql.functions.round</td>\n",
    "    <td>Rounds the values in a column</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>spark.withColumn</td>\n",
    "    <td>Adds a new column or replaces an existing one</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>pyspark.ml.feature.StringIndexer</td>\n",
    "    <td>Converts string labels into numerical indices</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>spark.fit</td>\n",
    "    <td>Trains a machine learning model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>21</td>\n",
    "    <td>spark.transform</td>\n",
    "    <td>Applies a transformation to a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>pyspark.ml.feature.VectorAssembler</td>\n",
    "    <td>Combines multiple columns into a single vector column</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>23</td>\n",
    "    <td>spark.randomSplit</td>\n",
    "    <td>Splits a DataFrame into random subsets</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>24</td>\n",
    "    <td>pyspark.ml.classification.DecisionTreeClassifier</td>\n",
    "    <td>Creates a decision tree classification model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25</td>\n",
    "    <td>spark.groupBy</td>\n",
    "    <td>Groups data in a DataFrame by specified columns</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26</td>\n",
    "    <td>pyspark.ml.classification.LogisticRegression</td>\n",
    "    <td>Creates a logistic regression classification model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27</td>\n",
    "    <td>pyspark.ml.evaluation.MulticlassClassificationEvaluator</td>\n",
    "    <td>Evaluates multiclass classification models</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>28</td>\n",
    "    <td>pyspark.ml.evaluation.BinaryClassificationEvaluator</td>\n",
    "    <td>Evaluates binary classification models</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>29</td>\n",
    "    <td>pyspark.sql.functions.regexp_replace</td>\n",
    "    <td>Replaces occurrences of a pattern in a string column</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>30</td>\n",
    "    <td>pyspark.ml.feature.Tokenizer</td>\n",
    "    <td>Splits text into words (tokens)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31</td>\n",
    "    <td>pyspark.ml.feature.StopWordsRemover</td>\n",
    "    <td>Removes common words (stop words) from text</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>32</td>\n",
    "    <td>pyspark.ml.feature.HashingTF</td>\n",
    "    <td>Converts text data into numerical vectors</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>33</td>\n",
    "    <td>pyspark.ml.feature.IDF</td>\n",
    "    <td>Applies Inverse Document Frequency (IDF) to text vectors</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: pyspark  \n",
    "Version: 3.4.1  \n",
    "Summary: Apache Spark Python API  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import pyspark                      # Apache Spark:             Cluster Computing\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "  \n",
    "In this lesson you are going to learn how to prepare data for building a Machine Learning model.\n",
    "  \n",
    "**Do you need all of those columns?**\n",
    "  \n",
    "You'll be working with the cars data again. This is what the data look like at present. There are columns for the maker and model, the origin (either USA or non-USA), the type, number of cylinders, engine size, weight, length, RPM and fuel consumption. The models that you'll be building will depend on the physical characteristics of the cars rather than the model names or manufacturers, so you'll remove the corresponding columns from the data.\n",
    "  \n",
    "<center><img src='../_images/data-preparation-in-pyspark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dropping columns**\n",
    "  \n",
    "There are two approaches to doing this: either you can `drop()` the columns that you don't want or you can `select()` the fields which you do want to retain. Either way, the resulting data does not include those columns.\n",
    "  \n",
    "<center><img src='../_images/data-preparation-in-pyspark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Filtering out missing data**\n",
    "  \n",
    "Earlier you saw that there is a missing value in the cylinders column. Let's check to see how many other missing values there are. You'll use the `.filter()` method and provide a logical predicate using SQL syntax which identifies NULL values. Then the `.count()` method tells you how many records there are remaining. Just one. In this case it makes sense to simply remove the record with the missing value. There are a couple of ways that you could to do this. You could use the `.filter()` method again with a different predicate. Or you could take a more aggressive approach and use the `.dropna()` method to drop all records with missing values in any column. However, this should be done with care because it could result in the loss of a lot of otherwise useful data. You've now stripped down the data to what's needed to build a model.\n",
    "  \n",
    "<center><img src='../_images/data-preparation-in-pyspark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Mutating columns**\n",
    "  \n",
    "At present the weight and length columns are in units of pounds and inches respectively. You'll use the `.withColumn()` method to create a new mass column in units of kilograms. The `round()` function is used to limit the precision of the result. You can also use the `.withColumn()` method to replace the existing length column with values in meters. You now have mass and length in metric units.\n",
    "  \n",
    "<center><img src='../_images/data-preparation-in-pyspark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Indexing categorical data**\n",
    "  \n",
    "The type column consists of strings which represent six categories of vehicle type. You'll need to transform those strings into numbers. You do this using an instance of the `StringIndexer` class. In the constructor you provide the name of the string input column and a name for the new output column to be created. The indexer is first fit to the data, creating a `StringIndexerModel`. During the fitting process the distinct string values are identified and an index is assigned to each value. The model is then used to transform the data, creating a new column with the index values. By default the index values are assigned according to the descending relative frequency of each of the string values. Midsize is most common, so it gets an index of zero. Small is next most common, so its index is one. And so on. It's possible to choose different strategies for assigning index values by specifying the `stringOrderType` argument. Rather than using frequency of occurrence, strings can be ordered alphabetically. It's also possible to choose between ascending and descending order.\n",
    "  \n",
    "<center><img src='../_images/data-preparation-in-pyspark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Indexing country of origin**\n",
    "  \n",
    "You'll be building a classifier to predict whether or not a car was manufactured in the USA. So the origin column also needs to be converted from strings into numbers.\n",
    "  \n",
    "<center><img src='../_images/data-preparation-in-pyspark5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Assembling columns**\n",
    "  \n",
    "The final step in preparing the cars data is to consolidate the various input columns into a single column. This is necessary because the Machine Learning algorithms in Spark operate on a single vector of predictors, although each element in that vector may consist of multiple values. To illustrate the process you'll start with just a pair of features, cylinders and size. First you create an instance of the VectorAssembler class, providing it with the names of the columns that you want to consolidate and the name of the new output column. The assembler is then used to transform the data. Taking a look at the relevant columns you see that the new \"features\" column consists of values from the cylinders and size columns consolidated into a vector. Ultimately you are going to assemble all of the predictors into a single column.\n",
    "  \n",
    "<center><img src='../_images/data-preparation-in-pyspark6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's practice!**\n",
    "  \n",
    "Let's try out what we have learned on the SMS and flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns and rows\n",
    "  \n",
    "You previously loaded airline flight data from a CSV file. You're going to develop a model which will predict whether or not a given flight will be delayed.\n",
    "  \n",
    "In this exercise you need to trim those data down by:\n",
    "  \n",
    "1. removing an uninformative column and\n",
    "2. removing rows which do not have information about whether or not a flight was delayed.\n",
    "  \n",
    "The data are available as flights.\n",
    "  \n",
    "Note: You might find it useful to revise the slides from the lessons in the Slides panel next to the IPython Shell.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Remove the flight column.\n",
    "2. Find out how many records have missing values in the delay column.\n",
    "3. Remove records with missing values in the delay column.\n",
    "4. Remove records with missing values in any column and get the number of remaining rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('flights').getOrCreate()\n",
    "\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv('../_datasets/flights-larger.csv', sep=',', header=True, \n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "print(flights_drop_column.filter('delay IS NULL').count())\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've discarded the columns and rows which will certainly not contribute to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column manipulation\n",
    "  \n",
    "The Federal Aviation Administration (FAA) considers a flight to be \"delayed\" when it arrives 15 minutes or more after its scheduled time.\n",
    "  \n",
    "The next step of preparing the flight data has two parts:\n",
    "  \n",
    "1. convert the units of distance, replacing the `mile` column with a `km` column; and\n",
    "2. create a Boolean column indicating whether or not a flight was delayed.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import a function which will allow you to round a number to a specific number of decimal places.\n",
    "2. Derive a new `km` column from the `mile` column, rounding to zero decimal places. One mile is 1.60934 km.\n",
    "3. Remove the `mile` column.\n",
    "4. Create a `label` column with a value of 1 indicating the delay was 15 minutes or more and 0 otherwise. Think carefully about the logical condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_km = flights_none_missing.withColumn('km', round(flights_none_missing.mile * 1.60934, 0)).drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not(0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifteen minutes seems like quite a wide margin, but who are you to argue with the FAA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns\n",
    "  \n",
    "In the flights data there are two columns, `carrier` and `org`, which hold categorical data. You need to transform those columns into indexed numerical values.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the appropriate class and create an indexer object to transform the `carrier` column from a string to an numeric index.\n",
    "2. Prepare the indexer object on the flight data.\n",
    "3. Use the prepared indexer to create the numeric index column.\n",
    "4. Repeat the process for the `org` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Machine Learning model needs numbers not strings, so these transformations are vital!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling columns\n",
    "  \n",
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column.\n",
    "  \n",
    "An updated version of the flights data, which takes into account all of the changes from the previous few exercises, has the following predictor columns:\n",
    "  \n",
    "- `mon`, `dom` and `dow`\n",
    "- `carrier_idx` (indexed value from carrier)\n",
    "- `org_idx` (indexed value from org)\n",
    "- `km`\n",
    "- `depart`\n",
    "- `duration`\n",
    "  \n",
    "Note: The `truncate=False` argument to the `show()` method prevents data being truncated in the output.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the class which will assemble the predictors.\n",
    "2. Create an assembler object that will allow you to merge the predictors columns into a single column.\n",
    "3. Use the assembler to generate a new consolidated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[10.0,10.0,1.0,2.0,0.0,253.0,8.18,51.0]  |27   |\n",
      "|[11.0,22.0,1.0,2.0,0.0,1188.0,7.17,127.0]|-19  |\n",
      "|[2.0,14.0,5.0,4.0,2.0,3618.0,21.17,365.0]|60   |\n",
      "|[5.0,25.0,3.0,3.0,5.0,621.0,12.92,85.0]  |22   |\n",
      "|[3.0,28.0,1.0,4.0,3.0,1732.0,13.33,182.0]|70   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow',\n",
    "    'carrier_idx', \n",
    "    'org_idx',\n",
    "    'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready for building our first Machine Learning model. You've worked hard to get this sorted: well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "  \n",
    "Your first Machine Learning model will be a Decision Tree. This is probably the most intuitive model, so it seems like a good place to start.\n",
    "  \n",
    "**Anatomy of a Decision Tree: Root node**\n",
    "  \n",
    "A Decision Tree is constructed using an algorithm called \"Recursive Partitioning\". Consider a hypothetical example in which you build a Decision Tree to divide data into two classes, green and blue. You start by putting all of the records into the root node. Suppose that there are more green records than blue, in which case this node will be labelled \"green\". Now from amongst the predictors in the data you need to choose the one that will result in the most informative split of the data into two groups. Ideally you want the groups to be as homogeneous (or \"pure\") as possible: one should be mostly green and the other should be mostly blue.\n",
    "  \n",
    "**Anatomy of a Decision Tree: First split**\n",
    "  \n",
    "Once you have identified the most informative predictor, you split the data into two sets, labeled \"green\" or \"blue\" according to the dominant class. And this is where the recursion kicks in: you then apply exactly the same procedure on each of the child nodes, selecting the most informative predictor and splitting again.\n",
    "  \n",
    "**Anatomy of a Decision Tree: Second split**\n",
    "  \n",
    "So, for example, the green node on the left could be split again into two groups.\n",
    "  \n",
    "**Anatomy of a Decision Tree: Third split**\n",
    "  \n",
    "And the resulting green node could once again be split. The depth of each branch of the tree need not be the same. There are a variety of stopping criteria which can cause splitting to stop along a branch. For example, if the number of records in a node falls below a threshold or the purity of a node is above a threshold, then you might stop splitting. Once you have built the Decision Tree you can use it to make predictions for new data by following the splits from the root node along to the tip of a branch. The label for the final node would then be the prediction for the new data.\n",
    "  \n",
    "**Classifying cars**\n",
    "  \n",
    "Let's make this more concrete by looking at the cars data. You've transformed the country of origin column into a numeric index called 'label', with zero corresponding to cars manufactured in the USA and one for everything else. The remaining columns have all been consolidated into a column called 'features'. You want to build a Decision Tree which will use \"features\" to predict \"label\".\n",
    "  \n",
    "<center><img src='../_images/classification-in-pyspark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Split train/test**\n",
    "  \n",
    "An important aspect of building a Machine Learning model is being able to assess how well it works. In order to do this we use the `.randomSplit()` method to randomly split our data into two sets, a training set and a testing set. The proportions may vary, but generally you're looking at something like an 80:20 split, which means that the training set ends up having around 4 times as many records as the testing set.\n",
    "  \n",
    "<center><img src='../_images/classification-in-pyspark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Build a Decision Tree model**\n",
    "  \n",
    "Finally the moment has come, you're going to build a Decision Tree. You start by creating a `DecisionTreeClassifier()` object. The next step is to fit the model to the training data by calling the `.fit()` method.\n",
    "  \n",
    "<center><img src='../_images/classification-in-pyspark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Evaluating**\n",
    "  \n",
    "Now that you've trained the model you can assess how effective it is by making predictions on the test set and comparing the predictions to the known values. The `.transform()` method adds new columns to the DataFrame. The prediction column gives the class assigned by the model. You can compare this directly to the known labels in the testing data. Although the model gets the first example wrong, it's correct for the following four examples. There's also a probability column which gives the probabilities assigned to each of the outcome classes. For the first example, the model predicts that the outcome is 0 with probability 96%.\n",
    "  \n",
    "<center><img src='../_images/classification-in-pyspark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Confusion matrix**\n",
    "  \n",
    "A good way to understand the performance of a model is to create a confusion matrix which gives a breakdown of the model predictions versus the known labels. The confusion matrix consists of four counts which are labelled as follows: - \"positive\" indicates a prediction of 1, while - \"negative\" indicates a prediction of 0 and - \"true\" corresponds to a correct prediction, while - \"false\" designates an incorrect prediction. In this case the true positives and true negatives dominate but the model still makes a number of incorrect predictions. These counts can be used to calculate the accuracy, which is the proportion of correct predictions. For our model the accuracy is 74%.\n",
    "  \n",
    "<center><img src='../_images/classification-in-pyspark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's build Decision Trees!**\n",
    "  \n",
    "So, now that you know how to build a Decision Tree model with Spark, you can try that out on the flight data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "  \n",
    "To objectively assess a Machine Learning model you need to be able to test it on an independent set of data. You can't use the same data that you used to train the model: of course the model will perform (relatively) well on those data!\n",
    "  \n",
    "You will split the data into two components:\n",
    "  \n",
    "- training data (used to train the model) and\n",
    "- testing data (used to test the model).\n",
    "  \n",
    "Note: From here on you'll be working with a smaller subset of the `flights` data, which just makes the exercises run more quickly.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Randomly split the `flights` data into two sets with 80:20 proportions. For repeatability set a random number seed of 43 for the split.\n",
    "2. Check that the training data has roughly 80% of the records from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7996856234682855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights_assembled.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio looks as expected. You're ready to train and test a Decision Tree model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Decision Tree\n",
    "  \n",
    "Now that you've split the flights data into training and testing sets, you can use the training set to fit a Decision Tree model.\n",
    "  \n",
    "The data are available as `flights_train` and `flights_test`.\n",
    "  \n",
    "NOTE: It will take a few seconds for the model to train… please be patient!\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the class for creating a Decision Tree classifier.\n",
    "2. Create a classifier object and fit it to the training data.\n",
    "3. Make predictions for the testing data and take a look at the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|1    |0.0       |[0.5576430401366353,0.44235695986336465]|\n",
      "|1    |0.0       |[0.5576430401366353,0.44235695986336465]|\n",
      "|0    |1.0       |[0.37154355176634096,0.628456448233659] |\n",
      "|1    |1.0       |[0.37154355176634096,0.628456448233659] |\n",
      "|0    |0.0       |[0.6310591646280692,0.3689408353719308] |\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've built your first Machine Learning model with PySpark. Now to test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Decision Tree\n",
    "  \n",
    "You can assess the quality of your model by evaluating how well it performs on the testing data. Because the model was not trained on these data, this represents an objective assessment of the model.\n",
    "  \n",
    "A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n",
    "  \n",
    "- True Negatives (TN) — model predicts negative outcome & known outcome is negative\n",
    "- True Positives (TP) — model predicts positive outcome & known outcome is positive\n",
    "- False Negatives (FN) — model predicts negative outcome but known outcome is positive\n",
    "- False Positives (FP) — model predicts positive outcome but known outcome is negative.\n",
    "  \n",
    "These counts (TN, TP, FN and FP) should sum to the number of records in the testing data, which is only a subset of the flights data. You can compare to the number of records in the tests data, which is flights_test.count().\n",
    "  \n",
    "Note: These predictions are made on the testing data, so the counts are smaller than they would have been for predictions on the training data.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Create a confusion matrix by counting the combinations of label and prediction. Display the result.\n",
    "2. Count the number of True Negatives, True Positives, False Negatives and False Positives.\n",
    "3. Calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 7704|\n",
      "|    0|       0.0|14460|\n",
      "|    1|       1.0|18411|\n",
      "|    0|       1.0|11164|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6353234503952531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is decent but there are a lot of false predictions. We can make this model better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "  \n",
    "You've learned to build a Decision Tree. But it's good to have options. Logistic Regression is another commonly used classification model.\n",
    "  \n",
    "**Logistic Curve**\n",
    "  \n",
    "It uses a logistic function to model a binary target, where the target states are usually denoted by 1 and 0 or TRUE and FALSE. The maths of the model are outside the scope of this course, but this is what the logistic function looks like. For a Logistic Regression model the x-axis is a linear combination of predictor variables and the y-axis is the output of the model. Since the value of the logistic function is a number between zero and one, it's often thought of as a probability. In order to translate this number into one or other of the target states it's compared to a threshold, which is normally set at one half.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark.png' alt='img' width='740'></center>\n",
    "  \n",
    "If the number is above the threshold then the predicted state is one.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "Conversely, if it's below the threshold then the predicted state is zero. The model derives coefficients for each of the numerical predictors. Those coefficients might...\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "shift the curve to the right...\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "or to the left. They might make the transition between states...\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "more gradual...\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark5.png' alt='img' width='740'></center>\n",
    "  \n",
    "or more rapid. These characteristics are all extracted from the training data and will vary from one set of data to another.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Cars revisited**\n",
    "  \n",
    "Let's make this more concrete by returning to the cars data. You'll focus on the numerical predictors for the moment and return to categorical predictors later on. As before you prepare the data by consolidating the predictors into a single column and then randomly splitting the data into training and testing sets.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Build a Logistic Regression model**\n",
    "  \n",
    "To build a Logistic Regression model you first need to import the associated class and then create a classifier object. This is then fit to the training data using the `.fit()` method.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Predictions**\n",
    "  \n",
    "With a trained model you are able to make predictions on the testing data. As you saw with the Decision Tree, the `.transform()` method adds the prediction and probability columns. The probability column gives the predicted probability of each class, while the prediction column reflects the predicted label, which is derived from the probabilities by applying the threshold mentioned earlier.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Precision and recall**\n",
    "  \n",
    "You can assess the quality of the predictions by forming a confusion matrix. The quantities in the cells of the matrix can then be used to form some informative ratios. Recall that a positive prediction indicates that a car is manufactured outside of the USA and that predictions are considered to be true or false depending on whether they are correct or not. Precision is the proportion of positive predictions which are correct. For your model, two thirds of predictions for cars manufactured outside of the USA are correct. Recall is the proportion of positive targets which are correctly predicted. Your model also identifies 80% of cars which are actually manufactured outside of the USA. Bear in mind that these metrics are based on a relatively small testing set.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark10.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Weighted metrics**\n",
    "  \n",
    "Another way of looking at these ratios is to weight them across the positive and negative predictions. You can do this by creating an evaluator object and then calling the evaluate() method. This method accepts an argument which specifies the required metric. It's possible to request the weighted precision and recall as well as the overall accuracy. It's also possible to get the F1 metric, the harmonic mean of precision and recall, which is generally more robust than the accuracy. All of these metrics have assumed a threshold of one half. What happens if you vary that threshold?\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark11.png' alt='img' width='740'></center>\n",
    "  \n",
    "**ROC and AUC**\n",
    "  \n",
    "A threshold is used to decide whether the number returned by the Logistic Regression model translates into either the positive or the negative class. By default that threshold is set at a half. However, this is not the only choice. Choosing a larger or smaller value for the threshold will affect the performance of the model. The ROC curve plots the true positive rate versus the false positive rate as the threshold increases from zero (top right) to one (bottom left). The AUC summarizes the ROC curve in a single number. It's literally the area under the ROC curve. AUC indicates how well a model performs across all values of the threshold. An ideal model, that performs perfectly regardless of the threshold, would have AUC of 1. In an exercise we'll see how to use another evaluator to calculate the AUC.\n",
    "  \n",
    "<center><img src='../_images/logistic-regression-in-pyspark12.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's do Logistic Regression!**\n",
    "  \n",
    "You now know how to build a Logistic Regression model and assess the performance of that model using various metrics. Let's give this a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Logistic Regression model\n",
    "  \n",
    "You've already built a Decision Tree model using the flights data. Now you're going to create a Logistic Regression model on the same data.\n",
    "  \n",
    "The objective is to predict whether a flight is likely to be delayed by at least 15 minutes (label `1`) or not (label `0`).\n",
    "  \n",
    "Although you have a variety of predictors at your disposal, you'll only use the `mon`, `depart` and `duration` columns for the moment. These are numerical features which can immediately be used for a Logistic Regression model. You'll need to do a little more work before you can include categorical features. Stay tuned!\n",
    "  \n",
    "The data have been split into training and testing sets and are available as `flights_train` and `flights_test`.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the class for creating a Logistic Regression classifier.\n",
    "2. Create a classifier object and train it on the training data.\n",
    "3. Make predictions for the testing data and create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/27 23:12:40 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "[Stage 84:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 9455|\n",
      "|    0|       0.0|14931|\n",
      "|    1|       1.0|16660|\n",
      "|    0|       1.0|10693|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Selecting numeric columns\n",
    "flights_train_num = flights_train.select(\"mon\", 'depart', 'duration', 'features', 'label')\n",
    "flights_test_num = flights_test.select(\"mon\", \"depart\", \"duration\", 'features', 'label')\n",
    "\n",
    "# Create classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train_num)\n",
    "\n",
    "# Create a predictions for the test data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test_num)\n",
    "prediction.groupBy(\"label\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's unpack that confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Logistic Regression model\n",
    "  \n",
    "Accuracy is generally not a very reliable metric because it can be biased by the most common target class.\n",
    "  \n",
    "There are two other useful metrics:\n",
    "  \n",
    "- precision\n",
    "- recall\n",
    "  \n",
    "Check the slides for this lesson to get the relevant expressions.\n",
    "  \n",
    "Precision is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed?\n",
    "  \n",
    "Recall is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model?\n",
    "  \n",
    "The precision and recall are generally formulated in terms of the positive target class. But it's also possible to calculate weighted versions of these metrics which look at both target classes.\n",
    "  \n",
    "The components of the confusion matrix are available as `TN`, `TP`, `FN` and `FP`, as well as the object `prediction`.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Find the precision and recall.\n",
    "2. Create a multi-class evaluator and evaluate weighted precision.\n",
    "3. Create a binary evaluator and evaluate AUC using the `\"areaUnderROC\"` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision \t= 0.61\n",
      "recall \t\t= 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision \\t= {:.2f}\\nrecall \\t\\t= {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6106605467579678\n",
      "0.6504928839090344\n"
     ]
    }
   ],
   "source": [
    "print(weighted_precision)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted precision indicates what proportion of predictions (positive and negative) are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Text into Tables\n",
    "  \n",
    "It's said that 80% of Machine Learning is data preparation. As we'll see in this lesson, this is particularly true for text data. Before you can use Machine Learning algorithms you need to take unstructured text data and create structure, ultimately transforming the data into a table.\n",
    "  \n",
    "**One record per document**\n",
    "  \n",
    "We start with a collection of documents. These documents might be anything from a short snippet of text, like an SMS or email, to a lengthy report or book. Each document will become a record in the table.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**One document, many columns**\n",
    "  \n",
    "The text in each document will be mapped to columns in the table. First the text is split into words or tokens. You then remove short or common words that do not convey too much information. The table will then indicate the number of times that each of the remaining words occurred in the text. This table is also known as a \"term-document matrix\". There are some nuances to the process, but that's the central idea.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**A selection of children's books**\n",
    "  \n",
    "Suppose that your documents are the names of children's books. The raw data might look like this. Your job will be to transform these data into a table with one row per document and a column for each of the words.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Removing punctuation**\n",
    "  \n",
    "You're interested in words, not punctuation. You'll use regular expressions (or REGEX), a mini-language for pattern matching, to remove the punctuation symbols. Regular expressions is another big topic and outside of the scope of this course, but basically you are giving a list of symbols or text pattern to match. The hyphen is escaped by the backslashes because it has another meaning in the context of regular expressions. By escaping it you tell Spark to interpret the hyphen literally. You need to specify a column name, books.text, a pattern to be matched (stored in the variable REGEX), and the replacement text, which is simply a space. You now have some double spaces but you can use REGEX to clean those up too.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Text to tokens**\n",
    "  \n",
    "Next you split the text into words or tokens. You create a tokenizer object, giving it the name of the input column containing the text and the output column which will contain the tokens. The tokenizer is then applied to the text using the `.transform()` method. In the results you see a new column in which each document has been transformed into a list of words. As a side effect the words have all been reduced to lower case.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**What are stop words?**\n",
    "  \n",
    "Some words occur frequently in all of the documents. These common or \"stop\" words convey very little information, so you will also remove them using an instance of the `StopWordsRemover` class. This contains a list of stop words which can be customized if necessary.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Removing stop words**\n",
    "  \n",
    "Since you didn't give the input and output column names earlier, you specify them now and then apply the `.transform()` method. You could also have given these names when you created the remover.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Feature hashing**\n",
    "  \n",
    "Your documents might contain a large variety of words, so in principle our table could end up with an enormous number of columns, many of which would be only sparsely populated. It would also be handy to convert the words into numbers. Enter the hashing trick, which in simple terms converts words into numbers. You create an instance of the `HashingTF` class, providing the names of the input and output columns. You also give the number of features, which is effectively the largest number that will be produced by the hashing trick. This needs to be sufficiently big to capture the diversity in the words. The output in the hash column is presented in sparse format, which we will talk about more later on. For the moment though it's enough to note that there are two lists. The first list contains the hashed values and the second list indicates how many times each of those values occurs. For example, in the first document the word \"long\" has a hash of 8 and occurs twice. Similarly, the word \"five\" has a hash of 6 and occurs once in each of the last two documents.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dealing with common words**\n",
    "  \n",
    "The final step is to account for some words occurring frequently across many documents. If a word appears in many documents then it's probably going to be less useful for building a classifier. We want to weight the number of counts for a word in a particular document against how frequently that word occurs across all documents. To do this you reduce the effective count for more common words, giving what is known as the \"inverse document frequency\". Inverse document frequency is generated by the IDF class, which is first fit to the hashed data and then used to generate weighted counts. The word \"five\", for example, occurs in multiple documents, so its effective frequency is reduced. Conversely, the word \"long\" only occurs in one document, so its effective frequency is increased.\n",
    "  \n",
    "<center><img src='../_images/turning-text-into-tables-pyspark8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Text ready for Machine Learning!**\n",
    "  \n",
    "The inverse document frequencies are precisely what we need for building a Machine Learning model. Let's do that with the SMS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation, numbers and tokens\n",
    "  \n",
    "At the end of the previous chapter you loaded a dataset of SMS messages which had been labeled as either \"spam\" (label 1) or \"ham\" (label 0). You're now going to use those data to build a classifier model.\n",
    "  \n",
    "But first you'll need to prepare the SMS messages as follows:\n",
    "  \n",
    "- remove punctuation and numbers\n",
    "- tokenize (split into individual words)\n",
    "- remove stop words\n",
    "- apply the hashing trick\n",
    "- convert to TF-IDF representation.\n",
    "  \n",
    "In this exercise you'll remove punctuation and numbers, then tokenize the messages.\n",
    "  \n",
    "The SMS data are available as `sms`.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the function to replace regular expressions and the feature to tokenize.\n",
    "2. Replace all punctuation characters from the `'text'` column with a space. Do the same for all numbers in the `'text'` column.\n",
    "3. Split the `'text'` column into tokens. Name the output column `'words'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('../_datasets/sms.csv', sep=';', header=False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Next you'll remove stop words and apply the hashing trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words and hashing\n",
    "  \n",
    "The next steps will be to remove stop words and then apply the hashing trick, converting the results into a TF-IDF.\n",
    "  \n",
    "A quick reminder about these concepts:\n",
    "  \n",
    "- The hashing trick provides a fast and space-efficient way to map a very large (possibly infinite) set of items (in this case, all words contained in the SMS messages) onto a smaller, finite number of values.\n",
    "- The TF-IDF matrix reflects how important a word is to each document. It takes into account both the frequency of the word within each document but also the frequency of the word across all of the documents in the collection.\n",
    "  \n",
    "The tokenized SMS data are stored in `sms` in a column named `'words'`. You've cleaned up the handling of spaces in the data so that the tokenized text is neater.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the `StopWordsRemover`, `HashingTF` and `IDF` classes.\n",
    "2. Create a `StopWordsRemover` object (input column `'words'`, output column `'terms'`). Apply to `sms`.\n",
    "3. Create a `HashingTF` object (input results from previous step, output column `'hash'`). Apply to `wrangled`.\n",
    "4. Create an `IDF` object (input results from previous step, output column `'features'`). Apply to `wrangled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])|\n",
      "|[dont, worry, guess, busy]      |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377]) |\n",
      "|[call, freephone]               |(1024,[133,138],[5.367951058306837,2.273418200008753])                                              |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])    |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "sms = wrangled.select('id', 'words', 'label')\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms').transform(sms)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024).transform(wrangled)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features').fit(wrangled).transform(wrangled)\n",
    "\n",
    "tf_idf.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you're ready to build a spam classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a spam classifier\n",
    "  \n",
    "The SMS data have now been prepared for building a classifier. Specifically, this is what you have done:\n",
    "  \n",
    "- removed numbers and punctuation\n",
    "- split the messages into words (or \"tokens\")\n",
    "- removed stop words\n",
    "- applied the hashing trick and\n",
    "- converted to a TF-IDF representation.\n",
    "  \n",
    "Next you'll need to split the TF-IDF data into training and testing sets. Then you'll use the training data to fit a Logistic Regression model and finally evaluate the performance of that model on the testing data.\n",
    "  \n",
    "The data are stored in `sms` and `LogisticRegression` has been imported for you.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Split the data into training and testing sets in a 4:1 ratio. Set the random number `seed=` to 13 to ensure repeatability.\n",
    "2. Create a `LogisticRegression` object and fit it to the training data.\n",
    "3. Generate predictions on the testing data.\n",
    "4. Use the predictions to form a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   39|\n",
      "|    0|       0.0|  932|\n",
      "|    1|       1.0|  121|\n",
      "|    0|       1.0|    4|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sms = tf_idf.select('label', 'features')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well played! Your classifier won't be fooled by spam SMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
