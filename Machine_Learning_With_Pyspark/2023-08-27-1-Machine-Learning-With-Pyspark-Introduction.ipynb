{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Pyspark Introduction\n",
    "  \n",
    "Spark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. In this course you'll learn how to get data into Spark and then delve into the three fundamental Spark Machine Learning algorithms: Linear Regression, Logistic Regression/Classifiers, and creating pipelines. Along the way you'll analyse a large dataset of flight delays and spam text messages. With this background you'll be ready to harness the power of Spark and apply it on your own Machine Learning projects!\n",
    "  \n",
    "Spark is a framework for working with Big Data. In this chapter you'll cover some background about Spark and Machine Learning. You'll then find out how to connect to Spark using Python and load CSV data.\n",
    "  \n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   \n",
    "      /_/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pyspark.sql.SparkSession</td>\n",
    "    <td>The main entry point for using Spark functionality</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>spark.version</td>\n",
    "    <td>Retrieves the version of Spark</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>spark.stop()</td>\n",
    "    <td>Terminates the Spark session and releases resources</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>SparkSession.builder.master('local[*]').appName('flights').getOrCreate()</td>\n",
    "    <td>Creates a SparkSession with specific configuration</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>spark.count()</td>\n",
    "    <td>Counts the number of rows in a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>spark.show()</td>\n",
    "    <td>Displays the contents of a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>spark.dtypes</td>\n",
    "    <td>Returns the data types of columns in a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>pyspark.sql.types.StructType</td>\n",
    "    <td>Defines the structure for a DataFrame's schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>pyspark.sql.types.StructField</td>\n",
    "    <td>Defines a single field within a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>pyspark.sql.types.IntegerType</td>\n",
    "    <td>Represents the integer data type in a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>pyspark.sql.types.StringType</td>\n",
    "    <td>Represents the string data type in a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>spark.read.csv</td>\n",
    "    <td>Reads data from a CSV file into a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>spark.printSchema()</td>\n",
    "    <td>Prints the schema of a DataFrame</td>\n",
    "  </tr>\n",
    "</table>\n",
    "  \n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: pyspark  \n",
    "Version: 3.4.1  \n",
    "Summary: Apache Spark Python API  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import pyspark                      # Apache Spark:             Cluster Computing\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning & Spark\n",
    "  \n",
    "Welcome to the course on Machine Learning with Apache Spark, in which you will learn how to build Machine Learning models on large data sets using distributed computing techniques. Let's start with some fundamental concepts.\n",
    "  \n",
    "**Building the perfect waffle (an analogy)**\n",
    "  \n",
    "Suppose you wanted to teach a computer how to make waffles. You could find a good recipe and then give the computer explicit instructions about ingredients and proportions. Alternatively, you could present the computer with a selection of different waffle recipes and let it figure out the ingredients and proportions for the best recipe. The second approach is how Machine Learning works: the computer literally learns from examples.\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Regression & classification**\n",
    "  \n",
    "Machine Learning problems are generally less esoteric than finding the perfect waffle recipe. The most common problems apply either Regression or Classification. A regression model learns to predict a number. For example, when making waffles, how much flour should be used for a particular amount of sugar? A classification model, on the other hand, predicts a discrete or categorical value. For example, is a recipe calling for a particular amount of sugar and salt more likely to be for waffles or cupcakes?\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Data in RAM**\n",
    "  \n",
    "The performance of a Machine Learning model depends on data. In general, more data is a good thing. If an algorithm is able to train on a larger set of data, then its ability to generalize to new data will inevitably improve. However, there are some practical constraints. If the data can fit entirely into RAM then the algorithm can operate efficiently. What happens when those data no longer fit into memory?\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Data exceeds RAM**\n",
    "  \n",
    "The computer will start to use *virtual memory* and data will be *paged* back and forth between RAM and disk. Relative to RAM access, retrieving data from disk is slow. As the size of the data grows, paging becomes more intense and the computer begins to spend more and more time waiting for data. Performance plummets.\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Data distributed across a cluster**\n",
    "  \n",
    "How then do we deal with truly large datasets? One option is to distribute the problem across multiple computers in a cluster. Rather than trying to handle a large dataset on a single machine, it's divided up into partitions which are processed separately. Ideally each data partition can fit into RAM on a single computer in the cluster. This is the approach used by Spark.\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**What is Spark?**\n",
    "  \n",
    "Spark is a general purpose framework for cluster computing. It is popular for two main reasons: 1. it's generally much faster than other Big Data technologies like Hadoop, because it does most processing in memory and 2. it has a developer-friendly interface which hides much of the complexity of distributed computing.\n",
    "  \n",
    "- Compute accross a distributed cluster.\n",
    "- Data processed in memory\n",
    "- Well documented high level API\n",
    "  \n",
    "**Components: nodes**\n",
    "  \n",
    "Let's review the components of a Spark cluster. The cluster itself consists of one or more nodes. Each node is a computer with CPU, RAM and physical storage.\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Components: cluster manager**\n",
    "  \n",
    "A cluster manager allocates resources and coordinates activity across the cluster.\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Components: driver**\n",
    "  \n",
    "Every application running on the Spark cluster has a driver program. Using the Spark API, the driver communicates with the cluster manager, which in turn distributes work to the nodes.\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Components: executors**\n",
    "  \n",
    "On each node Spark launches an executor process which persists for the duration of the application. Work is divided up into tasks, which are simply units of computation. The executors run tasks in multiple threads across the cores in a node. When working with Spark you normally don't need to worry *too* much about the details of the cluster. Spark sets up all of that infrastructure for you and handles all interactions within the cluster. However, it's still useful to know how it works under the hood.\n",
    "  \n",
    "<center><img src='../_images/maching-learning-with-spark-intro8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Onward!**\n",
    "  \n",
    "You now have a basic understanding of the principles of Machine Learning and distributed computing with Spark. Next we'll learn how to connect to a Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Spark\n",
    "  \n",
    "Spark is currently the most popular technology for processing large quantities of data. Not only is it able to handle enormous data volumes, but it does so very efficiently too! Also, unlike some other distributed computing technologies, developing with Spark is a pleasure.\n",
    "  \n",
    "---\n",
    "  \n",
    "Which of these describe Spark?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] Spark is a framework for cluster computing.\n",
    "- [ ] Spark does most processing in memory.\n",
    "- [ ] Spark has a high-level API, which conceals a lot of complexity.\n",
    "- [x] All of the above.\n",
    "  \n",
    "Spark has all of this and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in a Spark Cluster\n",
    "  \n",
    "Spark is a distributed computing platform. It achieves efficiency by distributing data and computation across a cluster of computers.\n",
    "  \n",
    "A Spark cluster consists of a number of hardware and software components which work together.\n",
    "  \n",
    "---\n",
    "  \n",
    "Which of these is not part of a Spark cluster?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] One or more nodes\n",
    "- [ ] A cluster manager\n",
    "- [x] A load balancer\n",
    "- [ ] Executors\n",
    "  \n",
    "A load balancer distributes work across multiple resources, preventing overload on any one resource. In Spark this function is performed by the cluster manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Spark\n",
    "  \n",
    "The previous lesson was high level overviews of Machine Learning and Spark. In this lesson you'll review the process of connecting to Spark.\n",
    "  \n",
    "**Interacting with Spark**\n",
    "  \n",
    "The connection with Spark is established by the driver, which can be written in either Java, Scala, Python or R. Each of these languages has advantages and disadvantages. Java is relatively verbose, requiring a lot of code to accomplish even simple tasks. By contrast, Scala, Python and R, are high-level languages which can accomplish much with only a small amount of code. They also offer a REPL, or Read-Evaluate-Print loop, which is crucial for interactive development. You'll be using Python.\n",
    "  \n",
    "<center><img src='../_images/connecting-to-spark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Importing pyspark**\n",
    "  \n",
    "Python doesn't talk natively to Spark, so we'll kick off by importing the `pyspark` module, which makes Spark functionality available in the Python interpreter. Spark is under vigorous development. Because the interface is evolving it's important to know what version you're working with. We'll be using version 2.4.1, which was released in March 2019 (they can but im not).\n",
    "  \n",
    "**Sub-modules**\n",
    "  \n",
    "In addition to the main `pyspark` module, there are a few sub-modules which implement different aspects of the Spark interface. There are two versions of Spark Machine Learning: mllib, which uses an unstructured representation of data in RDDs and has been deprecated, and ml which is based on a structured, tabular representation of data in DataFrames. We'll be using the latter.\n",
    "  \n",
    "<center><img src='../_images/connecting-to-spark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Spark URL**\n",
    "  \n",
    "With the `pyspark` module loaded, you are able to connect to Spark. The next thing you need to do is tell Spark where the cluster is located. Here there are two options. You can either connect to a remote cluster, in which case you need to specify a Spark URL, which gives the network location of the cluster's master node. The URL is composed of an IP address or DNS name and a port number. The default port for Spark is 7077, but this must still be explicitly specified. When you're figuring out how Spark works, the infrastructure of a distributed cluster can get in the way. That's why it's useful to create a local cluster, where everything happens on a single computer. This is the setup that you're going to use throughout this course. For a local cluster, you need only specify \"local\" and, optionally, the number of cores to use. By default, a local cluster will run on a single core. Alternatively, you can give a specific number of cores or simply use the wildcard to choose all available cores.\n",
    "  \n",
    "<center><img src='../_images/connecting-to-spark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Creating a SparkSession**\n",
    "  \n",
    "You connect to Spark by creating a `SparkSession` object. The `SparkSession` class is found in the pyspark.sql sub-module. You specify the location of the cluster using the `.master()` method. Optionally you can assign a name to the application using the `.appName()` method. Finally you call the `.getOrCreate()` method, which will either create a new session object or return an existing object. Once the session has been created you are able to interact with Spark. Finally, although it's possible for multiple SparkSessions to co-exist, it's good practice to stop the `SparkSession` when you're done.\n",
    "  \n",
    "<center><img src='../_images/connecting-to-spark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's connect to Spark!**\n",
    "  \n",
    "Great! Let's connect to Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location of Spark master\n",
    "  \n",
    "Which of the following is not a valid way to specify the location of a Spark cluster?\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] `spark://13.59.151.161:7077`\n",
    "- [ ] `spark://ec2-18-188-22-23.us-east-2.compute.amazonaws.com:7077`\n",
    "- [x] `spark://18.188.22.23`\n",
    "- [ ] `local`\n",
    "- [ ] `local[4]`\n",
    "- [ ] `local[*]`\n",
    "  \n",
    "A Spark URL must always include a port number, so this URL is not valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SparkSession\n",
    "  \n",
    "In this exercise, you'll spin up a local Spark cluster using all available cores. The cluster will be accessible via a `SparkSession` object.\n",
    "  \n",
    "The `SparkSession` class has a `.builder` attribute, which is an instance of the `Builder` class. The `Builder` class exposes three important methods that let you:\n",
    "  \n",
    "Specify the location of the master node;\n",
    "  \n",
    "- name the application (optional); and\n",
    "- retrieve an existing `SparkSession` or, if there is `none`, create a new one.\n",
    "- The `SparkSession` class has a `.version` attribute which gives the version of Spark. Note: The version can also be accessed via the `.__version__` attribute on the `pyspark` module.\n",
    "  \n",
    "Find out more about `SparkSession` [here](https://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession).\n",
    "  \n",
    "Once you are finished with the cluster, it's a good idea to shut it down, which will free up its resources, making them available for other processes.\n",
    "  \n",
    "Note: You might find it useful to review the slides from the lessons in the Slides panel next to the IPython Shell.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the `SparkSession` class from `pyspark.sql`.\n",
    "2. Create a `SparkSession` object connected to a local cluster. Use all available cores. Name the application `'test'`.\n",
    "3. Use the version attribute on the `SparkSession` object to retrieve the version of Spark running on the cluster. Note: The version might be different to the one that's used in the presentation (it gets updated from time to time).\n",
    "4. Shut down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/27 21:37:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = (\n",
    "    SparkSession.builder.master('local[*]').appName('test').getOrCreate()\n",
    ")\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)\n",
    "\n",
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! The session object will now allow us to load data into Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "  \n",
    "In this lesson you'll look at how to read data into Spark.\n",
    "  \n",
    "**DataFrames: A refresher**\n",
    "  \n",
    "Spark represents tabular data using the DataFrame class. The data are captured as rows (or \"records\"), each of which is broken down into one or more columns (or \"fields\"). Every column has a name and a specific data type. Some selected methods and attributes of the DataFrame class are listed here. The `.count()` method gives the number of rows. The `.show()` method will display a subset of rows. The `.printSchema()` method and the `.dtypes` attribute give different views on column types. This is really scratching the surface of what's possible with a DataFrame. You can find out more by consulting the extensive documentation.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**CSV data for cars**\n",
    "  \n",
    "CSV is a common format for storing tabular data. For illustration we'll be using a CSV file with characteristics for a selection of motor vehicles. Each line in a CSV file is a new record and within each record, fields are separated by a delimiter character, which is normally a comma. The first line is an optional header record which gives column names.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Reading data from CSV**\n",
    "  \n",
    "Our session object has a \"`.read`\" attribute which, in turn, has a `.csv()` method which reads data from a CSV file and returns a DataFrame. The `.csv()` method has one mandatory argument, the path to the CSV file. There are a number of optional arguments. We'll take a quick look at some of the most important ones. The header argument specifies whether or not there is a header record. The `sep=` argument gives the field separator, which is a comma by default. There are two arguments which pertain to column data types, `schema=` and `inferSchema=`. Finally, the `nullValue=` argument gives the placeholder used to indicate missing data. Let's take a look at the data we've just loaded.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Peek at the data**\n",
    "  \n",
    "Using the `.show()` method we can take a look at a slice of the DataFrame. The `.csv()` method has split the data into rows and columns and picked up the column names from the header record. Looks great, doesn't it? Unfortunately there's a small snag. Before we unravel that snag, it's important to note that the first value in the cylinder column is not a number. It's the string `\"NA\"` which indicates missing data.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Check column types**\n",
    "  \n",
    "If you check the column data types then you'll find that they are all strings. That doesn't make sense since the last six columns are clearly numbers! However, this is the expected behavior: the `.csv()` method treats all columns as strings by default. You need to do a little more work to get the correct column types. There are two ways that you can do this: infer the column types from the data or manually specify the types.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Inferring column types from data**\n",
    "  \n",
    "It's possible to reasonably deduce the column types by setting the inferSchema argument to `True`. There is a price to pay though: Spark needs to make an extra pass over the data to figure out the column types before reading the data. If the data file is big then this will increase load time notably. Using this approach all of the column types are correctly identified except for cylinder. Why? The first value in this column is `\"NA\"`, so Spark thinks that the column contains strings.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dealing with missing data**\n",
    "  \n",
    "Missing data in CSV files are normally represented by a placeholder like the `\"NA\"` string. You can use the `nullValue=` argument to specify the placeholder. It's always a good idea to explicitly define the missing data placeholder. The `nullValue=` argument is case sensitive, so it's important to provide it in exactly the same form as it appears in the data file.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Specify column types**\n",
    "  \n",
    "If inferring column type is not successful then you have the option of specifying the type of each column in an explicit schema. This also makes it possible to choose alternative column names.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Final cars data**\n",
    "  \n",
    "This is what the final cars data look like. Note that the missing value at the top of the cylinders column is indicated by the special `null` constant.\n",
    "  \n",
    "<center><img src='../_images/loading-data-with-spark8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's load some data!**\n",
    "  \n",
    "You're ready to use what you've learned to load data from CSV files!\n",
    "  \n",
    "**Notes on CSV format**\n",
    "  \n",
    "- fields are separated by a comma (this is the default separator) and\n",
    "- missing data are denoted by the string `'NA'`.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flights data\n",
    "  \n",
    "In this exercise you're going to load some airline flight data from a CSV file. To ensure that the exercise runs quickly these data have been trimmed down to only 50 000 records. You can get a larger dataset in the same format here.\n",
    "  \n",
    "Data dictionary:\n",
    "  \n",
    "- `mon` — month (integer between 1 and 12)\n",
    "- `dom` — day of month (integer between 1 and 31)\n",
    "- `dow` — day of week (integer; 1 = Monday and 7 = Sunday)\n",
    "- `org` — origin airport ([IATA code](https://en.wikipedia.org/wiki/List_of_airline_codes))\n",
    "- `mile` — distance (miles)\n",
    "- `carrier` — carrier ([IATA code](https://en.wikipedia.org/wiki/List_of_airline_codes))\n",
    "- `depart` — departure time (decimal hour)\n",
    "- `duration` — expected duration (minutes)\n",
    "- `delay` — delay (minutes)\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Read data from a CSV file called `'flights.csv'`. Assign data types to columns automatically. Deal with missing data.\n",
    "2. How many records are in the data?\n",
    "3. Take a look at the first five records.\n",
    "4. What data types have been assigned to the columns? Do these look correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 275000 record(s).\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- mon: integer (nullable = true)\n",
      " |-- dom: integer (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- org: string (nullable = true)\n",
      " |-- mile: integer (nullable = true)\n",
      " |-- depart: double (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      "\n",
      "None\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('flights').getOrCreate()\n",
    "\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv('../_datasets/flights-larger.csv', sep=',', header=True, inferSchema=True, nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contains {} record(s).\".format(flights.count()))\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.printSchema())\n",
    "print(flights.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct data types have been inferred for all of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading SMS spam data\n",
    "  \n",
    "You've seen that it's possible to infer data types directly from the data. Sometimes it's convenient to have direct control over the column types. You do this by defining an explicit schema.\n",
    "  \n",
    "The file `sms.csv` contains a selection of SMS messages which have been classified as either 'spam' or 'ham'. These data have been adapted from the UCI [Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). There are a total of 5574 SMS, of which 747 have been labelled as spam.\n",
    "  \n",
    "Notes on CSV format:\n",
    "  \n",
    "- no header record and\n",
    "- fields are separated by a semicolon (this is not the default separator).\n",
    "  \n",
    "Data dictionary:\n",
    "  \n",
    "- `id` — record identifier\n",
    "- `text` — content of SMS message\n",
    "- `label` — spam or ham (integer; 0 = ham and 1 = spam)\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Specify the data schema, giving columns names (`\"id\"`, `\"text\"`, and `\"label\"`) and column types.\n",
    "2. Read data from a delimited file called `\"sms.csv\"`.\n",
    "3. Print the schema for the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('../_datasets/sms.csv', sep=';', header=False, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! You now know how to initiate a Spark session and load data. In the next chapter you'll use the data you've just loaded to build a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
