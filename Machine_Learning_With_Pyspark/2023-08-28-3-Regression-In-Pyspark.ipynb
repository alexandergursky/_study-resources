{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression In PySpark\n",
    "  \n",
    "Next you'll learn to create Linear Regression models. You'll also find out how to augment your data by engineering new predictors as well as a robust approach to selecting only the most relevant predictors.\n",
    "  \n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   \n",
    "      /_/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "  \n",
    "**Notebook Syntax**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "- Denotes additional information deemed to be *contextually* important\n",
    "- Colored in blue, HEX #7393B3\n",
    "  \n",
    "<span style='color:#E74C3C'>WARNING:</span>  \n",
    "- Significant information that is *functionally* critical  \n",
    "- Colored in red, HEX #E74C3C\n",
    "  \n",
    "---\n",
    "  \n",
    "**Links**\n",
    "  \n",
    "[NumPy Documentation](https://numpy.org/doc/stable/user/index.html#user)  \n",
    "[Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)  \n",
    "[Matplotlib Documentation](https://matplotlib.org/stable/index.html)  \n",
    "[Seaborn Documentation](https://seaborn.pydata.org)  \n",
    "[Apache Spark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Notable Functions**\n",
    "  \n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Operator</th>\n",
    "    <th>Use</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>pyspark.sql.SparkSession</td>\n",
    "    <td>Main entry point for using Spark functionality</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>spark.version</td>\n",
    "    <td>Retrieves the version of Spark</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>spark.stop()</td>\n",
    "    <td>Terminates the Spark session and releases resources</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>SparkSession.builder.master('local[*]').appName('flights').getOrCreate()</td>\n",
    "    <td>Creates a SparkSession with specific configuration</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>spark.count()</td>\n",
    "    <td>Counts the number of rows in a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>spark.show()</td>\n",
    "    <td>Displays the contents of a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>pyspark.sql.types.StructType</td>\n",
    "    <td>Defines the structure for a DataFrame's schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>pyspark.sql.types.StructField</td>\n",
    "    <td>Defines a single field within a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>pyspark.sql.types.IntegerType</td>\n",
    "    <td>Represents the integer data type in a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>pyspark.sql.types.StringType</td>\n",
    "    <td>Represents the string data type in a schema</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>spark.read.csv</td>\n",
    "    <td>Reads data from a CSV file into a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>spark.printSchema()</td>\n",
    "    <td>Prints the schema of a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>spark.filter</td>\n",
    "    <td>Filters rows from a DataFrame based on a condition</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>spark.select</td>\n",
    "    <td>Selects specific columns from a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>spark.dropna</td>\n",
    "    <td>Removes rows with missing values from a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>spark.drop</td>\n",
    "    <td>Removes specified columns from a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>pyspark.sql.functions.round</td>\n",
    "    <td>Rounds the values in a column</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>spark.withColumn</td>\n",
    "    <td>Adds a new column or replaces an existing one</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>pyspark.ml.feature.StringIndexer</td>\n",
    "    <td>Converts string labels into numerical indices</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>spark.fit</td>\n",
    "    <td>Trains a machine learning model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>21</td>\n",
    "    <td>spark.transform</td>\n",
    "    <td>Applies a transformation to a DataFrame</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>pyspark.ml.feature.VectorAssembler</td>\n",
    "    <td>Combines multiple columns into a single vector column</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>23</td>\n",
    "    <td>spark.randomSplit</td>\n",
    "    <td>Splits a DataFrame into random subsets</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>24</td>\n",
    "    <td>pyspark.ml.classification.DecisionTreeClassifier</td>\n",
    "    <td>Creates a decision tree classification model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25</td>\n",
    "    <td>spark.groupBy</td>\n",
    "    <td>Groups data in a DataFrame by specified columns</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26</td>\n",
    "    <td>pyspark.ml.classification.LogisticRegression</td>\n",
    "    <td>Creates a logistic regression classification model</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27</td>\n",
    "    <td>pyspark.ml.evaluation.MulticlassClassificationEvaluator</td>\n",
    "    <td>Evaluates multiclass classification models</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>28</td>\n",
    "    <td>pyspark.ml.evaluation.BinaryClassificationEvaluator</td>\n",
    "    <td>Evaluates binary classification models</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>29</td>\n",
    "    <td>pyspark.sql.functions.regexp_replace</td>\n",
    "    <td>Replaces occurrences of a pattern in a string column</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>30</td>\n",
    "    <td>pyspark.ml.feature.Tokenizer</td>\n",
    "    <td>Splits text into words (tokens)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>31</td>\n",
    "    <td>pyspark.ml.feature.StopWordsRemover</td>\n",
    "    <td>Removes common words (stop words) from text</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>32</td>\n",
    "    <td>pyspark.ml.feature.HashingTF</td>\n",
    "    <td>Converts text data into numerical vectors</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>33</td>\n",
    "    <td>pyspark.ml.feature.IDF</td>\n",
    "    <td>Applies Inverse Document Frequency (IDF) to text vectors</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>34</td>\n",
    "    <td>pyspark.ml.feature.OneHotEncoder</td>\n",
    "    <td>This is different from scikit-learnâ€™s OneHotEncoder, which keeps all categories. The output vectors are sparse.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>35</td>\n",
    "    <td>pyspark.ml.regression.LinearRegression</td>\n",
    "    <td>This supports multiple types of regularization: none (a.k.a. ordinary least squares), L2 (ridge regression), L1 (Lasso), L2 + L1 (elastic net)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>36</td>\n",
    "    <td>pyspark.ml.evaluation.RegressionEvaluator</td>\n",
    "    <td>Evaluator for Regression, which expects input columns prediction, label and an optional weight column.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>37</td>\n",
    "    <td>pyspark.ml.feature.Bucketizer</td>\n",
    "    <td>Maps a column of continuous features to a column of feature buckets. Since 3.0.0, Bucketizer can map multiple columns at once by setting the inputCols parameter. Note that when both the inputCol and inputCols parameters are set, an Exception will be thrown. The splits parameter is only used for single column usage, and splitsArray is for multiple columns.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "---\n",
    "  \n",
    "**Language and Library Information**  \n",
    "  \n",
    "Python 3.11.0  \n",
    "  \n",
    "Name: numpy  \n",
    "Version: 1.24.3  \n",
    "Summary: Fundamental package for array computing in Python  \n",
    "  \n",
    "Name: pandas  \n",
    "Version: 2.0.3  \n",
    "Summary: Powerful data structures for data analysis, time series, and statistics  \n",
    "  \n",
    "Name: matplotlib  \n",
    "Version: 3.7.2  \n",
    "Summary: Python plotting package  \n",
    "  \n",
    "Name: seaborn  \n",
    "Version: 0.12.2  \n",
    "Summary: Statistical data visualization  \n",
    "  \n",
    "Name: pyspark  \n",
    "Version: 3.4.1  \n",
    "Summary: Apache Spark Python API  \n",
    "  \n",
    "---\n",
    "  \n",
    "**Miscellaneous Notes**\n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "`python3.11 -m IPython` : Runs python3.11 interactive jupyter notebook in terminal.\n",
    "  \n",
    "`nohup ./relo_csv_D2S.sh > ./output/relo_csv_D2S.log &` : Runs csv data pipeline in headless log.  \n",
    "  \n",
    "`print(inspect.getsourcelines(test))` : Get self-defined function schema  \n",
    "  \n",
    "<span style='color:#7393B3'>NOTE:</span>  \n",
    "  \n",
    "Snippet to plot all built-in matplotlib styles :\n",
    "  \n",
    "```python\n",
    "\n",
    "x = np.arange(-2, 8, .1)\n",
    "y = 0.1 * x ** 3 - x ** 2 + 3 * x + 2\n",
    "fig = plt.figure(dpi=100, figsize=(10, 20), tight_layout=True)\n",
    "available = ['default'] + plt.style.available\n",
    "for i, style in enumerate(available):\n",
    "    with plt.style.context(style):\n",
    "        ax = fig.add_subplot(10, 3, i + 1)\n",
    "        ax.plot(x, y)\n",
    "    ax.set_title(style)\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Numerical Python:         Arrays and linear algebra\n",
    "import pandas as pd                 # Panel Datasets:           Dataset manipulation\n",
    "import matplotlib.pyplot as plt     # MATLAB Plotting Library:  Visualizations\n",
    "import seaborn as sns               # Seaborn:                  Visualizations\n",
    "import pyspark                      # Apache Spark:             Cluster Computing\n",
    "\n",
    "# Setting a standard figure size\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "# Set the maximum number of columns to be displayed\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "  \n",
    "In the last chapter you saw how to use categorical variables in a model by simply converting them to indexed numerical values. In general this is not sufficient for a regression model. Let's see why.\n",
    "  \n",
    "**The problem with indexed values**\n",
    "  \n",
    "In the cars data the type column is categorical, with six levels: 'Midsize', 'Small', 'Compact', 'Sporty', 'Large' and 'Van'. Here you can see the number of times that each of those levels occurrs in the data. You used a string indexer to assign a numerical index to each level. However, there's a problem with the index: the numbers don't have any objective meaning. The index for 'Sporty' is 3. Does it make sense to do arithmetic on that index? No. For example, it wouldn't be meaningful to add the index for 'Sporty' to the index for 'Compact'. Nor would it be valid to compare those indexes and say that 'Sporty' is larger or smaller than 'Compact'. However, a regression model works by doing precisely this: arithmetic on predictor variables. You need to convert the index values into a format in which you can perform meaningful mathematical operations.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dummy variables**\n",
    "  \n",
    "The first step is to create a column for each of the levels. Effectively you then place a check in the column corresponding to the value in each row. So, for example, a record with a type of 'Sporty' would have a check in the 'Sporty' column. These new columns are known as 'dummy variables'.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dummy variables: binary encoding**\n",
    "  \n",
    "However, rather than having checks in the dummy variable columns it makes more sense to use binary values, where a one indicates the presence of the corresponding level. It might occur to you that the volume of data has exploded. You've gone from a single column of categorical values to six binary encoded dummy variables. If there were more levels then you'd have even more columns. This could get out of hand. However, the majority of the cells in the new columns contain zeros. The non-zero values, which actually encode the information, are relatively infrequent. This effect becomes even more pronounced if there are more levels. You can exploit this by converting the data into a sparse format.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dummy variables: sparse representation**\n",
    "  \n",
    "Rather than recording the individual values, the sparse representation simply records the column numbers and value for the non-zero values.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dummy variables: redundant column**\n",
    "  \n",
    "You can take this one step further. Since the categorical levels are mutually exclusive you can drop one of the columns. If type is not 'Midsize', 'Small', 'Compact', 'Sporty' or 'Large' then it must be 'Van'. The process of creating dummy variables is called 'One-Hot Encoding' because only one of the columns created is ever active or 'hot'. Let's see how this is done in Spark.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**One-hot encoding**\n",
    "  \n",
    "As you might expect, there's a class for doing one-hot encoding. Import the `OneHotEncoder` class from the `feature` sub-module. When instantiating the class you need to specify the names of the input and output columns. For car type the input column is the index we defined earlier. Choose 'type_dummy' as the output column name. Note that these arguments are given as lists, so it's possible to specify multiple columns if necessary. Next fit the encoder to the data. Check how many category levels have been identified: six as expected.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**One-hot encoding**\n",
    "  \n",
    "Now that the encoder is set up it can be applied to the data by calling the `.transform()` method. Let's take a look at the results. There's now a type_dummy column which captures the dummy variables. As mentioned earlier, the final level is treated differently. No column is assigned to type Van because if a vehicle isn't one of the other types then it must be a Van. To have a separate dummy variable for Van would be redundant. The sparse format used to represent dummy variables looks a little complicated. Let's take a moment to dig into dense versus sparse formats.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Dense versus sparse**\n",
    "  \n",
    "Suppose that you want to store a vector which consists mostly of zeros. You could store it as a dense vector, in which each of the elements of the vector is stored explicitly. This is wasteful though because most of those elements are zeros. A sparse representation is a much better alternative. To create a sparse vector you need to specify the size of the vector (in this case, eight), the positions which are non-zero (in this case, positions zero and five, noting that we start counting at zero) and the values for each of those positions, one and seven. Sparse representation is essential for effective one-hot encoding on large data sets.\n",
    "  \n",
    "<center><img src='../_images/ohe-in-pyspark7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**One-Hot Encode categoricals**\n",
    "  \n",
    "Let's try out one-hot encoding on the flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding flight origin\n",
    "  \n",
    "The `org` column in the `flights` data is a categorical variable giving the airport from which a flight departs.\n",
    "  \n",
    "- ORD â€” O'Hare International Airport (Chicago)\n",
    "- SFO â€” San Francisco International Airport\n",
    "- JFK â€” John F Kennedy International Airport (New York)\n",
    "- LGA â€” La Guardia Airport (New York)\n",
    "- SMF â€” Sacramento\n",
    "- SJC â€” San Jose\n",
    "- OGG â€” Kahului (Hawaii)\n",
    "  \n",
    "Obviously this is only a small subset of airports. Nevertheless, since this is a categorical variable, it needs to be one-hot encoded before it can be used in a regression model.\n",
    "  \n",
    "The data are in a variable called `flights`. You have already used a string indexer to create a column of indexed values corresponding to the strings in `org`.\n",
    "  \n",
    "You might find it useful to revise the slides from the lessons in the Slides panel next to the IPython Shell.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Import the one-hot encoder class.\n",
    "2. Create a one-hot encoder instance, naming the input column `'org_idx'` and the output column `'org_dummy'`.\n",
    "3. Apply the one-hot encoder to the `flights` data.\n",
    "4. Generate a summary of the mapping from categorical values to binary encoded dummy variables. Include only unique values and order by `'org_idx'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/28 10:34:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 275000 record(s).\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- mon: integer (nullable = true)\n",
      " |-- dom: integer (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- org: string (nullable = true)\n",
      " |-- mile: integer (nullable = true)\n",
      " |-- depart: double (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      "\n",
      "None\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.master('local[*]').appName('flights').getOrCreate()\n",
    "\n",
    "# Read data in from CSV\n",
    "flights = spark.read.csv('../_datasets/flights-larger.csv', \n",
    "                         sep=',', header=True, inferSchema=True, nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print('The dataset contains {} record(s).'.format(flights.count()))\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "print(flights.printSchema())\n",
    "print(flights.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|org_idx|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|    0.0|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|    0.0|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|    0.0|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|    2.0|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|    5.0|\n",
      "|  3| 28|  1|     B6|   377|LGA|1076| 13.33|     182|   70|    3.0|\n",
      "|  5| 28|  6|     B6|   904|ORD| 740|  9.58|     130|   47|    0.0|\n",
      "|  1| 19|  2|     UA|   820|SFO| 679| 12.75|     123|  135|    1.0|\n",
      "|  8|  5|  5|     US|  2175|LGA| 214|  13.0|      71|  -10|    3.0|\n",
      "|  5| 27|  5|     AA|  1240|ORD|1197| 14.42|     195|  -11|    0.0|\n",
      "|  8| 20|  6|     B6|   119|JFK|1182| 14.67|     198|   20|    2.0|\n",
      "|  2|  3|  1|     AA|  1881|JFK|1090| 15.92|     200|   -9|    2.0|\n",
      "|  8| 26|  5|     B6|    35|JFK|1028| 20.58|     193|  102|    2.0|\n",
      "|  4|  9|  5|     AA|   336|ORD| 733|  20.5|     125|   32|    0.0|\n",
      "|  3|  8|  2|     UA|   678|ORD| 733| 10.95|     129|   55|    0.0|\n",
      "|  8| 10|  3|     OH|  6347|LGA| 292| 11.75|     102|    8|    3.0|\n",
      "|  8| 14|  0|     UA|   624|ORD| 612| 17.92|     109|   57|    0.0|\n",
      "|  4|  8|  4|     OH|  5585|JFK| 301| 13.25|      88|   23|    2.0|\n",
      "|  1| 14|  4|     UA|  1524|SFO| 414| 14.87|      91|   27|    1.0|\n",
      "|  1|  2|  6|     AA|  1341|ORD|1846|   7.5|     275|   26|    0.0|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "flights = StringIndexer(\n",
    "    inputCol='org',\n",
    "    outputCol='org_idx'\n",
    ").fit(flights).transform(flights)\n",
    "\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SMF|    4.0|(7,[4],[1.0])|\n",
      "|SJC|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights_onehot = onehot.transform(flights)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Note that one of the category levels, OGG, does not get a dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding shirt sizes\n",
    "  \n",
    "You have data for a consignment of t-shirts. The data includes the size of the shirt, which is given as either S, M, L or XL.\n",
    "  \n",
    "Here are the counts for the different sizes:\n",
    "  \n",
    "```\n",
    "+----+-----+\n",
    "|size|count|\n",
    "+----+-----+\n",
    "|   S|    8|\n",
    "|   M|   15|\n",
    "|   L|   20|\n",
    "|  XL|    7|\n",
    "+----+-----+\n",
    "```\n",
    "  \n",
    "The sizes are first converted to an index using `StringIndexer` and then one-hot encoded using `OneHotEncoder`.\n",
    "  \n",
    "---\n",
    "  \n",
    "Which of the following is **not** true:\n",
    "  \n",
    "Possible Answers\n",
    "  \n",
    "- [ ] S shirts get index 2.0 and are one-hot encoded as `(3,[2],[1.0])`\n",
    "- [ ] M shirts get index 1.0 and are one-hot encoded as `(3,[1],[1.0])`\n",
    "- [ ] L shirts get index 0.0 and are one-hot encoded as `(3,[0],[1.0])`\n",
    "- [x] XL shirts get index 3.0 and are one-hot encoded as `(3,[3],[1.0])`\n",
    "  \n",
    "Correct! This statement is false: XL is the least frequent size, so it receives an index of 3. However, it is one-hot encoded to `(3,[],[])` because it does not get it's own dummy variable. If none of the other dummy variables are true, then this one must be true. So to make a separate dummy variable would be redundant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "  \n",
    "In the previous lesson you learned how to one-hot encode categorical features, which is essential for building regression models. In this lesson you'll find out how to build a regression model to predict numerical values.\n",
    "  \n",
    "**Consumption versus mass: scatter**\n",
    "  \n",
    "Returning to the cars data, suppose you wanted to predict fuel consumption using vehicle mass. A scatter plot is a good way to visualize the relationship between those two variables. Only a subset of the data are included in this plot, but it's clear that consumption increases with mass. However the relationship is not perfectly linear: there's scatter for individual points. A model should describe the average relationship of consumption to mass, without necessarily passing through individual points.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Consumption versus mass: fit**\n",
    "  \n",
    "This line, for example, might describe the underlying trend in the data.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Consumption versus mass: alternative fits**\n",
    "  \n",
    "But there are other lines which could equally well describe that trend. How do you choose the line which best describes the relationship?\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Consumption versus mass: residuals**\n",
    "  \n",
    "First we need to define the concept of residuals. The residual is the difference between the observed value and the corresponding modeled value. The residuals are indicated in the plot as the vertical lines between the data points and the model line. The best model would somehow make these residuals as small as possible.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Loss function**\n",
    "  \n",
    "Out of all possible models, the best model is found by minimizing a loss function, which is an equation that describes how well the model fits the data. This is the equation for the mean squared error loss function. Let's quickly break it down.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Loss function: Observed values**\n",
    "  \n",
    "You've got the observed values, $y_i$, â€¦\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Loss function: Model values**\n",
    "  \n",
    "and the modeled values, $\\hat{y}_i$. The difference between these is the residual. The residuals are squared and then summed togetherâ€¦\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Loss function: Mean**\n",
    "  \n",
    "before finally dividing through by the number of data points to give the mean or average. By minimizing the loss function you are effectively minimizing the average residual or the average distance between the observed and modeled values. If this looks a little complicated, don't worry: Spark will do all of the maths for you.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Assemble predictors**\n",
    "  \n",
    "Let's build a regression model to predict fuel consumption using three predictors: mass, number of cylinders and vehicle type, where the last is a categorical which we've already one-hot encoded. As before the first step towards building a model is to take our predictors and assemble them into a single column called 'features'. The data are then randomly split into training and testing sets.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Build regression model**\n",
    "  \n",
    "The model is created using the `LinearRegression` class which is imported from the `regression` module. By default this class expects to find the target data in a column called \"label\". Since you are aiming to predict the \"consumption\" column you need to explicitly specify the name of the label column when creating a regression object. Next train the model on the training data using the `.fit()` method. The trained model can then be used to making predictions on the testing data using the `.transform()` method.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Examine predictions**\n",
    "  \n",
    "Comparing the predicted values to the known values from the testing data you'll see that there is reasonable agreement. It's hard to tell from a table though. A plot gives a clearer picture. The dashed diagonal lie represents perfect prediction. Most of the points lie close to this line, which is good.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview10.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Calculate RMSE**\n",
    "  \n",
    "It's useful to have a single number which summarizes the performance of a model. For classifiers there are a variety of such metrics. The Root Mean Squared Error is often used for regression models. It's the square root of the Mean Squared Error, which you've already encountered, and corresponds to the standard deviation of the residuals. The metrics for a classifier, like accuracy, precision and recall, are measured on an absolute scale where it's possible to immediately identify values that are \"good\" or \"bad\". Values of RMSE are relative to the scale of the value that you're aiming to predict, so interpretation is a little more challenging. A smaller RMSE, however, always indicates better predictions.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview11.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Consumption versus mass: intercept**\n",
    "  \n",
    "Let's examine the model. The intercept is the value predicted by the model when all predictors are zero. On the plot this is the point where the model line intersects the vertical dashed line.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview12.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Examine intercept**\n",
    "  \n",
    "You can find this value for the model using the intercept attribute. This is the predicted fuel consumption when both mass and number of cylinders are zero and the vehicle type is 'Van'. Of course, this is an entirely hypothetical scenario: no vehicle could have zero mass!\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview13.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Consumption versus mass: slope**\n",
    "  \n",
    "There's a slope associated with each of the predictors too, which represents how rapidly the model changes when that predictor changes.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview14.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Examine Coefficients**\n",
    "  \n",
    "The `.coefficients` attribute gives you access to those values. There's a coefficient for each of the predictors. The coefficients for mass and number of cylinders are positive, indicating that heavier cars with more cylinders consume more fuel. These coefficients also represent the rate of change for the corresponding predictor. For example, the coefficient for mass indicates the change in fuel consumption when mass increases by one unit. Remember that there's no dummy variable for Van? The coefficients for the type dummy variables are relative to Vans. These coefficients should also be interpreted with care: if you are going to compare the values for different vehicle types then this needs to be done for fixed mass and number of cylinders. Since all of the type dummy coefficients are negative, the model indicates that, for a specific mass and number of cylinders, all other vehicle types consume less fuel than a Van. Large vehicles have the most negative coefficient, so it's possible to say that, for a specific mass and number of cylinders, Large vehicles are the most fuel efficient.\n",
    "  \n",
    "<center><img src='../_images/regression-in-pyspark-overview15.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Regression for numeric predictions**\n",
    "  \n",
    "You've covered a lot of ground in this lesson. Let's apply what you've learned to the flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Just distance\n",
    "  \n",
    "In this exercise you'll build a regression model to predict flight duration (the `'duration'` column).\n",
    "  \n",
    "For the moment you'll keep the model simple, including only the distance of the flight (the `'km'` column) as a predictor.\n",
    "  \n",
    "The data are in `flights`. The first few records are displayed in the terminal. These data have also been split into training and testing sets and are available as `flights_train` and `flights_test`.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Create a linear regression object. Specify the name of the label column. Fit it to the training data.\n",
    "2. Make predictions on the testing data.\n",
    "3. Create a regression evaluator object and use it to evaluate RMSE on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+-------------+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|org_idx|    org_dummy|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+-------------+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|    0.0|(7,[0],[1.0])|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|    0.0|(7,[0],[1.0])|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|    0.0|(7,[0],[1.0])|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|    2.0|(7,[2],[1.0])|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|    5.0|(7,[5],[1.0])|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_onehot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|    km|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+\n",
      "| 10| 10|  1|     OO|  5836|ORD|  8.18|      51|   27|    0.0|(7,[0],[1.0])| 253.0|\n",
      "|  1|  4|  1|     OO|  5866|ORD|  15.5|     102| null|    0.0|(7,[0],[1.0])| 750.0|\n",
      "| 11| 22|  1|     OO|  6016|ORD|  7.17|     127|  -19|    0.0|(7,[0],[1.0])|1188.0|\n",
      "|  2| 14|  5|     B6|   199|JFK| 21.17|     365|   60|    2.0|(7,[2],[1.0])|3618.0|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 12.92|      85|   22|    5.0|(7,[5],[1.0])| 621.0|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_onehot = flights_onehot.withColumn('km', round(flights_onehot.mile * 1.60934)\n",
    ").drop('mile')\n",
    "\n",
    "flights_onehot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+--------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|    km|features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+--------+\n",
      "| 10| 10|  1|     OO|  5836|ORD|  8.18|      51|   27|    0.0|(7,[0],[1.0])| 253.0| [253.0]|\n",
      "|  1|  4|  1|     OO|  5866|ORD|  15.5|     102| null|    0.0|(7,[0],[1.0])| 750.0| [750.0]|\n",
      "| 11| 22|  1|     OO|  6016|ORD|  7.17|     127|  -19|    0.0|(7,[0],[1.0])|1188.0|[1188.0]|\n",
      "|  2| 14|  5|     B6|   199|JFK| 21.17|     365|   60|    2.0|(7,[2],[1.0])|3618.0|[3618.0]|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 12.92|      85|   22|    5.0|(7,[5],[1.0])| 621.0| [621.0]|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object so we can pack in our features\n",
    "assembler = VectorAssembler(inputCols=['km'], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights = assembler.transform(flights_onehot)\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/28 10:35:58 WARN Instrumentation: [da377eef] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/08/28 10:36:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/28 10:36:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|420     |496.47387688102566|\n",
      "|160     |133.6052673711577 |\n",
      "|130     |133.6052673711577 |\n",
      "|125     |133.6052673711577 |\n",
      "|120     |133.6052673711577 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.122513250221225"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(featuresCol='features', labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration', metricName='rmse').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've built a simple regression model. Let's make sense of the coefficients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients\n",
    "  \n",
    "The linear regression model for flight duration as a function of distance takes the form\n",
    "  \n",
    "$\\text{duration} = Î± + Î² Ã— \\text{distance}$\n",
    "\n",
    "$where.$\n",
    "\n",
    "$Î± â€” \\text{intercept}$ (component of duration which does not depend on distance) and $Î² â€” \\text{coefficient}$ (rate at which duration increases as a function of distance; also called the slope). \n",
    "  \n",
    "By looking at the coefficients of your model you will be able to infer:\n",
    "  \n",
    "- How much of the average flight duration is actually spent on the ground\n",
    "- What the average speed is during a flight.\n",
    "  \n",
    "The linear regression model is available as `regression`.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. What's the intercept?\n",
    "2. What are the coefficients? This is a vector.\n",
    "3. Extract the element from the vector which corresponds to the slope for distance.\n",
    "4. Find the average speed in km per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.25114399435385\n",
      "[0.07572383337017277]\n",
      "0.07572383337017277\n",
      "792.3529136024126\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average speed of a commercial jet is around 850 km/hour. But you got that already from the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Adding origin airport\n",
    "  \n",
    "Some airports are busier than others. Some airports are bigger than others too. Flights departing from large or busy airports are likely to spend more time taxiing or waiting for their takeoff slot. So it stands to reason that the duration of a flight might depend not only on the distance being covered but also the airport from which the flight departs.\n",
    "  \n",
    "You are going to make the regression model a little more sophisticated by including the departure airport as a predictor.\n",
    "  \n",
    "These data have been split into training and testing sets and are available as `flights_train` and `flights_test`. The origin airport, stored in the `'org'` column, has been indexed into `org_idx`, which in turn has been one-hot encoded into `'org_dummy'`. The first few records are displayed in the terminal.\n",
    "  \n",
    "```\n",
    "Subset from the flights DataFrame:\n",
    "\n",
    "+------+-------+-------------+----------------------+\n",
    "|km    |org_idx|org_dummy    |features              |\n",
    "+------+-------+-------------+----------------------+\n",
    "|3465.0|2.0    |(7,[2],[1.0])|(8,[0,3],[3465.0,1.0])|\n",
    "|509.0 |0.0    |(7,[0],[1.0])|(8,[0,1],[509.0,1.0]) |\n",
    "|542.0 |1.0    |(7,[1],[1.0])|(8,[0,2],[542.0,1.0]) |\n",
    "|1989.0|0.0    |(7,[0],[1.0])|(8,[0,1],[1989.0,1.0])|\n",
    "|415.0 |0.0    |(7,[0],[1.0])|(8,[0,1],[415.0,1.0]) |\n",
    "+------+-------+-------------+----------------------+\n",
    "only showing top 5 rows\n",
    "```\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Fit a linear regression model to the training data.\n",
    "2. Make predictions for the testing data.\n",
    "3. Calculate the RMSE for predictions on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+--------------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|    km|            features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+--------------------+\n",
      "| 10| 10|  1|     OO|  5836|ORD|  8.18|      51|   27|    0.0|(7,[0],[1.0])| 253.0|(8,[0,1],[253.0,1...|\n",
      "|  1|  4|  1|     OO|  5866|ORD|  15.5|     102| null|    0.0|(7,[0],[1.0])| 750.0|(8,[0,1],[750.0,1...|\n",
      "| 11| 22|  1|     OO|  6016|ORD|  7.17|     127|  -19|    0.0|(7,[0],[1.0])|1188.0|(8,[0,1],[1188.0,...|\n",
      "|  2| 14|  5|     B6|   199|JFK| 21.17|     365|   60|    2.0|(7,[2],[1.0])|3618.0|(8,[0,3],[3618.0,...|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 12.92|      85|   22|    5.0|(7,[5],[1.0])| 621.0|(8,[0,6],[621.0,1...|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights = assembler.transform(flights_onehot)\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/28 10:36:37 WARN Instrumentation: [3d4c7af8] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.149356528137124"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=999)\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(featuresCol='features', labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the test data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on test data\n",
    "RegressionEvaluator(labelCol='duration', metricName='rmse').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/28 10:37:03 WARN Instrumentation: [79383342] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.149356528137124"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(featuresCol='features', labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration', metricName='rmse').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's try to make sense of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting coefficients\n",
    "  \n",
    "Remember that origin airport, `'org'`, has eight possible values (ORD, SFO, JFK, LGA, SMF, SJC, TUS and OGG) which have been one-hot encoded to seven dummy variables in `'org_dummy'`.\n",
    "  \n",
    "The values for `'km'` and `'org_dummy'` have been assembled into features, which has eight columns with sparse representation. Column indices in features are as follows:\n",
    "  \n",
    "- 0 â€” km\n",
    "- 1 â€” ORD\n",
    "- 2 â€” SFO\n",
    "- 3 â€” JFK\n",
    "- 4 â€” LGA\n",
    "- 5 â€” SMF\n",
    "- 6 â€” SJC\n",
    "- 7 â€” TUS\n",
    "  \n",
    "Note that OGG does not appear in this list because it is the reference level for the origin airport category.\n",
    "  \n",
    "In this exercise you'll be using the `intercept` and `coefficients` attributes to interpret the model.\n",
    "  \n",
    "The `coefficients` attribute is a list, where the first element indicates how flight duration changes with flight distance.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Find the average speed in km per hour. This will be different to the value that you got earlier because your model is now more sophisticated.\n",
    "2. What's the average time on the ground at OGG?\n",
    "3. What's the average time on the ground at JFK?\n",
    "4. What's the average time on the ground at LGA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807.4210180010705\n",
      "16.08342956009249\n",
      "68.73127420263674\n",
      "62.80283321068916\n"
     ]
    }
   ],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60 / regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Averate minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk= inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to spend over an hour on the ground at JFK or LGA but only around 15 minutes at OGG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing & Engineering\n",
    "  \n",
    "The largest improvements in Machine Learning model performance are often achieved by carefully manipulating features. In this lesson you'll be learning about a few approaches to doing this.\n",
    "  \n",
    "**Bucketing**\n",
    "  \n",
    "Let's start with bucketing. It's often convenient to convert a continuous variable, like age or height, into discrete values. This can be done by assigning values to buckets or bins with well defined boundaries. The buckets might have uniform or variable width.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Bucketing heights**\n",
    "  \n",
    "Let's make this more concrete by thinking about observations of people's heights. If you plot the heights on a histogram then it seems reasonableâ€¦\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "â€¦ to divide the heights up into ranges. To each of these rangesâ€¦\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "â€¦ you assign a label. Then you create a new column in the dataâ€¦\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "â€¦ with the appropriate labels. The resulting categorical variable is often a more powerful predictor than the original continuous variable.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**RPM histogram**\n",
    "  \n",
    "Let's apply this to the cars data. Looking at the distribution of values for RPM you see that the majority lie in the range between 4500 and 6000. There are a few either below or above this range. This suggests that it would make sense to bucket these values according to those boundaries.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**RPM buckets**\n",
    "  \n",
    "You create a `Bucketizer` object, specifying the bin boundaries as the \"`splits=`\" argument and also providing the names of the input and output columns. You then apply this object to the data by calling the `.transform()` method.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark6.png' alt='img' width='740'></center>\n",
    "  \n",
    "The result has a new column with the discrete bucket values. The three buckets have been assigned index values zero, one and two, corresponding to the low, medium and high ranges for RPM.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**One-hot encoded RPM buckets**\n",
    "  \n",
    "As you saw earlier, before you can use these index values in a regression model, they first need to be one-hot encoded. The low and medium RPM ranges are mapped to distinct dummy variables, while the high range is the reference level and does not get a separate dummy variable.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Model with bucketed RPM**\n",
    "  \n",
    "Let's look at the intercept and coefficients for a model which predicts fuel consumption based on bucketed RPM data. The intercept tells us what the fuel consumption is for the reference level, which is the high RPM bucket. To get the consumption for the low RPM bucket you add the first coefficient to the intercept. Similarly, to find the consumption for the medium RPM bucket you add the second coefficient to the intercept.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**More feature engineering**\n",
    "  \n",
    "There are many other approaches to engineering new features. It's common to apply arithmetic operations to one or more columns to create new features.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark10.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Mass & Height to BMI**\n",
    "  \n",
    "Returning to the heights data. Suppose that we also had data for mass.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark11.png' alt='img' width='740'></center>\n",
    "  \n",
    "Then it might be perfectly reasonable to engineer a new column for BMI. Potentially BMI might be a more powerful predictor than either height or mass in isolation.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark12.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Engineering density**\n",
    "  \n",
    "Let's apply this idea to the cars data. You have columns for mass and length. Perhaps some combination of the two might be even more meaningful. You can create different forms of density by dividing the mass through by the first three powers of length. Since you only have the length of the vehicles but not their width or height, the length is being used as a proxy for these missing dimensions. In so doing you create three new predictors. The first density represents how mass changes with vehicle length. The second and third densities approximate how mass varies with the area and volume of the vehicle. Which of these will be meaningful for our model? Right now you don't know, you're just trying things out. Powerful new features are often discovered through trial and error. In the next lesson you'll learn about a technique for selecting only the relevant predictors in a regression model.\n",
    "  \n",
    "<center><img src='../_images/bucketing-and-engineering-in-pyspark13.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Let's engineer some features!**\n",
    "  \n",
    "Right now though, let's apply what you've learned to the flights data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing departure time\n",
    "  \n",
    "Time of day data are a challenge with regression models. They are also a great candidate for bucketing.\n",
    "  \n",
    "In this lesson you will convert the flight departure times from numeric values between 0 (corresponding to 00:00) and 24 (corresponding to 24:00) to binned values. You'll then take those binned values and one-hot encode them.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Create a bucketizer object with bin boundaries at 0, 3, 6, â€¦, 24 which correspond to times 0:00, 03:00, 06:00, â€¦, 24:00. Specify input column as `depart` and output column as `depart_bucket`.\n",
    "2. Bucket the departure times in the `flights` data. Show the first five values for `depart` and `depart_bucket`.\n",
    "3. Create a one-hot encoder object. Specify output column as `depart_dummy`.\n",
    "4. Train the encoder on the data and then use it to convert the bucketed departure times to dummy variables. Show the first five values for `depart`, `depart_bucket` and `depart_dummy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "|  8.18|          2.0|\n",
      "|  15.5|          5.0|\n",
      "|  7.17|          2.0|\n",
      "| 21.17|          7.0|\n",
      "| 12.92|          4.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "|  8.18|          2.0|(7,[2],[1.0])|\n",
      "|  15.5|          5.0|(7,[5],[1.0])|\n",
      "|  7.17|          2.0|(7,[2],[1.0])|\n",
      "| 21.17|          7.0|    (7,[],[])|\n",
      "| 12.92|          4.0|(7,[4],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[\n",
    "    3 * x for x in range(9)\n",
    "], inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can add departure time to your regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Adding departure time\n",
    "  \n",
    "In the previous exercise the departure time was bucketed and converted to dummy variables. Now you're going to include those dummy variables in a regression model for flight duration.\n",
    "  \n",
    "The data are in flights. The `km`, `org_dummy` and `depart_dummy` columns have been assembled into `features`, where `km` is index 0, `org_dummy` runs from index 1 to 7 and `depart_dummy` from index 8 to 14.\n",
    "  \n",
    "The data have been split into training and testing sets and a linear regression model, regression, has been built on the training data. Predictions have been made on the testing data and are available as `predictions`.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Find the RMSE for predictions on the testing data.\n",
    "2. Find the average time spent on the ground for flights departing from OGG between 21:00 and 24:00.\n",
    "3. Find the average time spent on the ground for flights departing from OGG between 03:00 and 06:00.\n",
    "4. Find the average time spent on the ground for flights departing from JFK between 03:00 and 06:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+--------------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|    km|depart_bucket| depart_dummy|            features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+--------------------+\n",
      "| 10| 10|  1|     OO|  5836|ORD|  8.18|      51|   27|    0.0|(7,[0],[1.0])| 253.0|          2.0|(7,[2],[1.0])|(15,[0,1,10],[253...|\n",
      "|  1|  4|  1|     OO|  5866|ORD|  15.5|     102| null|    0.0|(7,[0],[1.0])| 750.0|          5.0|(7,[5],[1.0])|(15,[0,1,13],[750...|\n",
      "| 11| 22|  1|     OO|  6016|ORD|  7.17|     127|  -19|    0.0|(7,[0],[1.0])|1188.0|          2.0|(7,[2],[1.0])|(15,[0,1,10],[118...|\n",
      "|  2| 14|  5|     B6|   199|JFK| 21.17|     365|   60|    2.0|(7,[2],[1.0])|3618.0|          7.0|    (7,[],[])|(15,[0,3],[3618.0...|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 12.92|      85|   22|    5.0|(7,[5],[1.0])| 621.0|          4.0|(7,[4],[1.0])|(15,[0,6,12],[621...|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'depart_dummy'], outputCol='features')\n",
    "\n",
    "flights = assembler.transform(flights_onehot.drop('features'))\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/28 10:37:18 WARN Instrumentation: [b5255f2a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "[Stage 32:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 10.830159839817965\n",
      "10.540857158639529\n",
      "-3.798073907621628\n",
      "48.00341282699934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train with training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Find the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration', metricName='rmse').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "avg_eve_ogg = regression.intercept\n",
    "print(avg_eve_ogg)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
    "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
    "print(avg_night_ogg)\n",
    "\n",
    "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\n",
    "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[8]\n",
    "print(avg_night_jfk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding departure time resulted in a smaller RMSE. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "  \n",
    "The regression models that you've built up until now have blindly included all of the provided features. Next you are going to learn about a more sophisticated model which effectively selects only the most useful features.\n",
    "  \n",
    "**Features: Only a few**\n",
    "  \n",
    "A linear regression model attempts to derive a coefficient for each feature in the data. The coefficients quantify the effect of the corresponding features. More features imply more coefficients. This works well when your dataset has a few columns and many rows. You need to derive a few coefficients and you have plenty of data.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Features: Too many**\n",
    "  \n",
    "The converse situation, many columns and few rows, is much more challenging. Now you need to calculate values for numerous coefficients but you don't have much data to do it. Even if you do manage to derive values for all of those coefficients, your model will end up being very complicated and difficult to interpret. Ideally you want to create a parsimonious model: one that has just the minimum required number of predictors. It will be as simple as possible, yet still able to make robust predictions.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark1.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Features: Selected**\n",
    "  \n",
    "The obvious solution is to simply select the \"best\" subset of columns. But how to choose that subset? There are a variety of approaches to this \"feature selection\" problem.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark2.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Loss function (revisited)**\n",
    "  \n",
    "In this lesson we'll be exploring one such approach to feature selection known as \"penalized regression\". The basic idea is that the model is penalized, or punished, for having too many coefficients. Recall that the conventional regression algorithm chooses coefficients to minimize the loss function, which is average of the squared residuals. A good model will result in low MSE because its predictions will be close to the observed values.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark3.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Loss function with regularization**\n",
    "  \n",
    "With penalized regression an additional \"regularization\" or \"shrinkage\" term is added to the loss function. Rather than depending on the data, this term is a function of the model coefficients.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark4.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Regularization term**\n",
    "  \n",
    "There are two standard forms for the regularization term. Lasso regression uses a term which is proportional to the absolute value of the coefficients, while Ridge regression uses the square of the coefficients. In both cases this extra term in the loss function penalizes models with too many coefficients. There's a subtle distinction between Lasso and Ridge regression. Both will shrink the coefficients of unimportant predictors. However, whereas Ridge will result in those coefficients being close to zero, Lasso will actually force them to zero precisely. It's also possible to have a mix of Lasso and Ridge. The strength of the regularization is determined by a parameter which is generally denoted by the Greek symbol lambda. When lambda = 0 there is no regularization and when lambda is large regularization completely dominates. Ideally you want to choose a value for lambda between these two extremes!\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark5.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Cars again**\n",
    "  \n",
    "Let's make this more concrete by returning to the cars data. We've assembled the mass, cylinders and type columns along with the freshly engineered density columns. We've effectively got ten predictors available for the model. As usual we'll split these data into training and testing sets.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark6.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Cars: Linear regression**\n",
    "  \n",
    "Let's start by fitting a standard linear regression model to the training data. You can then make predictions on the testing data and calculate the RMSE. When you look at the model coefficients you find that all predictors have been assigned non-zero values. This means that every predictor is contributing to the model. This is certainly possible, but it's unlikely that all of the features are actually important for predicting consumption.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark7.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Cars: Ridge regression**\n",
    "  \n",
    "Now let's fit a Ridge Regression model to the same data. You get a Ridge Regression model by giving a value of zero for `elasticNetParam=`. An arbitrary value of 0.1 has been chosen for the regularization strength. Later you'll learn a way to choose good values for this parameter based on the data. When you calculate the RMSE on the testing data you find that it has increased slightly, but not enough to cause concern. Looking at the coefficients you see that they are all smaller than the coefficients for the standard linear regression model. They have been \"shrunk\".\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark8.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Cars: Lasso regression**\n",
    "  \n",
    "Finally let's build a Lasso Regression model, by setting `elasticNetParam=` to 1. Again you find that the testing RMSE has increased, but not by a significant degree. Turning to the coefficients though, you see that something important has happened: all but two of the coefficients are now zero. There are effectively only two predictors left in the model: the dummy variable for a small type car and the linear density. Lasso Regression has identified the most important predictors and set the coefficients for the rest to zero. This tells us that we can get a good model by simply knowing whether or not a car is 'small' and it's linear density. A simpler model with no significant loss in performance.\n",
    "  \n",
    "<center><img src='../_images/regularization-in-pyspark9.png' alt='img' width='740'></center>\n",
    "  \n",
    "**Regularization ? simple model**\n",
    "  \n",
    "Let's try out regularization on our flight duration model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: More features!\n",
    "  \n",
    "Let's add more features to our model. This will not necessarily result in a better model. Adding some features might improve the model. Adding other features might make it worse.\n",
    "  \n",
    "More features will always make the model more complicated and difficult to interpret.\n",
    "  \n",
    "These are the features you'll include in the next model:\n",
    "  \n",
    "- `'km'`\n",
    "- `'org'` (origin airport, one-hot encoded, 8 levels)\n",
    "- `'depart'` (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)\n",
    "- `'dow'` (departure day of week, one-hot encoded, 7 levels) and\n",
    "- `'mon'` (departure month, one-hot encoded, 12 levels).\n",
    "  \n",
    "These have been assembled into the `'features'` column, which is a sparse representation of 32 columns (remember one-hot encoding produces a number of columns which is one fewer than the number of levels).\n",
    "  \n",
    "The data are available as `flights`, randomly split into `flights_train` and `flights_test`.\n",
    "  \n",
    "This exercise is based on a small subset of the `flights` data.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Fit a linear regression model to the training data.\n",
    "2. Generate predictions for the testing data.\n",
    "3. Calculate the RMSE on the testing data.\n",
    "4. Look at the model coefficients. Are any of them zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+--------------------+-------------+---------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|    km|depart_bucket| depart_dummy|            features|    dow_dummy|      mon_dummy|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+--------------------+-------------+---------------+\n",
      "| 10| 10|  1|     OO|  5836|ORD|  8.18|      51|   27|    0.0|(7,[0],[1.0])| 253.0|          2.0|(7,[2],[1.0])|(15,[0,1,10],[253...|(6,[1],[1.0])|(11,[10],[1.0])|\n",
      "|  1|  4|  1|     OO|  5866|ORD|  15.5|     102| null|    0.0|(7,[0],[1.0])| 750.0|          5.0|(7,[5],[1.0])|(15,[0,1,13],[750...|(6,[1],[1.0])| (11,[1],[1.0])|\n",
      "| 11| 22|  1|     OO|  6016|ORD|  7.17|     127|  -19|    0.0|(7,[0],[1.0])|1188.0|          2.0|(7,[2],[1.0])|(15,[0,1,10],[118...|(6,[1],[1.0])|     (11,[],[])|\n",
      "|  2| 14|  5|     B6|   199|JFK| 21.17|     365|   60|    2.0|(7,[2],[1.0])|3618.0|          7.0|    (7,[],[])|(15,[0,3],[3618.0...|(6,[5],[1.0])| (11,[2],[1.0])|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 12.92|      85|   22|    5.0|(7,[5],[1.0])| 621.0|          4.0|(7,[4],[1.0])|(15,[0,6,12],[621...|(6,[3],[1.0])| (11,[5],[1.0])|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+--------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot = OneHotEncoder(inputCols=['dow'], outputCols=['dow_dummy'])\n",
    "flights = onehot.fit(flights).transform(flights)\n",
    "\n",
    "onehot = OneHotEncoder(inputCols=['mon'], outputCols=['mon_dummy'])\n",
    "flights = onehot.fit(flights).transform(flights)\n",
    "\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+-------------+---------------+--------------------+\n",
      "|mon|dom|dow|carrier|flight|org|depart|duration|delay|org_idx|    org_dummy|    km|depart_bucket| depart_dummy|    dow_dummy|      mon_dummy|            features|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+-------------+---------------+--------------------+\n",
      "| 10| 10|  1|     OO|  5836|ORD|  8.18|      51|   27|    0.0|(7,[0],[1.0])| 253.0|          2.0|(7,[2],[1.0])|(6,[1],[1.0])|(11,[10],[1.0])|(32,[0,1,10,16,31...|\n",
      "|  1|  4|  1|     OO|  5866|ORD|  15.5|     102| null|    0.0|(7,[0],[1.0])| 750.0|          5.0|(7,[5],[1.0])|(6,[1],[1.0])| (11,[1],[1.0])|(32,[0,1,13,16,22...|\n",
      "| 11| 22|  1|     OO|  6016|ORD|  7.17|     127|  -19|    0.0|(7,[0],[1.0])|1188.0|          2.0|(7,[2],[1.0])|(6,[1],[1.0])|     (11,[],[])|(32,[0,1,10,16],[...|\n",
      "|  2| 14|  5|     B6|   199|JFK| 21.17|     365|   60|    2.0|(7,[2],[1.0])|3618.0|          7.0|    (7,[],[])|(6,[5],[1.0])| (11,[2],[1.0])|(32,[0,3,20,23],[...|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 12.92|      85|   22|    5.0|(7,[5],[1.0])| 621.0|          4.0|(7,[4],[1.0])|(6,[3],[1.0])| (11,[5],[1.0])|(32,[0,6,12,18,26...|\n",
      "+---+---+---+-------+------+---+------+--------+-----+-------+-------------+------+-------------+-------------+-------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\n",
    "    'km', 'org_dummy', 'depart_dummy', 'dow_dummy', 'mon_dummy'\n",
    "], outputCol='features')\n",
    "\n",
    "flights = assembler.transform(flights.drop('features'))\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/28 10:37:42 WARN Instrumentation: [78e3156f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "[Stage 39:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 10.850936124799667\n",
      "[0.07439835694918875,27.09895740021517,19.997833817619185,52.10921517310121,46.05299868275918,15.161174897047564,17.293371265297885,17.449657789834422,-15.23565970076243,0.5274374990064833,4.017646428723883,6.927515991639946,4.724764655566283,8.816210532339365,8.98976207988636,-0.07619208810391921,-0.1391376128958442,-0.11542137397069945,-0.043457626274793,-0.03253487439649401,-0.0433327206852537,-1.8775076307010177,-1.9767443443542692,-2.1042962718579648,-3.4524168812997913,-4.0687114520881105,-3.959552175903325,-4.024562613823524,-4.019647615602236,-3.92905128716337,-2.652150810202571,-0.39224279470642326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flights_train, flights_test = flights.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Fit linear regressino model to training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on test data\n",
    "rmse = RegressionEvaluator(labelCol='duration', metricName='rmse').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all those non-zero coefficients the model is a little hard to interpret!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight duration model: Regularization!\n",
    "  \n",
    "In the previous exercise you added more predictors to the flight duration model. The model performed well on testing data, but with so many coefficients it was difficult to interpret.\n",
    "  \n",
    "In this exercise you'll use Lasso regression (regularized with a L1 penalty) to create a more parsimonious model. Many of the coefficients in the resulting model will be set to zero. This means that only a subset of the predictors actually contribute to the model. Despite the simpler model, it still produces a good RMSE on the testing data.\n",
    "  \n",
    "You'll use a specific value for the regularization strength. Later you'll learn how to find the best value using cross validation.\n",
    "  \n",
    "The data (same as previous exercise) are available as `flights`, randomly split into `flights_train` and `flights_test`.\n",
    "  \n",
    "There are two parameters for this model, Î» (`regParam=`) and Î± (`elasticNetParam=`), where Î± determines the type of regularization and Î» gives the strength of regularization.\n",
    "  \n",
    "---\n",
    "  \n",
    "1. Fit a linear regression model to the training data. Set the regularization strength to 1.\n",
    "2. Calculate the RMSE on the testing data.\n",
    "3. Look at the model coefficients.\n",
    "4. How many of the coefficients are equal to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 11.75160605749653\n",
      "[0.07355424286775926,5.432928790143289,0.0,29.411490370261465,22.33666602511861,-2.048290181992168,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0662052202234578,1.3808582422528557,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Number of coefficients equal to 0: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit Lasso model (Î» = 1, Î± = 1) to training data\n",
    "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration', metricName='rmse').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization produced a far simpler model with similar test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
